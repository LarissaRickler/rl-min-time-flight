{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0649b76a",
   "metadata": {},
   "source": [
    "# Init Bionic VTOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "96751412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module Flyonic.\n",
      "WARNING: using Flyonic.eth_vtol_param in module Main conflicts with an existing identifier.\n"
     ]
    }
   ],
   "source": [
    "include(\"../Flyonic.jl\");\n",
    "using .Flyonic;\n",
    "\n",
    "using Rotations; # used for initial position\n",
    "\n",
    "using ReinforcementLearning;\n",
    "using StableRNGs;\n",
    "using Flux;\n",
    "using Flux.Losses;\n",
    "using Random;\n",
    "using IntervalSets;\n",
    "using LinearAlgebra;\n",
    "using Distributions;\n",
    "\n",
    "using Plots;\n",
    "using Statistics;\n",
    "\n",
    "using BSON: @save, @load # save mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f7e4ee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_visualization();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "7d9557df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# indicates how many threads Julia was started with. This is important for the multi-threaded environment\n",
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5411db62",
   "metadata": {},
   "source": [
    "# Create Reinforcement Learning Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "96af6ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct VtolEnv{A,T,ACT,R<:AbstractRNG} <: AbstractEnv # Parametric Constructor for a subtype of AbstractEnv\n",
    "    action_space::A\n",
    "    observation_space::Space{Vector{ClosedInterval{T}}}\n",
    "    state::Vector{T}\n",
    "    action::ACT\n",
    "    done::Bool\n",
    "    t::T\n",
    "    rng::R\n",
    "\n",
    "    name::String #for multible environoments\n",
    "    \n",
    "    # Everything you need aditionaly can also go in here.\n",
    "    x_W::Vector{T}\n",
    "    v_B::Vector{T}\n",
    "    R_W::Matrix{T}\n",
    "    ω_B::Vector{T}\n",
    "    wind_W::Vector{T}\n",
    "    Δt::T\n",
    "    # Bonus\n",
    "    x_d_W::Vector{T}\n",
    "    angle_d_W::T\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "14a6873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a keyword-based constructor for the type declared in the mutable struct typedef. \n",
    "# It could also be done with the macro Base.@kwdef.\n",
    "function VtolEnv(;\n",
    "     \n",
    "    #continuous = true,\n",
    "    rng = Random.GLOBAL_RNG, # Random number generation\n",
    "    name = \"vtol\",\n",
    "    kwargs... # let the function take an arbitrary number of keyword arguments \n",
    ")\n",
    "    \n",
    "    T = Float64; # explicit type which is used e.g. in state. Cannot be altered due to the poor matrix defininon.\n",
    "\n",
    "    #action_space = Base.OneTo(21) # 21 discrete positions for the flaps\n",
    "    \n",
    "    action_space = Space(\n",
    "        ClosedInterval{T}[\n",
    "            0.0..2.0, # propeller 1\n",
    "            0.0..2.0, # propeller 2\n",
    "            ], \n",
    "    )\n",
    "\n",
    "    \n",
    "    state_space = Space( # Three continuous values in state space.\n",
    "        ClosedInterval{T}[\n",
    "            # TODO: Maybe add rotation velovity around z-axis...\n",
    "            typemin(T)..typemax(T), # world position along x\n",
    "            typemin(T)..typemax(T), # world position along y\n",
    "            typemin(T)..typemax(T), # rotation arround z\n",
    "            #########\n",
    "            #typemin(T)..typemax(T), # Maybe rotation speed around z\n",
    "            #########\n",
    "            ], \n",
    "    )\n",
    "\n",
    "    # Calculate random vector & angle\n",
    "\n",
    "    #################################\n",
    "    \n",
    "    create_VTOL(name, actuators = true, color_vec=[1.0; 1.0; 0.6; 1.0]);\n",
    "    # TODO Why exactly give a matrix as input + Output is not quaternion...\n",
    "    set_transform(name, [0.0; 0.0; 0.0] ,QuatRotation(UnitQuaternion(RotY(-pi/2.0)*RotX(pi))));\n",
    "    #set_transform(name, [0.0; 0.0; 0.0] ,QuatRotation(UnitQuaternion(RotX(pi))));\n",
    "    set_actuators(name, [0.0; 0.0; 0.0; 0.0]) \n",
    "    # TODO: Set desired position\n",
    "\n",
    "    environment = VtolEnv(\n",
    "        action_space,\n",
    "        state_space,\n",
    "        zeros(T, 4), # current state, needs to be extended.\n",
    "        rand(action_space),\n",
    "        false, # episode done ?\n",
    "        0.0, # time\n",
    "        rng, # random number generator  \n",
    "        name,\n",
    "        zeros(T, 3), # x_W\n",
    "        zeros(T, 3), # v_B\n",
    "        #Matrix(UnitQuaternion((RotX(pi)))),\n",
    "        Matrix(UnitQuaternion(RotY(-pi/2.0)*RotX(pi))), # Float64... so T needs to be Float64\n",
    "        zeros(T, 3), # ω_B\n",
    "        zeros(T, 3), # wind_W\n",
    "        T(0.025), # Δt \n",
    "        # TODO Random\n",
    "        [1.0, 1.0, 0.0], # desired distance \n",
    "        calculateAngle([1.0 ,0.0, 0.0], [1.0, 1.0, 0.0]), # desired angle\n",
    "    )\n",
    "    \n",
    "    \n",
    "    reset!(environment)\n",
    "    \n",
    "    return environment\n",
    "    \n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "0fb392f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Don't get that part. Ask next meeting\n",
    "print(typeof(RotY(-pi/2.0)*RotX(pi)))\n",
    "print(typeof(UnitQuaternion(RotY(-pi/2.0)*RotX(pi))))\n",
    "print(typeof(QuatRotation(UnitQuaternion(RotY(-pi/2.0)*RotX(pi)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec660d5e",
   "metadata": {},
   "source": [
    "Just for explanation:\n",
    "\n",
    "1. A mutable Struct is created. A struct is a constructor and a constructor is a function that creates new objects.\n",
    "2. A outer keyword-based constructor method is added for the type declared in the mutable struct typedef before.\n",
    "\n",
    "So now we have a function with two methods. Julia will decide which method to call by multiple dispatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "fa93cbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "# 2 methods for type constructor:<ul><li> VtolEnv(; <i>rng, name, kwargs...</i>) in Main at <a href=\"https://github.com/LarissaRickler/tum-adlr-01/tree/1e6d86c9a2054799c5e77cc97a41b8c8171ff482//src/2d/rl_continuous_2d.ipynb#L3\" target=\"_blank\">/Users/leonardoigler/Documents/Studium/Semester 3/ADLR/Project/tum-adlr-01/src/2d/rl_continuous_2d.ipynb:3</a></li> <li> VtolEnv(action_space::<b>A</b>, observation_space::<b>Space{Array{ClosedInterval{T}, 1}}</b>, state::<b>Vector{T}</b>, action::<b>ACT</b>, done::<b>Bool</b>, t::<b>T</b>, rng::<b>R</b>, name::<b>String</b>, x_W::<b>Vector{T}</b>, v_B::<b>Vector{T}</b>, R_W::<b>Matrix{T}</b>, ω_B::<b>Vector{T}</b>, wind_W::<b>Vector{T}</b>, Δt::<b>T</b>, x_d_W::<b>Vector{T}</b>, angle_d_W::<b>T</b>)<i> where {A, T, ACT, R<:AbstractRNG}</i> in Main at <a href=\"https://github.com/LarissaRickler/tum-adlr-01/tree/1e6d86c9a2054799c5e77cc97a41b8c8171ff482//src/2d/rl_continuous_2d.ipynb#L2\" target=\"_blank\">/Users/leonardoigler/Documents/Studium/Semester 3/ADLR/Project/tum-adlr-01/src/2d/rl_continuous_2d.ipynb:2</a></li> </ul>"
      ],
      "text/plain": [
       "# 2 methods for type constructor:\n",
       "[1] VtolEnv(; rng, name, kwargs...) in Main at /Users/leonardoigler/Documents/Studium/Semester 3/ADLR/Project/tum-adlr-01/src/2d/rl_continuous_2d.ipynb:3\n",
       "[2] VtolEnv(action_space::A, observation_space::Space{Array{ClosedInterval{T}, 1}}, state::Vector{T}, action::ACT, done::Bool, t::T, rng::R, name::String, x_W::Vector{T}, v_B::Vector{T}, R_W::Matrix{T}, ω_B::Vector{T}, wind_W::Vector{T}, Δt::T, x_d_W::Vector{T}, angle_d_W::T) where {A, T, ACT, R<:AbstractRNG} in Main at /Users/leonardoigler/Documents/Studium/Semester 3/ADLR/Project/tum-adlr-01/src/2d/rl_continuous_2d.ipynb:2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "methods(VtolEnv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806413d1",
   "metadata": {},
   "source": [
    "# Define the RL interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "0f822029",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(env::VtolEnv, seed) = Random.seed!(env.rng, seed)\n",
    "RLBase.action_space(env::VtolEnv) = env.action_space\n",
    "RLBase.state_space(env::VtolEnv) = env.observation_space\n",
    "RLBase.is_terminated(env::VtolEnv) = env.done\n",
    "RLBase.state(env::VtolEnv) = env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "1f7fb89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "function computeReward(env::VtolEnv{A,T}) where {A,T}\n",
    "    \n",
    "    stay_alive = 3.0\n",
    "\n",
    "    distance_goal = norm(env.x_d_W-[env.state[1], env.state[2], 0])*100.0\n",
    "\n",
    "    difference_angle = abs(env.state[3]-env.angle_d_W)*50.0\n",
    "    \n",
    "\n",
    "    # TODO Save last position or last projection somewhere (env.last) --> Compare \n",
    "    # to current project along line\n",
    "\n",
    "    #not_upright_orientation = abs(env.state[1]-pi*0.5)*10.0\n",
    "    #not_centered_position = abs(env.state[2])*10.0\n",
    "    #hight = env.state[4]*100.0\n",
    "    \n",
    "    #return stay_alive - not_upright_orientation - not_centered_position #+ hight\n",
    "    return stay_alive - distance_goal - difference_angle\n",
    "end\n",
    "\n",
    "\n",
    "RLBase.reward(env::VtolEnv{A,T}) where {A,T} = computeReward(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ae45ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "function RLBase.reset!(env::VtolEnv{A,T}) where {A,T}\n",
    "    \n",
    "    # Visualize initial state\n",
    "    set_transform(env.name, env.x_W,QuatRotation(env.R_W));\n",
    "    set_actuators(env.name, [0.0; 0.0; 0.0; 0.0])#; 0.0; 0.0])\n",
    "    \n",
    "    env.x_W = [0.0; 0.0; 0.0];\n",
    "    env.v_B = [0.0; 0.0; 0.0];\n",
    "    env.R_W = Matrix(UnitQuaternion(RotY(-pi/2.0)*RotX(pi)));\n",
    "    #env.R_W = Matrix(UnitQuaternion(RotX(pi)));\n",
    "    env.ω_B = [0.0; 0.0; 0.0];\n",
    "    env.wind_W = [0.0; 0.0; 0.0];\n",
    "\n",
    "\n",
    "    env.x_d_W = [1.0, 1.0, 0.0] # desired distance \n",
    "    env.angle_d_W = calculateAngle([1.0 ,0.0, 0.0], env.x_d_W) # desired angle\n",
    "    \n",
    "    # TODO: Check why NaN\n",
    "    env.state = [env.x_W[1]; env.x_W[2]; Rotations.params(RotYXZ(env.R_W))[3]]\n",
    "    env.t = 0.0\n",
    "    env.action = [0.0]#, 0.0, 0.0, 0.0]\n",
    "    env.done = false\n",
    "    nothing\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "5d575982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element StaticArraysCore.SVector{3, Float64} with indices SOneTo(3):\n",
       " 3.141592653589793\n",
       " 1.2246467991473532e-16\n",
       " 3.141592653589793"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#calculateAngle([1.0 ,0.0, 0.0], [1.0, 1.0, 0.0])\n",
    "R_W = Matrix(UnitQuaternion(RotX(pi)));\n",
    "Rotations.params(RotYXZ(R_W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "4a1f1ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element StaticArraysCore.SVector{3, Float64} with indices SOneTo(3):\n",
       " 3.141592653589793\n",
       " 1.2246467991473532e-16\n",
       " 3.141592653589793"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "R_W = Matrix(UnitQuaternion(RotX(pi)))\n",
    "Rotations.params(RotYXZ(R_W))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "1cf1a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines a methods for a callable object.\n",
    "# So when a VtolEnv object is created, it has this method that can be called\n",
    "function (env::VtolEnv)(a)\n",
    "\n",
    "    # set the propeller trust and the two flaps 2D case\n",
    "    next_action = [a[1], a[2], 0.0 ,0.0]\n",
    "   \n",
    "    _step!(env, next_action)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "26a5a0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# VtolEnv\n",
       "\n",
       "## Traits\n",
       "\n",
       "| Trait Type        |                  Value |\n",
       "|:----------------- | ----------------------:|\n",
       "| NumAgentStyle     |          SingleAgent() |\n",
       "| DynamicStyle      |           Sequential() |\n",
       "| InformationStyle  | ImperfectInformation() |\n",
       "| ChanceStyle       |           Stochastic() |\n",
       "| RewardStyle       |           StepReward() |\n",
       "| UtilityStyle      |           GeneralSum() |\n",
       "| ActionStyle       |     MinimalActionSet() |\n",
       "| StateStyle        |     Observation{Any}() |\n",
       "| DefaultStateStyle |     Observation{Any}() |\n",
       "\n",
       "## Is Environment Terminated?\n",
       "\n",
       "No\n",
       "\n",
       "## State Space\n",
       "\n",
       "`Space{Vector{ClosedInterval{Float64}}}(ClosedInterval{Float64}[-Inf..Inf, -Inf..Inf, -Inf..Inf])`\n",
       "\n",
       "## Action Space\n",
       "\n",
       "`Space{Vector{ClosedInterval{Float64}}}(ClosedInterval{Float64}[0.0..2.0, 0.0..2.0])`\n",
       "\n",
       "## Current State\n",
       "\n",
       "```\n",
       "[0.0, 0.0, 3.141592653589793]\n",
       "```\n"
      ],
      "text/plain": [
       "# VtolEnv\n",
       "\n",
       "## Traits\n",
       "\n",
       "| Trait Type        |                  Value |\n",
       "|:----------------- | ----------------------:|\n",
       "| NumAgentStyle     |          SingleAgent() |\n",
       "| DynamicStyle      |           Sequential() |\n",
       "| InformationStyle  | ImperfectInformation() |\n",
       "| ChanceStyle       |           Stochastic() |\n",
       "| RewardStyle       |           StepReward() |\n",
       "| UtilityStyle      |           GeneralSum() |\n",
       "| ActionStyle       |     MinimalActionSet() |\n",
       "| StateStyle        |     Observation{Any}() |\n",
       "| DefaultStateStyle |     Observation{Any}() |\n",
       "\n",
       "## Is Environment Terminated?\n",
       "\n",
       "No\n",
       "\n",
       "## State Space\n",
       "\n",
       "`Space{Vector{ClosedInterval{Float64}}}(ClosedInterval{Float64}[-Inf..Inf, -Inf..Inf, -Inf..Inf])`\n",
       "\n",
       "## Action Space\n",
       "\n",
       "`Space{Vector{ClosedInterval{Float64}}}(ClosedInterval{Float64}[0.0..2.0, 0.0..2.0])`\n",
       "\n",
       "## Current State\n",
       "\n",
       "```\n",
       "[0.0, 0.0, 3.141592653589793]\n",
       "```\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = VtolEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5aa39474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "# 3 methods for callable object:<ul><li> (env::<b>VtolEnv</b>)(a) in Main at <a href=\"https://github.com/LarissaRickler/tum-adlr-01/tree/1e6d86c9a2054799c5e77cc97a41b8c8171ff482//src/2d/rl_continuous_2d.ipynb#L3\" target=\"_blank\">/Users/leonardoigler/Documents/Studium/Semester 3/ADLR/Project/tum-adlr-01/src/2d/rl_continuous_2d.ipynb:3</a></li> <li> (env::<b>AbstractEnv</b>)(action) in ReinforcementLearningBase</li> <li> (env::<b>AbstractEnv</b>)(action, player) in ReinforcementLearningBase</li> </ul>"
      ],
      "text/plain": [
       "# 3 methods for callable object:\n",
       "[1] (env::VtolEnv)(a) in Main at /Users/leonardoigler/Documents/Studium/Semester 3/ADLR/Project/tum-adlr-01/src/2d/rl_continuous_2d.ipynb:3\n",
       "[2] (env::AbstractEnv)(action) in ReinforcementLearningBase\n",
       "[3] (env::AbstractEnv)(action, player) in ReinforcementLearningBase"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "methods(env) # Just to explain which methods the object has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "9e7d4727",
   "metadata": {},
   "outputs": [],
   "source": [
    "function _step!(env::VtolEnv, next_action)\n",
    "        \n",
    "    # caluclate wind impact\n",
    "    v_in_wind_B = vtol_add_wind(env.v_B, env.R_W, env.wind_W)\n",
    "    # caluclate aerodynamic forces\n",
    "    torque_B, force_B = vtol_model(v_in_wind_B, next_action, eth_vtol_param);\n",
    "    # integrate rigid body dynamics for Δt\n",
    "    env.x_W, env.v_B, env.R_W, env.ω_B, time = rigid_body_simple(torque_B, force_B, env.x_W, env.v_B, env.R_W, env.ω_B, env.t, env.Δt, eth_vtol_param)\n",
    "\n",
    "\n",
    "    # Visualize the new state \n",
    "    # TODO: Can be removed for real trainings\n",
    "    set_transform(env.name, env.x_W, QuatRotation(env.R_W));\n",
    "    set_actuators(env.name, next_action)\n",
    " \n",
    "    env.t += env.Δt\n",
    "    \n",
    "    # State space\n",
    "    rot = Rotations.params(RotYXZ(env.R_W))[3]\n",
    "    print(rot)\n",
    "    env.state[1] = env.x_W[1] # world position in x\n",
    "    env.state[2] = env.ω_B[2] # world position in y\n",
    "    env.state[3] = rot # rotation around z    \n",
    "    \n",
    "    # Termination criteria\n",
    "    env.done = #true\n",
    "\n",
    "        # TODO Zu lang\n",
    "        # TODO Zu weit weg\n",
    "        # TODO Ziel erreicht\n",
    "\n",
    "        # After time... How fast is drone+Range of desired point\n",
    "        # After reaching position (circle of r_tol)\n",
    "\n",
    "        #norm(env.v_B) > 2.0 || # stop if body is too fast\n",
    "        #env.x_W[2] < -1.0 || # stop if body is below -1m\n",
    "        #0.0 > rot || # Stop if the drone is pitched 90°.\n",
    "        #rot > pi || # Stop if the drone is pitched 90°.\n",
    "        env.t > 10 # stop after 10s\n",
    "    nothing\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "6e1cd988",
   "metadata": {},
   "outputs": [
    {
     "ename": "Test.TestSetException",
     "evalue": "Some tests did not pass: 1379 passed, 621 failed, 0 errored, 0 broken.",
     "output_type": "error",
     "traceback": [
      "Some tests did not pass: 1379 passed, 621 failed, 0 errored, 0 broken.\n",
      "\n",
      "Stacktrace:\n",
      " [1] finish(ts::Test.DefaultTestSet)\n",
      "   @ Test /Applications/Julia-1.8.app/Contents/Resources/julia/share/julia/stdlib/v1.8/Test/src/Test.jl:1092\n",
      " [2] macro expansion\n",
      "   @ /Applications/Julia-1.8.app/Contents/Resources/julia/share/julia/stdlib/v1.8/Test/src/Test.jl:1368 [inlined]\n",
      " [3] test_runnable!(env::VtolEnv{Space{Vector{ClosedInterval{Float64}}}, Float64, Vector{Float64}, Random._GLOBAL_RNG}, n::Int64; rng::Random._GLOBAL_RNG)\n",
      "   @ ReinforcementLearningBase ~/.julia/packages/ReinforcementLearningBase/E7jI5/src/base.jl:267\n",
      " [4] test_runnable! (repeats 2 times)\n",
      "   @ ~/.julia/packages/ReinforcementLearningBase/E7jI5/src/base.jl:265 [inlined]\n",
      " [5] top-level scope\n",
      "   @ ~/Documents/Studium/Semester 3/ADLR/Project/tum-adlr-01/src/2d/rl_continuous_2d.ipynb:1"
     ]
    }
   ],
   "source": [
    "RLBase.test_runnable!(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c223a31f",
   "metadata": {},
   "source": [
    "Show an overview of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe6de74",
   "metadata": {},
   "source": [
    "# Setup of a reinforcement learning experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e5683fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "MultiThreadEnv(8 x VtolEnv)"
      ],
      "text/plain": [
       "MultiThreadEnv(8 x VtolEnv)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = 123    \n",
    "rng = StableRNG(seed)\n",
    "    N_ENV = 8\n",
    "    UPDATE_FREQ = 1024\n",
    "    \n",
    "    \n",
    "    # define multiple environments for parallel training\n",
    "    env = MultiThreadEnv([\n",
    "        # use different names for the visualization\n",
    "        VtolEnv(; rng = StableRNG(hash(seed+i)), name = \"vtol$i\") for i in 1:N_ENV\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d1f128b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function approximator\n",
    "    ns, na = length(state(env[1])), length(action_space(env[1]))\n",
    "    approximator = ActorCritic(\n",
    "                actor = GaussianNetwork(\n",
    "                    pre = Chain(\n",
    "                    Dense(ns, 16, relu; initW = glorot_uniform(rng)),#\n",
    "                    Dense(16, 16, relu; initW = glorot_uniform(rng)),\n",
    "                    ),\n",
    "                    μ = Chain(Dense(16, na; initW = glorot_uniform(rng))),\n",
    "                    logσ = Chain(Dense(16, na; initW = glorot_uniform(rng))),\n",
    "                ),\n",
    "                critic = Chain(\n",
    "                    Dense(ns, 16, relu; initW = glorot_uniform(rng)),\n",
    "                    Dense(16, 16, relu; initW = glorot_uniform(rng)),\n",
    "                    Dense(16, 1; initW = glorot_uniform(rng)),\n",
    "                ),\n",
    "                optimizer = ADAM(1e-3),\n",
    "            );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "7ea4c37c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "typename(Agent)\n",
       "├─ policy => typename(PPOPolicy)\n",
       "│  ├─ approximator => typename(ActorCritic)\n",
       "│  │  ├─ actor => typename(GaussianNetwork)\n",
       "│  │  │  ├─ pre => typename(Chain)\n",
       "│  │  │  │  └─ layers\n",
       "│  │  │  │     ├─ 1\n",
       "│  │  │  │     │  └─ typename(Dense)\n",
       "│  │  │  │     │     ├─ weight => 16×3 Matrix{Float32}\n",
       "│  │  │  │     │     ├─ bias => 16-element Vector{Float32}\n",
       "│  │  │  │     │     └─ σ => typename(typeof(relu))\n",
       "│  │  │  │     └─ 2\n",
       "│  │  │  │        └─ typename(Dense)\n",
       "│  │  │  │           ├─ weight => 16×16 Matrix{Float32}\n",
       "│  │  │  │           ├─ bias => 16-element Vector{Float32}\n",
       "│  │  │  │           └─ σ => typename(typeof(relu))\n",
       "│  │  │  ├─ μ => typename(Chain)\n",
       "│  │  │  │  └─ layers\n",
       "│  │  │  │     └─ 1\n",
       "│  │  │  │        └─ typename(Dense)\n",
       "│  │  │  │           ├─ weight => 2×16 Matrix{Float32}\n",
       "│  │  │  │           ├─ bias => 2-element Vector{Float32}\n",
       "│  │  │  │           └─ σ => typename(typeof(identity))\n",
       "│  │  │  ├─ logσ => typename(Chain)\n",
       "│  │  │  │  └─ layers\n",
       "│  │  │  │     └─ 1\n",
       "│  │  │  │        └─ typename(Dense)\n",
       "│  │  │  │           ├─ weight => 2×16 Matrix{Float32}\n",
       "│  │  │  │           ├─ bias => 2-element Vector{Float32}\n",
       "│  │  │  │           └─ σ => typename(typeof(identity))\n",
       "│  │  │  ├─ min_σ => 0.0\n",
       "│  │  │  ├─ max_σ => Inf\n",
       "│  │  │  └─ normalizer => typename(typeof(tanh))\n",
       "│  │  ├─ critic => typename(Chain)\n",
       "│  │  │  └─ layers\n",
       "│  │  │     ├─ 1\n",
       "│  │  │     │  └─ typename(Dense)\n",
       "│  │  │     │     ├─ weight => 16×3 Matrix{Float32}\n",
       "│  │  │     │     ├─ bias => 16-element Vector{Float32}\n",
       "│  │  │     │     └─ σ => typename(typeof(relu))\n",
       "│  │  │     ├─ 2\n",
       "│  │  │     │  └─ typename(Dense)\n",
       "│  │  │     │     ├─ weight => 16×16 Matrix{Float32}\n",
       "│  │  │     │     ├─ bias => 16-element Vector{Float32}\n",
       "│  │  │     │     └─ σ => typename(typeof(relu))\n",
       "│  │  │     └─ 3\n",
       "│  │  │        └─ typename(Dense)\n",
       "│  │  │           ├─ weight => 1×16 Matrix{Float32}\n",
       "│  │  │           ├─ bias => 1-element Vector{Float32}\n",
       "│  │  │           └─ σ => typename(typeof(identity))\n",
       "│  │  └─ optimizer => typename(ADAM)\n",
       "│  │     ├─ eta => 0.001\n",
       "│  │     ├─ beta\n",
       "│  │     │  ├─ 1\n",
       "│  │     │  │  └─ 0.9\n",
       "│  │     │  └─ 2\n",
       "│  │     │     └─ 0.999\n",
       "│  │     ├─ epsilon => 1.0e-8\n",
       "│  │     └─ state => typename(IdDict)\n",
       "│  ├─ γ => 0.99\n",
       "│  ├─ λ => 0.95\n",
       "│  ├─ clip_range => 0.2\n",
       "│  ├─ max_grad_norm => 0.5\n",
       "│  ├─ n_microbatches => 4\n",
       "│  ├─ n_epochs => 4\n",
       "│  ├─ actor_loss_weight => 1.0\n",
       "│  ├─ critic_loss_weight => 0.5\n",
       "│  ├─ entropy_loss_weight => 0.01\n",
       "│  ├─ rng => typename(Random._GLOBAL_RNG)\n",
       "│  ├─ n_random_start => 0\n",
       "│  ├─ update_freq => 1024\n",
       "│  ├─ update_step => 0\n",
       "│  ├─ norm => 4×4 Matrix{Float32}\n",
       "│  ├─ actor_loss => 4×4 Matrix{Float32}\n",
       "│  ├─ critic_loss => 4×4 Matrix{Float32}\n",
       "│  ├─ entropy_loss => 4×4 Matrix{Float32}\n",
       "│  └─ loss => 4×4 Matrix{Float32}\n",
       "└─ trajectory => typename(Trajectory)\n",
       "   └─ traces => typename(NamedTuple)\n",
       "      ├─ action_log_prob => 8×0 CircularArrayBuffers.CircularArrayBuffer{Float64, 2, Matrix{Float64}}\n",
       "      ├─ state => 3×8×0 CircularArrayBuffers.CircularArrayBuffer{Float64, 3, Array{Float64, 3}}\n",
       "      ├─ action => 2×8×0 CircularArrayBuffers.CircularArrayBuffer{Float64, 3, Array{Float64, 3}}\n",
       "      ├─ reward => 8×0 CircularArrayBuffers.CircularArrayBuffer{Float64, 2, Matrix{Float64}}\n",
       "      └─ terminal => 8×0 CircularArrayBuffers.CircularArrayBuffer{Bool, 2, Matrix{Bool}}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    agent = Agent( # A wrapper of an AbstractPolicy\n",
    "        # AbstractPolicy: the policy to use\n",
    "        policy = PPOPolicy(;\n",
    "                    approximator = approximator |> gpu,\n",
    "                    update_freq=UPDATE_FREQ,\n",
    "                    dist = Normal,\n",
    "                    # For parameters visit the docu: https://juliareinforcementlearning.org/docs/rlzoo/#ReinforcementLearningZoo.PPOPolicy\n",
    "                    ),\n",
    "        \n",
    "        # AbstractTrajectory: used to store transitions between an agent and an environment source\n",
    "        trajectory = PPOTrajectory(;\n",
    "            capacity = UPDATE_FREQ,\n",
    "            state = Matrix{Float64} => (ns, N_ENV),\n",
    "            action = Matrix{Float64} => (na, N_ENV),\n",
    "            action_log_prob = Vector{Float64} => (N_ENV,),\n",
    "            reward = Vector{Float64} => (N_ENV,),\n",
    "            terminal = Vector{Bool} => (N_ENV,),\n",
    "        ),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "7f158a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "saveModel (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function saveModel(t, agent, env)\n",
    "    model = cpu(agent.policy.approximator)   \n",
    "    f = joinpath(\"./RL_models/\", \"vtol_ppo_2_$t.bson\")\n",
    "    @save f model\n",
    "    println(\"parameters at step $t saved to $f\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "289acdd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loadModel (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function loadModel()\n",
    "    f = joinpath(\"./RL_models/\", \"vtol_ppo_2_9320000.bson\")\n",
    "    @load f model\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "f0c9d222",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent.policy.approximator = loadModel();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "bb737010",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "ArgumentError: Normal: the condition σ >= zero(σ) is not satisfied.",
     "output_type": "error",
     "traceback": [
      "ArgumentError: Normal: the condition σ >= zero(σ) is not satisfied.\n",
      "\n",
      "Stacktrace:\n",
      "  [1] macro expansion\n",
      "    @ ~/.julia/packages/Distributions/Xrm9e/src/utils.jl:6 [inlined]\n",
      "  [2] #Normal#120\n",
      "    @ ~/.julia/packages/Distributions/Xrm9e/src/univariate/continuous/normal.jl:37 [inlined]\n",
      "  [3] Normal\n",
      "    @ ~/.julia/packages/Distributions/Xrm9e/src/univariate/continuous/normal.jl:36 [inlined]\n",
      "  [4] createinstance\n",
      "    @ ~/.julia/packages/StructArrays/w2GaP/src/interface.jl:49 [inlined]\n",
      "  [5] getindex\n",
      "    @ ~/.julia/packages/StructArrays/w2GaP/src/structarray.jl:365 [inlined]\n",
      "  [6] _getindex\n",
      "    @ ./abstractarray.jl:1274 [inlined]\n",
      "  [7] getindex\n",
      "    @ ./abstractarray.jl:1241 [inlined]\n",
      "  [8] _broadcast_getindex\n",
      "    @ ./broadcast.jl:636 [inlined]\n",
      "  [9] _getindex\n",
      "    @ ./broadcast.jl:667 [inlined]\n",
      " [10] _getindex\n",
      "    @ ./broadcast.jl:666 [inlined]\n",
      " [11] _broadcast_getindex\n",
      "    @ ./broadcast.jl:642 [inlined]\n",
      " [12] getindex\n",
      "    @ ./broadcast.jl:597 [inlined]\n",
      " [13] copyto_nonleaf!(dest::Matrix{Float64}, bc::Base.Broadcast.Broadcasted{StructArrays.StructArrayStyle{Base.Broadcast.DefaultArrayStyle{2}, 2}, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}}, typeof(rand), Tuple{Base.RefValue{Random._GLOBAL_RNG}, Base.Broadcast.Extruded{StructArrays.StructArray{Normal, 2, NamedTuple{(:μ, :σ), Tuple{Matrix{Float64}, Matrix{Float64}}}, Int64}, Tuple{Bool, Bool}, Tuple{Int64, Int64}}}}, iter::CartesianIndices{2, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}}}, state::CartesianIndex{2}, count::Int64)\n",
      "    @ Base.Broadcast ./broadcast.jl:1055\n",
      " [14] copy(bc::Base.Broadcast.Broadcasted{StructArrays.StructArrayStyle{Base.Broadcast.DefaultArrayStyle{2}, 2}, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}}, typeof(rand), Tuple{Base.RefValue{Random._GLOBAL_RNG}, StructArrays.StructArray{Normal, 2, NamedTuple{(:μ, :σ), Tuple{Matrix{Float64}, Matrix{Float64}}}, Int64}}})\n",
      "    @ Base.Broadcast ./broadcast.jl:907\n",
      " [15] materialize\n",
      "    @ ./broadcast.jl:860 [inlined]\n",
      " [16] (::Agent{PPOPolicy{ActorCritic{GaussianNetwork{Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, typeof(tanh)}, Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, ADAM}, Normal, Random._GLOBAL_RNG}, Trajectory{NamedTuple{(:action_log_prob, :state, :action, :reward, :terminal), Tuple{CircularArrayBuffers.CircularArrayBuffer{Float64, 2, Matrix{Float64}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 3, Array{Float64, 3}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 3, Array{Float64, 3}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 2, Matrix{Float64}}, CircularArrayBuffers.CircularArrayBuffer{Bool, 2, Matrix{Bool}}}}}})(env::MultiThreadEnv{VtolEnv{Space{Vector{ClosedInterval{Float64}}}, Float64, Vector{Float64}, StableRNGs.LehmerRNG}, Matrix{Float64}, Vector{Float64}, Space{Matrix{ClosedInterval{Float64}}}, Space{Matrix{ClosedInterval{Float64}}}, Nothing})\n",
      "    @ ReinforcementLearningZoo ~/.julia/packages/ReinforcementLearningZoo/tvfq9/src/algorithms/policy_gradient/ppo.jl:192\n",
      " [17] _run(policy::Agent{PPOPolicy{ActorCritic{GaussianNetwork{Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, typeof(tanh)}, Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, ADAM}, Normal, Random._GLOBAL_RNG}, Trajectory{NamedTuple{(:action_log_prob, :state, :action, :reward, :terminal), Tuple{CircularArrayBuffers.CircularArrayBuffer{Float64, 2, Matrix{Float64}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 3, Array{Float64, 3}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 3, Array{Float64, 3}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 2, Matrix{Float64}}, CircularArrayBuffers.CircularArrayBuffer{Bool, 2, Matrix{Bool}}}}}}, env::MultiThreadEnv{VtolEnv{Space{Vector{ClosedInterval{Float64}}}, Float64, Vector{Float64}, StableRNGs.LehmerRNG}, Matrix{Float64}, Vector{Float64}, Space{Matrix{ClosedInterval{Float64}}}, Space{Matrix{ClosedInterval{Float64}}}, Nothing}, stop_condition::StopAfterStep{ProgressMeter.Progress}, hook::DoEveryNStep{typeof(saveModel)})\n",
      "    @ ReinforcementLearningZoo ~/.julia/packages/ReinforcementLearningZoo/tvfq9/src/algorithms/policy_gradient/run.jl:17\n",
      " [18] run(policy::Agent{PPOPolicy{ActorCritic{GaussianNetwork{Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, typeof(tanh)}, Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, ADAM}, Normal, Random._GLOBAL_RNG}, Trajectory{NamedTuple{(:action_log_prob, :state, :action, :reward, :terminal), Tuple{CircularArrayBuffers.CircularArrayBuffer{Float64, 2, Matrix{Float64}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 3, Array{Float64, 3}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 3, Array{Float64, 3}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 2, Matrix{Float64}}, CircularArrayBuffers.CircularArrayBuffer{Bool, 2, Matrix{Bool}}}}}}, env::MultiThreadEnv{VtolEnv{Space{Vector{ClosedInterval{Float64}}}, Float64, Vector{Float64}, StableRNGs.LehmerRNG}, Matrix{Float64}, Vector{Float64}, Space{Matrix{ClosedInterval{Float64}}}, Space{Matrix{ClosedInterval{Float64}}}, Nothing}, stop_condition::StopAfterStep{ProgressMeter.Progress}, hook::DoEveryNStep{typeof(saveModel)})\n",
      "    @ ReinforcementLearningCore ~/.julia/packages/ReinforcementLearningCore/yeRLW/src/core/run.jl:10\n",
      " [19] top-level scope\n",
      "    @ ~/Documents/Studium/Semester 3/ADLR/Project/tum-adlr-01/src/2d/rl_continuous_2d.ipynb:1"
     ]
    }
   ],
   "source": [
    "run(\n",
    "           agent,\n",
    "           env,\n",
    "           StopAfterStep(100_000),\n",
    "           DoEveryNStep(saveModel, n=40_000)\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "6643cd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "close_visualization(); # closes the MeshCat visualization"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
