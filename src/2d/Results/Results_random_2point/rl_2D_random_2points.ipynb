{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0649b76a",
   "metadata": {},
   "source": [
    "# Init Bionic VTOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96751412",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../Flyonic.jl\");\n",
    "using .Flyonic;\n",
    "\n",
    "using Rotations; # used for initial position\n",
    "\n",
    "using ReinforcementLearning;\n",
    "using StableRNGs;\n",
    "using Flux;\n",
    "using Flux.Losses;\n",
    "using Random;\n",
    "using IntervalSets;\n",
    "using LinearAlgebra;\n",
    "using Distributions;\n",
    "\n",
    "using Plots;\n",
    "using Statistics;\n",
    "\n",
    "using BSON: @save, @load # save mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7e4ee64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: MeshCat server started. You can open the visualizer by visiting the following URL in your browser:\n",
      "│ http://127.0.0.1:8702\n",
      "└ @ MeshCat /home/larissa/.julia/packages/MeshCat/Ax8pH/src/visualizer.jl:73\n"
     ]
    }
   ],
   "source": [
    "create_visualization();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d9557df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indicates how many threads Julia was started with. This is important for the multi-threaded environment\n",
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea451ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eth_vtol_param[\"gravity\"] = 9.81; # set gravity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5411db62",
   "metadata": {},
   "source": [
    "# Create Reinforcement Learning Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96af6ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct VtolEnv{A,T,ACT,R<:AbstractRNG} <: AbstractEnv # Parametric Constructor for a subtype of AbstractEnv\n",
    "    action_space::A # action space\n",
    "    observation_space::Space{Vector{ClosedInterval{T}}} # observation space\n",
    "    state::Vector{T} # current state space\n",
    "    action::ACT # action space\n",
    "    done::Bool # done\n",
    "    t::T # time\n",
    "    rng::R # random number generator\n",
    "\n",
    "    name::String # for multible environoments\n",
    "    visualization::Bool # visualization\n",
    "    realtime::Bool # realtime\n",
    "    \n",
    "    # Everything you need aditionaly can also go in here.\n",
    "    x_W::Vector{T} # current position\n",
    "    v_B::Vector{T} # velocity\n",
    "    R_W::Matrix{T} # current rotation\n",
    "    ω_B::Vector{T} # rotation velocitiy\n",
    "    wind_W::Vector{T} # wind\n",
    "    Δt::T # Δ time\n",
    "    \n",
    "    # Current Bonus / Target\n",
    "    num_waypoints::Int # includig start point\n",
    "    waypoints::Vector{Vector{T}}\n",
    "    reached_goal::BitVector\n",
    "    \n",
    "    progress::T\n",
    "    progress_prev::T\n",
    "    current_point::Int\n",
    "    reached_goal_in_step::Bool\n",
    "    \n",
    "    r_tol::T\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14a6873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a keyword-based constructor for the type declared in the mutable struct typedef. \n",
    "# It could also be done with the macro Base.@kwdef.\n",
    "function VtolEnv(;\n",
    "    rng = Random.GLOBAL_RNG, # Random number generation\n",
    "    name = \"vtol\",\n",
    "    visualization = false,\n",
    "    realtime = false,\n",
    "    kwargs... # let the function take an arbitrary number of keyword arguments \n",
    ")\n",
    "    \n",
    "    T = Float64; # explicit type which is used e.g. in state. Cannot be altered due to the poor matrix defininon.\n",
    "\n",
    "    \n",
    "    action_space = Space(\n",
    "        ClosedInterval{T}[\n",
    "            0.0..2.0, # propeller 1\n",
    "            0.0..2.0, # propeller 2\n",
    "            ], \n",
    "    )\n",
    "\n",
    "    \n",
    "    state_space = Space( # Three continuous values in state space.\n",
    "        ClosedInterval{T}[\n",
    "            # orientate yourself on the state space from the paper\n",
    "            typemin(T)..typemax(T), # position along x\n",
    "            typemin(T)..typemax(T), # position along z\n",
    "            \n",
    "            typemin(T)..typemax(T), # orientation along x\n",
    "            typemin(T)..typemax(T), # orientation along z\n",
    "            \n",
    "            typemin(T)..typemax(T), # velocity along x BODY coordinates\n",
    "            typemin(T)..typemax(T), # velocity along y BODY coordinates\n",
    "            \n",
    "            typemin(T)..typemax(T), # rotational velocity along z BODY coordinates\n",
    "            \n",
    "            typemin(T)..typemax(T), # position error along x\n",
    "            typemin(T)..typemax(T), # position error along z\n",
    "            # TODO: more points?\n",
    "            ], \n",
    "    )\n",
    "    \n",
    "    num_waypoints = 3 # number of waypoints, includig start point\n",
    "    waypoints = generate_trajectory(num_waypoints) # trajectory with num_waypoints waypoints (+ start point) \n",
    "    reached_goal = falses(num_waypoints)\n",
    "    \n",
    "    if visualization #visualizes VTOL and waypoints\n",
    "        create_VTOL(name, actuators = true, color_vec=[1.0; 1.0; 0.6; 1.0]);\n",
    "        visualize_waypoints(waypoints, 0.15)\n",
    "    end\n",
    "\n",
    "    environment = VtolEnv(\n",
    "        action_space,\n",
    "        state_space, \n",
    "        zeros(T, length(state_space)), # current state, needs to be extended\n",
    "        rand(action_space), #initialization action\n",
    "        false, # episode done \n",
    "        0.0, # time\n",
    "        rng, # random number generator  \n",
    "        \n",
    "        name,\n",
    "        visualization,\n",
    "        realtime,\n",
    "        \n",
    "        zeros(T, 3), # x_W, current position\n",
    "        zeros(T, 3), # v_B, velocity\n",
    "        [1.0 0.0 0.0; 0.0 1.0 0.0; 0.0 0.0 1.0], # R_W, current rotation, Float64... so T needs to be Float64\n",
    "        zeros(T, 3), # ω_B\n",
    "        zeros(T, 3), # wind_W\n",
    "        T(0.025), # Δt \n",
    "        \n",
    "        num_waypoints, # includig start point\n",
    "        waypoints, \n",
    "        reached_goal,\n",
    "        \n",
    "        0.0, # progress\n",
    "        0.0, # progress_prev\n",
    "        2, # current point\n",
    "        false, # reached_goal_in_step\n",
    "        \n",
    "        0.5 # r_tol\n",
    "    )\n",
    "    \n",
    "    \n",
    "    reset!(environment)\n",
    "    \n",
    "    return environment\n",
    "    \n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec660d5e",
   "metadata": {},
   "source": [
    "Just for explanation:\n",
    "\n",
    "1. A mutable Struct is created. A struct is a constructor and a constructor is a function that creates new objects.\n",
    "2. A outer keyword-based constructor method is added for the type declared in the mutable struct typedef before.\n",
    "\n",
    "So now we have a function with two methods. Julia will decide which method to call by multiple dispatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc3c4bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "# 2 methods for type constructor:<ul><li> VtolEnv(; <i>rng, name, visualization, realtime, kwargs...</i>) in Main at In[6]:3</li> <li> VtolEnv(action_space::<b>A</b>, observation_space::<b>Space{Array{ClosedInterval{T}, 1}}</b>, state::<b>Vector{T}</b>, action::<b>ACT</b>, done::<b>Bool</b>, t::<b>T</b>, rng::<b>R</b>, name::<b>String</b>, visualization::<b>Bool</b>, realtime::<b>Bool</b>, x_W::<b>Vector{T}</b>, v_B::<b>Vector{T}</b>, R_W::<b>Matrix{T}</b>, ω_B::<b>Vector{T}</b>, wind_W::<b>Vector{T}</b>, Δt::<b>T</b>, num_waypoints::<b>Int64</b>, waypoints::<b>Array{Vector{T}, 1}</b>, reached_goal::<b>BitVector</b>, progress::<b>T</b>, progress_prev::<b>T</b>, current_point::<b>Int64</b>, reached_goal_in_step::<b>Bool</b>, r_tol::<b>T</b>)<i> where {A, T, ACT, R<:AbstractRNG}</i> in Main at In[5]:2</li> </ul>"
      ],
      "text/plain": [
       "# 2 methods for type constructor:\n",
       "[1] VtolEnv(; rng, name, visualization, realtime, kwargs...) in Main at In[6]:3\n",
       "[2] VtolEnv(action_space::A, observation_space::Space{Array{ClosedInterval{T}, 1}}, state::Vector{T}, action::ACT, done::Bool, t::T, rng::R, name::String, visualization::Bool, realtime::Bool, x_W::Vector{T}, v_B::Vector{T}, R_W::Matrix{T}, ω_B::Vector{T}, wind_W::Vector{T}, Δt::T, num_waypoints::Int64, waypoints::Array{Vector{T}, 1}, reached_goal::BitVector, progress::T, progress_prev::T, current_point::Int64, reached_goal_in_step::Bool, r_tol::T) where {A, T, ACT, R<:AbstractRNG} in Main at In[5]:2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methods(VtolEnv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806413d1",
   "metadata": {},
   "source": [
    "# Define the RL interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f822029",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(env::VtolEnv, seed) = Random.seed!(env.rng, seed)\n",
    "RLBase.action_space(env::VtolEnv) = env.action_space\n",
    "RLBase.state_space(env::VtolEnv) = env.observation_space\n",
    "RLBase.is_terminated(env::VtolEnv) = env.done\n",
    "RLBase.state(env::VtolEnv) = env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f7fb89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "function computeReward(env::VtolEnv{A,T}) where {A,T}\n",
    "    \n",
    "    k_wp = 5.0 # factor for reached gate reward\n",
    "    r_wp = 0.0 # reward for reached gate, TODO: change to gates later (when gates != waypoints)\n",
    "    if env.reached_goal_in_step\n",
    "        r_wp = exp(-norm(env.x_W - env.waypoints[env.current_point - 1])/env.r_tol)\n",
    "    end\n",
    "    \n",
    "    k_ω = 0.01 # factor for too high body rate\n",
    "    norm_ω = norm(env.ω_B[3]) # reward for body rate (penalty)\n",
    "\n",
    "    k_s = 0.0 #5.0 # factor for reached distance (overall) reward, TODO later add factor as in paper (p. 4)\n",
    "    r_s = env.progress # reward for reached distance (overall)\n",
    "    \n",
    "    \n",
    "    \n",
    "    k_p = 5.0 # factor for progress (between current position and last position) reward \n",
    "    r_p = env.progress - env.progress_prev # reward for progress (between current position and last position)\n",
    "\n",
    "    return k_p * r_p + k_s * k_s + k_wp * r_wp - k_ω * norm_ω\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "RLBase.reward(env::VtolEnv{A,T}) where {A,T} = computeReward(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae45ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "function RLBase.reset!(env::VtolEnv{A,T}) where {A,T}\n",
    "    # Visualize initial state\n",
    "    if env.visualization\n",
    "        set_transform(env.name, env.x_W, QuatRotation(env.R_W));\n",
    "        set_actuators(env.name, [0.0; 0.0; 0.0; 0.0]);\n",
    "    end\n",
    "    \n",
    "    env.x_W = [0.0; 0.0; 0.0];\n",
    "    env.v_B = [0.0; 0.0; 0.0];\n",
    "    env.R_W = Matrix(UnitQuaternion(RotZ(-pi/2.0)*RotY(-pi/2.0)*RotX(pi)));\n",
    "\n",
    "    env.ω_B = [0.0; 0.0; 0.0];\n",
    "    env.wind_W = [0.0; 0.0; 0.0];\n",
    "    \n",
    "    env.num_waypoints = 3; # includig start point\n",
    "    env.waypoints = generate_trajectory(env.num_waypoints);\n",
    "    env.reached_goal = falses(env.num_waypoints);\n",
    "    env.reached_goal[1] = true; # set first point to reached (start point)\n",
    "    \n",
    "    env.current_point = 2;\n",
    "    env.reached_goal_in_step = false;\n",
    "    env.r_tol = 0.5;\n",
    "    \n",
    "    if env.visualization\n",
    "        visualize_waypoints(env.waypoints, 0.15); # debug: other radius?\n",
    "    end\n",
    "    \n",
    "\n",
    "    env.progress = 0.0;\n",
    "    env.progress_prev = 0.0;\n",
    "    \n",
    "    norm_dir = 1# norm(env.waypoints[2] - env.x_W)\n",
    "    \n",
    "    env.state = [env.x_W[1]; # position along x\n",
    "                 env.x_W[3]; # position along z\n",
    "        \n",
    "                 env.R_W[1,1]; # orientation along x\n",
    "                 env.R_W[3,1]; # orientation along z\n",
    "        \n",
    "                 env.v_B[1]; # velocity along x BODY coordinates\n",
    "                 env.v_B[2]; # velocity along y BODY coordinates  \n",
    "        \n",
    "                 env.ω_B[3]; # rotational velocity along z BODY coordinates\n",
    "        \n",
    "                 (env.waypoints[2][1] - env.x_W[1]) / norm_dir; # position error along x\n",
    "                 (env.waypoints[2][3] - env.x_W[3]) / norm_dir] # position error along z\n",
    "    \n",
    "    env.t = 0.0;\n",
    "    env.action = [0.0, 0.0];\n",
    "    env.done = false;\n",
    "    \n",
    "    nothing\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cf1a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines a methods for a callable object.\n",
    "# So when a VtolEnv object is created, it has this method that can be called\n",
    "function (env::VtolEnv)(a)\n",
    "    # TODO: set flaps later in 3D\n",
    "    # set the propeller trust and the two flaps 2D case\n",
    "    next_action = [a[1], a[2], 0.0, 0.0]\n",
    "   \n",
    "    _step!(env, next_action)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0e9eb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VtolEnv();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26a116cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "# 3 methods for callable object:<ul><li> (env::<b>VtolEnv</b>)(a) in Main at In[11]:3</li> <li> (env::<b>AbstractEnv</b>)(action) in ReinforcementLearningBase</li> <li> (env::<b>AbstractEnv</b>)(action, player) in ReinforcementLearningBase</li> </ul>"
      ],
      "text/plain": [
       "# 3 methods for callable object:\n",
       "[1] (env::VtolEnv)(a) in Main at In[11]:3\n",
       "[2] (env::AbstractEnv)(action) in ReinforcementLearningBase\n",
       "[3] (env::AbstractEnv)(action, player) in ReinforcementLearningBase"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methods(env) # Just to explain which methods the object has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e7d4727",
   "metadata": {},
   "outputs": [],
   "source": [
    "function _step!(env::VtolEnv, next_action)\n",
    "    # caluclate wind impact\n",
    "    v_in_wind_B = vtol_add_wind(env.v_B, env.R_W, env.wind_W)\n",
    "    # caluclate aerodynamic forces\n",
    "    torque_B, force_B = vtol_model(v_in_wind_B, next_action, eth_vtol_param);\n",
    "    # Limit to 2D\n",
    "    force_B[3] = 0.0; # Body Z\n",
    "    env.v_B[3] = 0.0;\n",
    "    torque_B[1] = 0.0; torque_B[2] = 0.0;  # Body X and Y\n",
    "    env.ω_B[1] = 0.0; env.ω_B[2] = 0.0;\n",
    "    # integrate rigid body dynamics for Δt\n",
    "    env.x_W, env.v_B, env.R_W, env.ω_B, time = rigid_body_simple(torque_B, force_B, env.x_W, env.v_B, env.R_W, env.ω_B, env.t, env.Δt, eth_vtol_param)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    env.reached_goal_in_step = false;\n",
    "    if norm(env.x_W - env.waypoints[env.current_point]) < env.r_tol\n",
    "        env.reached_goal_in_step = true;\n",
    "        env.reached_goal[env.current_point] = true;\n",
    "        env.current_point += 1;\n",
    "    end\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    # calculate progress on trajectory\n",
    "    env.progress_prev = env.progress\n",
    "    \n",
    "    current_progress = 0.0\n",
    "    line_segment, progress = calculate_progress(env.waypoints, env.x_W)\n",
    "    for i in 2:(line_segment)\n",
    "       current_progress +=  norm(env.waypoints[i] - env.waypoints[i - 1])  \n",
    "    end\n",
    "    current_progress += norm(env.waypoints[line_segment] - progress)\n",
    "    \n",
    "    env.progress = current_progress\n",
    "    \n",
    "    \n",
    "\n",
    "    if env.realtime\n",
    "        sleep(env.Δt) # TODO: just a dirty hack. this is of course slower than real time.\n",
    "    end\n",
    "\n",
    "    # Visualize the new state \n",
    "    if env.visualization\n",
    "        set_transform(env.name, env.x_W, QuatRotation(env.R_W));\n",
    "        set_actuators(env.name, next_action)\n",
    "        \n",
    "        for i in eachindex(env.reached_goal)\n",
    "            if env.reached_goal[i]\n",
    "                create_sphere(\"fixgoal_$i\", 0.2, color=RGBA{Float32}(1.0, 0.0, 0.0, 1.0));\n",
    "                set_transform(\"fixgoal_$i\", env.waypoints[i]);\n",
    "            end\n",
    "        end\n",
    "    end\n",
    " \n",
    "    env.t += env.Δt\n",
    "\n",
    "    \n",
    "    \n",
    "    env.state[1] = env.x_W[1]; # position along x\n",
    "    env.state[2] = env.x_W[3]; # position along z\n",
    "    \n",
    "    env.state[3] = env.R_W[1,1]; # orientation along x\n",
    "    env.state[4] = env.R_W[3,1]; # orientation along z\n",
    "    \n",
    "    env.state[5] = env.v_B[1]; # velocity along x BODY coordinates\n",
    "    env.state[6] = env.v_B[2]; # velocity along y BODY coordinates\n",
    "    \n",
    "    env.state[7] = env.ω_B[3];  # rotational velocity along z BODY coordinates\n",
    "    \n",
    "    \n",
    "    if env.current_point <= env.num_waypoints\n",
    "        norm_dir = 1 #norm(env.waypoints[env.current_point] - env.x_W)\n",
    "        env.state[8] = (env.waypoints[env.current_point][1] - env.x_W[1]) / norm_dir; # position error along x\n",
    "        env.state[9] = (env.waypoints[env.current_point][3] - env.x_W[3]) / norm_dir; # position error along z\n",
    "    else\n",
    "        env.state[8] = 0.0; # position error along x\n",
    "        env.state[9] = 0.0; # position error along z\n",
    "    end\n",
    "        \n",
    "    \n",
    "    # Termination criteria\n",
    "    # TODO: Use many termination criteria so that you do not train unnecessarily in wrong areas\n",
    "    env.done = #true\n",
    "        # After time... How fast is drone+Range of desired point\n",
    "        # After reaching position (circle of r_tol)\n",
    "        norm(env.ω_B) > 100.0 || \n",
    "        norm(env.v_B) > 100.0 || # stop if body is too fast # TODO: set higher later in fast training phase\n",
    "        env.x_W[3] < -5.0 || # stop if body is below -5m\n",
    "        env.t > env.num_waypoints * 5.0 ||# stop after 5s per point\n",
    "        norm(env.x_W - progress) > 2.0 || # too far off the path\n",
    "        env.current_point > env.num_waypoints # all points reached\n",
    "    nothing\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e1cd988",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[1mTest Summary:              | \u001b[22m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal  \u001b[22m\u001b[39m\u001b[0m\u001b[1mTime\u001b[22m\n",
      "random policy with VtolEnv | \u001b[32m2000  \u001b[39m\u001b[36m 2000  \u001b[39m\u001b[0m2.7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"random policy with VtolEnv\", Any[], 2000, false, false, true, 1.670350352711969e9, 1.67035035536765e9)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RLBase.test_runnable!(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c223a31f",
   "metadata": {},
   "source": [
    "Show an overview of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe6de74",
   "metadata": {},
   "source": [
    "# Setup of a reinforcement learning experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5683fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "MultiThreadEnv(8 x VtolEnv)"
      ],
      "text/plain": [
       "MultiThreadEnv(8 x VtolEnv)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 123    \n",
    "rng = StableRNG(seed)\n",
    "    N_ENV = 8\n",
    "    UPDATE_FREQ = 1024\n",
    "    \n",
    "    \n",
    "    # define multiple environments for parallel training\n",
    "    env = MultiThreadEnv([\n",
    "        # use different names for the visualization\n",
    "        VtolEnv(; rng = StableRNG(hash(seed+i)), name = \"vtol$i\") for i in 1:N_ENV\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1f128b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function approximator\n",
    "# TODO: change architecture eventually \n",
    "    ns, na = length(state(env[1])), length(action_space(env[1]))\n",
    "    approximator = ActorCritic(\n",
    "                actor = GaussianNetwork(\n",
    "                    pre = Chain(\n",
    "                    Dense(ns, 128, tanh; initW = glorot_uniform(rng)),#\n",
    "                    Dense(128, 128, tanh; initW = glorot_uniform(rng)),\n",
    "                    ),\n",
    "                    μ = Chain(Dense(128, na; initW = glorot_uniform(rng))),\n",
    "                    logσ = Chain(Dense(128, na; initW = glorot_uniform(rng))),\n",
    "                ),\n",
    "                critic = Chain(\n",
    "                    Dense(ns, 128, tanh; initW = glorot_uniform(rng)),\n",
    "                    Dense(128, 128, tanh; initW = glorot_uniform(rng)),\n",
    "                    Dense(128, 1; initW = glorot_uniform(rng)),\n",
    "                ),\n",
    "                optimizer = ADAM(1e-3),\n",
    "            );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ea4c37c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: The GPU function is being called but the GPU is not accessible. \n",
      "│ Defaulting back to the CPU. (No action is required if you want to run on the CPU).\n",
      "└ @ Flux /home/larissa/.julia/packages/Flux/7nTyc/src/functor.jl:187\n"
     ]
    }
   ],
   "source": [
    "    agent = Agent( # A wrapper of an AbstractPolicy\n",
    "        # AbstractPolicy: the policy to use\n",
    "        # TODO: change eventually\n",
    "        policy = PPOPolicy(;\n",
    "                    approximator = approximator |> gpu,\n",
    "                    update_freq=UPDATE_FREQ,\n",
    "                    dist = Normal,\n",
    "                    # For parameters visit the docu: https://juliareinforcementlearning.org/docs/rlzoo/#ReinforcementLearningZoo.PPOPolicy\n",
    "                    ),\n",
    "        \n",
    "        # AbstractTrajectory: used to store transitions between an agent and an environment source\n",
    "        trajectory = PPOTrajectory(;\n",
    "            capacity = UPDATE_FREQ,\n",
    "            state = Matrix{Float64} => (ns, N_ENV),\n",
    "            action = Matrix{Float64} => (na, N_ENV),\n",
    "            action_log_prob = Vector{Float64} => (N_ENV,),\n",
    "            reward = Vector{Float64} => (N_ENV,),\n",
    "            terminal = Vector{Bool} => (N_ENV,),\n",
    "        ),\n",
    "    );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f158a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "function saveModel(t, agent, env)\n",
    "    model = cpu(agent.policy.approximator)   \n",
    "    f = joinpath(\"./RL_models_l/\", \"vtol_2D_ppo_$t.bson\") # TODO: evtl anpassen\n",
    "    @save f model\n",
    "    println(\"parameters at step $t saved to $f\")\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07c5ad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "function loadModel()\n",
    "    f = joinpath(\"./RL_models_l/\", \"vtol_2D_ppo_trained_on_random_point.bson\") # TODO: evtl anpassen\n",
    "    @load f model\n",
    "    return model\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3c1858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "function validate_policy(t, agent, env)\n",
    "    run(agent.policy, test_env, StopAfterEpisode(1), episode_test_reward_hook)\n",
    "    # the result of the hook\n",
    "    println(\"test reward at step $t: $(episode_test_reward_hook.rewards[end])\")\n",
    "    \n",
    "end;\n",
    "\n",
    "episode_test_reward_hook = TotalRewardPerEpisode(;is_display_on_exit=false)\n",
    "# create a env only for reward test\n",
    "test_env = VtolEnv(;name = \"testVTOL\", visualization = true, realtime = true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdc71a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.policy.approximator = loadModel(); # TODO: un/comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb737010",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   1%|▎                                        |  ETA: 3:50:50\u001b[39m[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 10000: 48.92600054824201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   1%|▌                                        |  ETA: 2:11:29\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 20000: 51.7232011925365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   2%|▊                                        |  ETA: 1:44:42\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 30000: 49.008092562083235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   3%|█▏                                       |  ETA: 1:29:15\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 40000: 63.23140930093604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   3%|█▍                                       |  ETA: 1:20:08\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 50000: 42.11727758292815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   4%|█▋                                       |  ETA: 1:12:24\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 60000: 67.64387464535923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   5%|█▉                                       |  ETA: 1:07:46\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 70000: 56.01234408385234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   5%|██▏                                      |  ETA: 1:03:43\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 80000: 48.22069184780675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   6%|██▍                                      |  ETA: 0:59:49\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 90000: 44.09782525545844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   7%|██▊                                      |  ETA: 0:56:39\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 100000 saved to ./RL_models_l/vtol_2D_ppo_100000.bson\n",
      "test reward at step 100000: 48.0580761160821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   7%|███                                      |  ETA: 0:55:03\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 110000: 70.2346330970153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   8%|███▎                                     |  ETA: 0:52:58\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 120000: 73.64486463290638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   9%|███▌                                     |  ETA: 0:50:22\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 130000: 44.677857887741844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   9%|███▉                                     |  ETA: 0:48:39\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 140000: 71.01370222600521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  10%|████                                     |  ETA: 0:47:36\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 150000: 56.26017569769572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  11%|████▍                                    |  ETA: 0:45:43\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 160000: 53.54051892077932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  11%|████▋                                    |  ETA: 0:44:30\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 170000: 53.09382254701117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  12%|████▉                                    |  ETA: 0:43:34\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 180000: 77.60285580104869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  13%|█████▏                                   |  ETA: 0:42:28\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 190000: 61.102986172171896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  13%|█████▍                                   |  ETA: 0:41:43\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 200000 saved to ./RL_models_l/vtol_2D_ppo_200000.bson\n",
      "test reward at step 200000: 53.22604704889958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  14%|█████▊                                   |  ETA: 0:40:33\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 210000: 62.979383043467855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  15%|██████                                   |  ETA: 0:39:44\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 220000: 55.32955228446561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  15%|██████▎                                  |  ETA: 0:39:08\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 230000: 63.64124854990081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  16%|██████▌                                  |  ETA: 0:38:14\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 240000: 57.62725321983762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  17%|██████▊                                  |  ETA: 0:37:27\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 250000: 67.44440471173115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  17%|███████▏                                 |  ETA: 0:36:37\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 260000: 65.43231477811877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  18%|███████▍                                 |  ETA: 0:36:02\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 270000: 61.02188669219684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  19%|███████▋                                 |  ETA: 0:35:21\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 280000: 48.13352588316063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  19%|███████▉                                 |  ETA: 0:34:44\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 290000: 48.80224089604383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  20%|████████▏                                |  ETA: 0:34:09\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 300000 saved to ./RL_models_l/vtol_2D_ppo_300000.bson\n",
      "test reward at step 300000: 42.269105753373424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  21%|████████▌                                |  ETA: 0:33:35\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 310000: 59.276303813337876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  21%|████████▊                                |  ETA: 0:33:12\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 320000: 64.49802479472207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  22%|█████████                                |  ETA: 0:32:38\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 330000: 63.248028193878014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  23%|█████████▎                               |  ETA: 0:32:07\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 340000: 62.045859731390834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  23%|█████████▌                               |  ETA: 0:31:40\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 350000: 64.91235385542237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  24%|█████████▉                               |  ETA: 0:31:10\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 360000: 57.25047137631439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  25%|██████████▏                              |  ETA: 0:30:48\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 370000: 70.94958780023215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  25%|██████████▍                              |  ETA: 0:30:21\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 380000: 67.1469132712134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  26%|██████████▋                              |  ETA: 0:30:00\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 390000: 63.046805934876204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  27%|██████████▉                              |  ETA: 0:29:29\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 400000 saved to ./RL_models_l/vtol_2D_ppo_400000.bson\n",
      "test reward at step 400000: 59.91268132020651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  27%|███████████▏                             |  ETA: 0:29:10\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 410000: 73.21404527635673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  28%|███████████▌                             |  ETA: 0:28:45\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 420000: 48.99750424338873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  29%|███████████▊                             |  ETA: 0:28:20\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 430000: 44.60817944657768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  29%|████████████                             |  ETA: 0:27:57\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 440000: 74.70420127319096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  30%|████████████▎                            |  ETA: 0:27:35\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 450000: 65.54701972857127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  31%|████████████▌                            |  ETA: 0:27:15\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 460000: 67.25205648448271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  31%|████████████▊                            |  ETA: 0:26:58\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 470000: 59.759322463730776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  32%|█████████████▏                           |  ETA: 0:26:36\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 480000: 51.83585606308736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  33%|█████████████▍                           |  ETA: 0:26:10\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 490000: 41.973378489230775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  33%|█████████████▋                           |  ETA: 0:25:47\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 500000 saved to ./RL_models_l/vtol_2D_ppo_500000.bson\n",
      "test reward at step 500000: 60.078269331715205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  34%|█████████████▉                           |  ETA: 0:25:29\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 510000: 43.667192152885505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  35%|██████████████▎                          |  ETA: 0:25:09\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 520000: 69.78718203959505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  35%|██████████████▌                          |  ETA: 0:24:50\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 530000: 63.83010116773638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  36%|██████████████▊                          |  ETA: 0:24:30\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 540000: 57.39860730922856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  37%|███████████████                          |  ETA: 0:24:06\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 550000: 65.30413443908691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  37%|███████████████▎                         |  ETA: 0:23:48\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 560000: 50.01829954547022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  38%|███████████████▌                         |  ETA: 0:23:30\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 570000: 79.4167942537187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  39%|███████████████▉                         |  ETA: 0:23:12\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 580000: 60.08452721828997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  39%|████████████████▏                        |  ETA: 0:22:58\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 590000: 65.28742613003946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  40%|████████████████▍                        |  ETA: 0:22:37\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 600000 saved to ./RL_models_l/vtol_2D_ppo_600000.bson\n",
      "test reward at step 600000: 59.24308658211132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  41%|████████████████▋                        |  ETA: 0:22:19\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 610000: 56.44254515160088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  41%|████████████████▉                        |  ETA: 0:22:03\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 620000: 65.92113529327919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  42%|█████████████████▏                       |  ETA: 0:21:48\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 630000: 72.20691624788554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  43%|█████████████████▌                       |  ETA: 0:21:28\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 640000: 48.45890535104811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  43%|█████████████████▊                       |  ETA: 0:21:10\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 650000: 41.50435297949541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  44%|██████████████████                       |  ETA: 0:20:51\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 660000: 60.43246463849311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  45%|██████████████████▎                      |  ETA: 0:20:34\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 670000: 36.99746705245463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  45%|██████████████████▋                      |  ETA: 0:20:18\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 680000: 60.259585551324754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  46%|██████████████████▉                      |  ETA: 0:20:02\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 690000: 59.2129896003894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  47%|███████████████████▏                     |  ETA: 0:19:45\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 700000 saved to ./RL_models_l/vtol_2D_ppo_700000.bson\n",
      "test reward at step 700000: 42.412358695869244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  47%|███████████████████▍                     |  ETA: 0:19:28\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 710000: 51.975918891445374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  48%|███████████████████▋                     |  ETA: 0:19:11\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 720000: 52.056883967591254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  48%|███████████████████▊                     |  ETA: 0:19:10\u001b[39m"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      "  [1] iterate",
      "    @ ./multidimensional.jl:399 [inlined]",
      "  [2] copy",
      "    @ ./broadcast.jl:892 [inlined]",
      "  [3] materialize",
      "    @ ./broadcast.jl:860 [inlined]",
      "  [4] adjoint",
      "    @ ~/.julia/packages/Zygote/PD12J/src/lib/broadcast.jl:110 [inlined]",
      "  [5] _pullback",
      "    @ ~/.julia/packages/ZygoteRules/AIbCs/src/adjoint.jl:65 [inlined]",
      "  [6] _pullback",
      "    @ ~/.julia/packages/Flux/7nTyc/src/layers/basic.jl:158 [inlined]",
      "  [7] _pullback(ctx::Zygote.Context{true}, f::Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, args::Matrix{Float64})",
      "    @ Zygote ~/.julia/packages/Zygote/PD12J/src/compiler/interface2.jl:0",
      "  [8] _pullback",
      "    @ ~/.julia/packages/Flux/7nTyc/src/layers/basic.jl:47 [inlined]",
      "  [9] _pullback(::Zygote.Context{true}, ::typeof(Flux.applychain), ::Tuple{Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}, ::Matrix{Float64})",
      "    @ Zygote ~/.julia/packages/Zygote/PD12J/src/compiler/interface2.jl:0",
      " [10] _pullback",
      "    @ ~/.julia/packages/Flux/7nTyc/src/layers/basic.jl:47 [inlined]",
      " [11] _pullback(::Zygote.Context{true}, ::typeof(Flux.applychain), ::Tuple{Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}, ::Matrix{Float64})",
      "    @ Zygote ~/.julia/packages/Zygote/PD12J/src/compiler/interface2.jl:0",
      " [12] _pullback",
      "    @ ~/.julia/packages/Flux/7nTyc/src/layers/basic.jl:49 [inlined]",
      " [13] _pullback(ctx::Zygote.Context{true}, f::Chain{Tuple{Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, args::Matrix{Float64})",
      "    @ Zygote ~/.julia/packages/Zygote/PD12J/src/compiler/interface2.jl:0",
      " [14] _pullback",
      "    @ ~/.julia/packages/ReinforcementLearningZoo/tvfq9/src/algorithms/policy_gradient/ppo.jl:288 [inlined]",
      " [15] _pullback(::Zygote.Context{true}, ::ReinforcementLearningZoo.var\"#178#181\"{PPOPolicy{ActorCritic{GaussianNetwork{Chain{Tuple{Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, typeof(tanh)}, Chain{Tuple{Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, ADAM}, Normal, Random._GLOBAL_RNG}, Float32, Float32, Float32, Float32, ActorCritic{GaussianNetwork{Chain{Tuple{Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, typeof(tanh)}, Chain{Tuple{Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, ADAM}, Int64, Int64, Vector{Float64}, Vector{Float64}, Vector{Float64}, Matrix{Float64}})",
      "    @ Zygote ~/.julia/packages/Zygote/PD12J/src/compiler/interface2.jl:0",
      " [16] pullback(f::Function, ps::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}})",
      "    @ Zygote ~/.julia/packages/Zygote/PD12J/src/compiler/interface.jl:373",
      " [17] gradient(f::Function, args::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}})",
      "    @ Zygote ~/.julia/packages/Zygote/PD12J/src/compiler/interface.jl:96",
      " [18] _update!(p::PPOPolicy{ActorCritic{GaussianNetwork{Chain{Tuple{Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, typeof(tanh)}, Chain{Tuple{Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, ADAM}, Normal, Random._GLOBAL_RNG}, t::Trajectory{NamedTuple{(:action_log_prob, :state, :action, :reward, :terminal), Tuple{CircularArrayBuffers.CircularArrayBuffer{Float64, 2, Matrix{Float64}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 3, Array{Float64, 3}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 3, Array{Float64, 3}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 2, Matrix{Float64}}, CircularArrayBuffers.CircularArrayBuffer{Bool, 2, Matrix{Bool}}}}})",
      "    @ ReinforcementLearningZoo ~/.julia/packages/ReinforcementLearningZoo/tvfq9/src/algorithms/policy_gradient/ppo.jl:287",
      " [19] update!",
      "    @ ~/.julia/packages/ReinforcementLearningZoo/tvfq9/src/algorithms/policy_gradient/ppo.jl:210 [inlined]",
      " [20] (::Agent{PPOPolicy{ActorCritic{GaussianNetwork{Chain{Tuple{Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, typeof(tanh)}, Chain{Tuple{Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, ADAM}, Normal, Random._GLOBAL_RNG}, Trajectory{NamedTuple{(:action_log_prob, :state, :action, :reward, :terminal), Tuple{CircularArrayBuffers.CircularArrayBuffer{Float64, 2, Matrix{Float64}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 3, Array{Float64, 3}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 3, Array{Float64, 3}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 2, Matrix{Float64}}, CircularArrayBuffers.CircularArrayBuffer{Bool, 2, Matrix{Bool}}}}}})(stage::PreActStage, env::MultiThreadEnv{VtolEnv{Space{Vector{ClosedInterval{Float64}}}, Float64, Vector{Float64}, StableRNGs.LehmerRNG}, Matrix{Float64}, Vector{Float64}, Space{Matrix{ClosedInterval{Float64}}}, Space{Matrix{ClosedInterval{Float64}}}, Nothing}, action::ReinforcementLearningZoo.EnrichedAction{Matrix{Float64}, NamedTuple{(:action_log_prob,), Tuple{Vector{Float64}}}})",
      "    @ ReinforcementLearningCore ~/.julia/packages/ReinforcementLearningCore/yeRLW/src/policies/agents/agent.jl:78",
      " [21] _run(policy::Agent{PPOPolicy{ActorCritic{GaussianNetwork{Chain{Tuple{Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, typeof(tanh)}, Chain{Tuple{Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, ADAM}, Normal, Random._GLOBAL_RNG}, Trajectory{NamedTuple{(:action_log_prob, :state, :action, :reward, :terminal), Tuple{CircularArrayBuffers.CircularArrayBuffer{Float64, 2, Matrix{Float64}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 3, Array{Float64, 3}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 3, Array{Float64, 3}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 2, Matrix{Float64}}, CircularArrayBuffers.CircularArrayBuffer{Bool, 2, Matrix{Bool}}}}}}, env::MultiThreadEnv{VtolEnv{Space{Vector{ClosedInterval{Float64}}}, Float64, Vector{Float64}, StableRNGs.LehmerRNG}, Matrix{Float64}, Vector{Float64}, Space{Matrix{ClosedInterval{Float64}}}, Space{Matrix{ClosedInterval{Float64}}}, Nothing}, stop_condition::StopAfterStep{ProgressMeter.Progress}, hook::ComposedHook{Tuple{DoEveryNStep{typeof(saveModel)}, DoEveryNStep{typeof(validate_policy)}}})",
      "    @ ReinforcementLearningZoo ~/.julia/packages/ReinforcementLearningZoo/tvfq9/src/algorithms/policy_gradient/run.jl:18",
      " [22] run(policy::Agent{PPOPolicy{ActorCritic{GaussianNetwork{Chain{Tuple{Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, typeof(tanh)}, Chain{Tuple{Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, ADAM}, Normal, Random._GLOBAL_RNG}, Trajectory{NamedTuple{(:action_log_prob, :state, :action, :reward, :terminal), Tuple{CircularArrayBuffers.CircularArrayBuffer{Float64, 2, Matrix{Float64}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 3, Array{Float64, 3}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 3, Array{Float64, 3}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 2, Matrix{Float64}}, CircularArrayBuffers.CircularArrayBuffer{Bool, 2, Matrix{Bool}}}}}}, env::MultiThreadEnv{VtolEnv{Space{Vector{ClosedInterval{Float64}}}, Float64, Vector{Float64}, StableRNGs.LehmerRNG}, Matrix{Float64}, Vector{Float64}, Space{Matrix{ClosedInterval{Float64}}}, Space{Matrix{ClosedInterval{Float64}}}, Nothing}, stop_condition::StopAfterStep{ProgressMeter.Progress}, hook::ComposedHook{Tuple{DoEveryNStep{typeof(saveModel)}, DoEveryNStep{typeof(validate_policy)}}})",
      "    @ ReinforcementLearningCore ~/.julia/packages/ReinforcementLearningCore/yeRLW/src/core/run.jl:10",
      " [23] top-level scope",
      "    @ In[23]:1",
      " [24] eval",
      "    @ ./boot.jl:368 [inlined]",
      " [25] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1428"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: both Losses and NNlib export \"ctc_loss\"; uses of it in module Flux must be qualified\n"
     ]
    }
   ],
   "source": [
    "ReinforcementLearning.run(\n",
    "    agent,\n",
    "    env,\n",
    "    StopAfterStep(1_500_000),\n",
    "    ComposedHook(\n",
    "        DoEveryNStep(saveModel, n=100_000), \n",
    "        DoEveryNStep(validate_policy, n=10_000)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49a302a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip080\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip080)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip081\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip080)\" d=\"\n",
       "M141.853 1486.45 L2352.76 1486.45 L2352.76 47.2441 L141.853 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip082\">\n",
       "    <rect x=\"141\" y=\"47\" width=\"2212\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  175.049,1486.45 175.049,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  762.586,1486.45 762.586,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1350.12,1486.45 1350.12,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1937.66,1486.45 1937.66,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  141.853,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  175.049,1486.45 175.049,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  762.586,1486.45 762.586,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1350.12,1486.45 1350.12,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1937.66,1486.45 1937.66,1467.55 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip080)\" d=\"M175.049 1517.37 Q171.438 1517.37 169.609 1520.93 Q167.803 1524.47 167.803 1531.6 Q167.803 1538.71 169.609 1542.27 Q171.438 1545.82 175.049 1545.82 Q178.683 1545.82 180.489 1542.27 Q182.317 1538.71 182.317 1531.6 Q182.317 1524.47 180.489 1520.93 Q178.683 1517.37 175.049 1517.37 M175.049 1513.66 Q180.859 1513.66 183.914 1518.27 Q186.993 1522.85 186.993 1531.6 Q186.993 1540.33 183.914 1544.94 Q180.859 1549.52 175.049 1549.52 Q169.239 1549.52 166.16 1544.94 Q163.104 1540.33 163.104 1531.6 Q163.104 1522.85 166.16 1518.27 Q169.239 1513.66 175.049 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M741.359 1544.91 L757.679 1544.91 L757.679 1548.85 L735.734 1548.85 L735.734 1544.91 Q738.396 1542.16 742.98 1537.53 Q747.586 1532.88 748.767 1531.53 Q751.012 1529.01 751.892 1527.27 Q752.795 1525.51 752.795 1523.82 Q752.795 1521.07 750.85 1519.33 Q748.929 1517.6 745.827 1517.6 Q743.628 1517.6 741.174 1518.36 Q738.744 1519.13 735.966 1520.68 L735.966 1515.95 Q738.79 1514.82 741.244 1514.24 Q743.697 1513.66 745.734 1513.66 Q751.105 1513.66 754.299 1516.35 Q757.494 1519.03 757.494 1523.52 Q757.494 1525.65 756.683 1527.57 Q755.896 1529.47 753.79 1532.07 Q753.211 1532.74 750.109 1535.95 Q747.008 1539.15 741.359 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M777.493 1517.37 Q773.882 1517.37 772.054 1520.93 Q770.248 1524.47 770.248 1531.6 Q770.248 1538.71 772.054 1542.27 Q773.882 1545.82 777.493 1545.82 Q781.128 1545.82 782.933 1542.27 Q784.762 1538.71 784.762 1531.6 Q784.762 1524.47 782.933 1520.93 Q781.128 1517.37 777.493 1517.37 M777.493 1513.66 Q783.304 1513.66 786.359 1518.27 Q789.438 1522.85 789.438 1531.6 Q789.438 1540.33 786.359 1544.94 Q783.304 1549.52 777.493 1549.52 Q771.683 1549.52 768.605 1544.94 Q765.549 1540.33 765.549 1531.6 Q765.549 1522.85 768.605 1518.27 Q771.683 1513.66 777.493 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M1338.29 1518.36 L1326.49 1536.81 L1338.29 1536.81 L1338.29 1518.36 M1337.07 1514.29 L1342.95 1514.29 L1342.95 1536.81 L1347.88 1536.81 L1347.88 1540.7 L1342.95 1540.7 L1342.95 1548.85 L1338.29 1548.85 L1338.29 1540.7 L1322.69 1540.7 L1322.69 1536.19 L1337.07 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M1365.61 1517.37 Q1362 1517.37 1360.17 1520.93 Q1358.36 1524.47 1358.36 1531.6 Q1358.36 1538.71 1360.17 1542.27 Q1362 1545.82 1365.61 1545.82 Q1369.24 1545.82 1371.05 1542.27 Q1372.88 1538.71 1372.88 1531.6 Q1372.88 1524.47 1371.05 1520.93 Q1369.24 1517.37 1365.61 1517.37 M1365.61 1513.66 Q1371.42 1513.66 1374.48 1518.27 Q1377.55 1522.85 1377.55 1531.6 Q1377.55 1540.33 1374.48 1544.94 Q1371.42 1549.52 1365.61 1549.52 Q1359.8 1549.52 1356.72 1544.94 Q1353.67 1540.33 1353.67 1531.6 Q1353.67 1522.85 1356.72 1518.27 Q1359.8 1513.66 1365.61 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M1923.07 1529.7 Q1919.92 1529.7 1918.07 1531.86 Q1916.24 1534.01 1916.24 1537.76 Q1916.24 1541.49 1918.07 1543.66 Q1919.92 1545.82 1923.07 1545.82 Q1926.21 1545.82 1928.04 1543.66 Q1929.89 1541.49 1929.89 1537.76 Q1929.89 1534.01 1928.04 1531.86 Q1926.21 1529.7 1923.07 1529.7 M1932.35 1515.05 L1932.35 1519.31 Q1930.59 1518.48 1928.78 1518.04 Q1927 1517.6 1925.24 1517.6 Q1920.61 1517.6 1918.16 1520.72 Q1915.73 1523.85 1915.38 1530.17 Q1916.75 1528.15 1918.81 1527.09 Q1920.87 1526 1923.34 1526 Q1928.55 1526 1931.56 1529.17 Q1934.59 1532.32 1934.59 1537.76 Q1934.59 1543.08 1931.45 1546.3 Q1928.3 1549.52 1923.07 1549.52 Q1917.07 1549.52 1913.9 1544.94 Q1910.73 1540.33 1910.73 1531.6 Q1910.73 1523.41 1914.62 1518.55 Q1918.51 1513.66 1925.06 1513.66 Q1926.82 1513.66 1928.6 1514.01 Q1930.4 1514.36 1932.35 1515.05 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M1952.65 1517.37 Q1949.04 1517.37 1947.21 1520.93 Q1945.4 1524.47 1945.4 1531.6 Q1945.4 1538.71 1947.21 1542.27 Q1949.04 1545.82 1952.65 1545.82 Q1956.28 1545.82 1958.09 1542.27 Q1959.92 1538.71 1959.92 1531.6 Q1959.92 1524.47 1958.09 1520.93 Q1956.28 1517.37 1952.65 1517.37 M1952.65 1513.66 Q1958.46 1513.66 1961.51 1518.27 Q1964.59 1522.85 1964.59 1531.6 Q1964.59 1540.33 1961.51 1544.94 Q1958.46 1549.52 1952.65 1549.52 Q1946.84 1549.52 1943.76 1544.94 Q1940.7 1540.33 1940.7 1531.6 Q1940.7 1522.85 1943.76 1518.27 Q1946.84 1513.66 1952.65 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  141.853,1349.61 2352.76,1349.61 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  141.853,1029.54 2352.76,1029.54 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  141.853,709.461 2352.76,709.461 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  141.853,389.385 2352.76,389.385 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  141.853,69.3093 2352.76,69.3093 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  141.853,1486.45 141.853,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  141.853,1349.61 160.751,1349.61 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  141.853,1029.54 160.751,1029.54 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  141.853,709.461 160.751,709.461 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  141.853,389.385 160.751,389.385 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  141.853,69.3093 160.751,69.3093 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip080)\" d=\"M66.5939 1336.41 L54.7884 1354.85 L66.5939 1354.85 L66.5939 1336.41 M65.367 1332.33 L71.2466 1332.33 L71.2466 1354.85 L76.1772 1354.85 L76.1772 1358.74 L71.2466 1358.74 L71.2466 1366.89 L66.5939 1366.89 L66.5939 1358.74 L50.9921 1358.74 L50.9921 1354.23 L65.367 1332.33 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M93.9086 1335.41 Q90.2975 1335.41 88.4688 1338.98 Q86.6632 1342.52 86.6632 1349.65 Q86.6632 1356.75 88.4688 1360.32 Q90.2975 1363.86 93.9086 1363.86 Q97.5428 1363.86 99.3483 1360.32 Q101.177 1356.75 101.177 1349.65 Q101.177 1342.52 99.3483 1338.98 Q97.5428 1335.41 93.9086 1335.41 M93.9086 1331.71 Q99.7187 1331.71 102.774 1336.31 Q105.853 1340.9 105.853 1349.65 Q105.853 1358.37 102.774 1362.98 Q99.7187 1367.56 93.9086 1367.56 Q88.0984 1367.56 85.0197 1362.98 Q81.9642 1358.37 81.9642 1349.65 Q81.9642 1340.9 85.0197 1336.31 Q88.0984 1331.71 93.9086 1331.71 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M53.793 1012.26 L72.1494 1012.26 L72.1494 1016.19 L58.0754 1016.19 L58.0754 1024.66 Q59.0939 1024.32 60.1124 1024.15 Q61.131 1023.97 62.1495 1023.97 Q67.9365 1023.97 71.3161 1027.14 Q74.6957 1030.31 74.6957 1035.73 Q74.6957 1041.31 71.2235 1044.41 Q67.7513 1047.49 61.4319 1047.49 Q59.256 1047.49 56.9875 1047.12 Q54.7421 1046.75 52.3347 1046.01 L52.3347 1041.31 Q54.418 1042.44 56.6402 1043 Q58.8625 1043.55 61.3393 1043.55 Q65.3439 1043.55 67.6819 1041.45 Q70.0198 1039.34 70.0198 1035.73 Q70.0198 1032.12 67.6819 1030.01 Q65.3439 1027.9 61.3393 1027.9 Q59.4643 1027.9 57.5893 1028.32 Q55.7375 1028.74 53.793 1029.62 L53.793 1012.26 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M93.9086 1015.33 Q90.2975 1015.33 88.4688 1018.9 Q86.6632 1022.44 86.6632 1029.57 Q86.6632 1036.68 88.4688 1040.24 Q90.2975 1043.78 93.9086 1043.78 Q97.5428 1043.78 99.3483 1040.24 Q101.177 1036.68 101.177 1029.57 Q101.177 1022.44 99.3483 1018.9 Q97.5428 1015.33 93.9086 1015.33 M93.9086 1011.63 Q99.7187 1011.63 102.774 1016.24 Q105.853 1020.82 105.853 1029.57 Q105.853 1038.3 102.774 1042.9 Q99.7187 1047.49 93.9086 1047.49 Q88.0984 1047.49 85.0197 1042.9 Q81.9642 1038.3 81.9642 1029.57 Q81.9642 1020.82 85.0197 1016.24 Q88.0984 1011.63 93.9086 1011.63 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M64.3254 707.597 Q61.1773 707.597 59.3254 709.75 Q57.4967 711.903 57.4967 715.653 Q57.4967 719.38 59.3254 721.555 Q61.1773 723.708 64.3254 723.708 Q67.4735 723.708 69.3022 721.555 Q71.1541 719.38 71.1541 715.653 Q71.1541 711.903 69.3022 709.75 Q67.4735 707.597 64.3254 707.597 M73.6077 692.944 L73.6077 697.204 Q71.8485 696.37 70.0429 695.931 Q68.2606 695.491 66.5013 695.491 Q61.8717 695.491 59.418 698.616 Q56.9875 701.741 56.6402 708.06 Q58.006 706.046 60.0662 704.981 Q62.1263 703.894 64.6032 703.894 Q69.8115 703.894 72.8207 707.065 Q75.8531 710.213 75.8531 715.653 Q75.8531 720.977 72.705 724.194 Q69.5568 727.412 64.3254 727.412 Q58.33 727.412 55.1588 722.829 Q51.9875 718.222 51.9875 709.495 Q51.9875 701.301 55.8764 696.44 Q59.7652 691.556 66.3161 691.556 Q68.0754 691.556 69.8578 691.903 Q71.6633 692.25 73.6077 692.944 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M93.9086 695.259 Q90.2975 695.259 88.4688 698.824 Q86.6632 702.366 86.6632 709.495 Q86.6632 716.602 88.4688 720.167 Q90.2975 723.708 93.9086 723.708 Q97.5428 723.708 99.3483 720.167 Q101.177 716.602 101.177 709.495 Q101.177 702.366 99.3483 698.824 Q97.5428 695.259 93.9086 695.259 M93.9086 691.556 Q99.7187 691.556 102.774 696.162 Q105.853 700.745 105.853 709.495 Q105.853 718.222 102.774 722.829 Q99.7187 727.412 93.9086 727.412 Q88.0984 727.412 85.0197 722.829 Q81.9642 718.222 81.9642 709.495 Q81.9642 700.745 85.0197 696.162 Q88.0984 691.556 93.9086 691.556 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M52.5662 372.105 L74.7883 372.105 L74.7883 374.096 L62.2421 406.665 L57.3578 406.665 L69.1633 376.04 L52.5662 376.04 L52.5662 372.105 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M93.9086 375.184 Q90.2975 375.184 88.4688 378.748 Q86.6632 382.29 86.6632 389.42 Q86.6632 396.526 88.4688 400.091 Q90.2975 403.633 93.9086 403.633 Q97.5428 403.633 99.3483 400.091 Q101.177 396.526 101.177 389.42 Q101.177 382.29 99.3483 378.748 Q97.5428 375.184 93.9086 375.184 M93.9086 371.48 Q99.7187 371.48 102.774 376.086 Q105.853 380.67 105.853 389.42 Q105.853 398.146 102.774 402.753 Q99.7187 407.336 93.9086 407.336 Q88.0984 407.336 85.0197 402.753 Q81.9642 398.146 81.9642 389.42 Q81.9642 380.67 85.0197 376.086 Q88.0984 371.48 93.9086 371.48 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M63.7467 70.1773 Q60.4134 70.1773 58.4921 71.9597 Q56.5939 73.7421 56.5939 76.8671 Q56.5939 79.9921 58.4921 81.7745 Q60.4134 83.5569 63.7467 83.5569 Q67.08 83.5569 69.0013 81.7745 Q70.9226 79.9689 70.9226 76.8671 Q70.9226 73.7421 69.0013 71.9597 Q67.1032 70.1773 63.7467 70.1773 M59.0708 68.1866 Q56.0615 67.4459 54.3717 65.3857 Q52.7051 63.3255 52.7051 60.3626 Q52.7051 56.2191 55.6449 53.8117 Q58.6078 51.4043 63.7467 51.4043 Q68.9087 51.4043 71.8485 53.8117 Q74.7883 56.2191 74.7883 60.3626 Q74.7883 63.3255 73.0985 65.3857 Q71.4318 67.4459 68.4457 68.1866 Q71.8253 68.9736 73.7003 71.2653 Q75.5985 73.5569 75.5985 76.8671 Q75.5985 81.8902 72.5198 84.5754 Q69.4642 87.2606 63.7467 87.2606 Q58.0291 87.2606 54.9504 84.5754 Q51.8949 81.8902 51.8949 76.8671 Q51.8949 73.5569 53.793 71.2653 Q55.6912 68.9736 59.0708 68.1866 M57.3578 60.8024 Q57.3578 63.4876 59.0245 64.9922 Q60.7143 66.4968 63.7467 66.4968 Q66.7559 66.4968 68.4457 64.9922 Q70.1587 63.4876 70.1587 60.8024 Q70.1587 58.1172 68.4457 56.6126 Q66.7559 55.108 63.7467 55.108 Q60.7143 55.108 59.0245 56.6126 Q57.3578 58.1172 57.3578 60.8024 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M93.9086 55.108 Q90.2975 55.108 88.4688 58.6728 Q86.6632 62.2144 86.6632 69.344 Q86.6632 76.4504 88.4688 80.0152 Q90.2975 83.5569 93.9086 83.5569 Q97.5428 83.5569 99.3483 80.0152 Q101.177 76.4504 101.177 69.344 Q101.177 62.2144 99.3483 58.6728 Q97.5428 55.108 93.9086 55.108 M93.9086 51.4043 Q99.7187 51.4043 102.774 56.0107 Q105.853 60.594 105.853 69.344 Q105.853 78.0708 102.774 82.6773 Q99.7187 87.2606 93.9086 87.2606 Q88.0984 87.2606 85.0197 82.6773 Q81.9642 78.0708 81.9642 69.344 Q81.9642 60.594 85.0197 56.0107 Q88.0984 51.4043 93.9086 51.4043 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip082)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  204.426,1063.91 233.803,974.381 263.179,1061.28 292.556,606.031 321.933,1281.84 351.31,464.799 380.687,837.096 410.064,1086.49 439.441,1218.45 468.817,1091.69 \n",
       "  498.194,381.875 527.571,272.722 556.948,1199.89 586.325,356.939 615.702,829.163 645.079,916.213 674.456,930.511 703.832,146.036 733.209,674.157 762.586,926.278 \n",
       "  791.963,614.098 821.34,858.95 850.717,592.913 880.094,785.406 909.47,471.183 938.847,535.585 968.224,676.752 997.601,1089.28 1026.98,1067.87 1056.35,1276.98 \n",
       "  1085.73,732.624 1115.11,565.49 1144.49,605.499 1173.86,643.978 1203.24,552.228 1232.62,797.466 1261.99,358.991 1291.37,480.705 1320.75,611.94 1350.12,712.255 \n",
       "  1379.5,286.511 1408.88,1061.62 1438.25,1202.12 1467.63,238.815 1497.01,531.914 1526.38,477.34 1555.76,717.164 1585.14,970.775 1614.52,1286.45 1643.89,706.955 \n",
       "  1673.27,1232.23 1702.65,396.197 1732.02,586.868 1761.4,792.725 1790.78,539.688 1820.15,1028.95 1849.53,87.9763 1878.91,706.755 1908.28,540.223 1937.66,733.688 \n",
       "  1967.04,823.326 1996.41,519.939 2025.79,318.747 2055.17,1078.86 2084.55,1301.46 2113.92,695.618 2143.3,1445.72 2172.68,701.152 2202.05,734.651 2231.43,1272.4 \n",
       "  2260.81,966.292 2290.18,963.7 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip080)\" d=\"\n",
       "M1980.97 198.898 L2279.06 198.898 L2279.06 95.2176 L1980.97 95.2176  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1980.97,198.898 2279.06,198.898 2279.06,95.2176 1980.97,95.2176 1980.97,198.898 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2005.54,147.058 2152.93,147.058 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip080)\" d=\"M2191.34 166.745 Q2189.53 171.375 2187.82 172.787 Q2186.11 174.199 2183.24 174.199 L2179.84 174.199 L2179.84 170.634 L2182.34 170.634 Q2184.09 170.634 2185.07 169.8 Q2186.04 168.967 2187.22 165.865 L2187.98 163.921 L2177.5 138.412 L2182.01 138.412 L2190.11 158.689 L2198.22 138.412 L2202.73 138.412 L2191.34 166.745 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M2210.02 160.402 L2217.66 160.402 L2217.66 134.037 L2209.35 135.703 L2209.35 131.444 L2217.61 129.778 L2222.29 129.778 L2222.29 160.402 L2229.93 160.402 L2229.93 164.338 L2210.02 164.338 L2210.02 160.402 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(episode_test_reward_hook.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb0d4ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "close_visualization(); # closes the MeshCat visualization"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
