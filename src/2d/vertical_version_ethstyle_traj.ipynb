{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0649b76a",
   "metadata": {},
   "source": [
    "# Init Bionic VTOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96751412",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../Flyonic.jl\");\n",
    "using .Flyonic;\n",
    "\n",
    "using Rotations; # used for initial position\n",
    "\n",
    "using ReinforcementLearning;\n",
    "using StableRNGs;\n",
    "using Flux;\n",
    "using Flux.Losses;\n",
    "using Random;\n",
    "using IntervalSets;\n",
    "using LinearAlgebra;\n",
    "using Distributions;\n",
    "\n",
    "using Plots;\n",
    "using Statistics;\n",
    "\n",
    "using BSON: @save, @load # save mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7e4ee64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: MeshCat server started. You can open the visualizer by visiting the following URL in your browser:\n",
      "│ http://127.0.0.1:8702\n",
      "└ @ MeshCat /home/larissa/.julia/packages/MeshCat/Ax8pH/src/visualizer.jl:73\n"
     ]
    }
   ],
   "source": [
    "create_visualization();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d9557df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indicates how many threads Julia was started with. This is important for the multi-threaded environment\n",
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5411db62",
   "metadata": {},
   "source": [
    "# Create Reinforcement Learning Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96af6ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct VtolEnv{A,T,ACT,R<:AbstractRNG} <: AbstractEnv # Parametric Constructor for a subtype of AbstractEnv\n",
    "    action_space::A # action space\n",
    "    observation_space::Space{Vector{ClosedInterval{T}}} # observation space\n",
    "    state::Vector{T} # current state space\n",
    "    action::ACT # action space\n",
    "    done::Bool # done\n",
    "    t::T # time\n",
    "    rng::R # random number generator\n",
    "\n",
    "    name::String # for multible environoments\n",
    "    visualization::Bool # visualization\n",
    "    realtime::Bool # realtime\n",
    "    \n",
    "    # Everything you need aditionaly can also go in here.\n",
    "    x_W::Vector{T} # current position\n",
    "    v_B::Vector{T} # velocity\n",
    "    R_W::Matrix{T} # current rotation\n",
    "    ω_B::Vector{T} # rotation velocitiy\n",
    "    wind_W::Vector{T} # wind\n",
    "    Δt::T # Δ time\n",
    "    \n",
    "    # Current Bonus / Target\n",
    "    num_waypoints::Int # includig start point\n",
    "    waypoints::Vector{Vector{T}}\n",
    "    reached_goal::BitVector\n",
    "    \n",
    "    norm_way::T\n",
    "    progress::T\n",
    "    progress_prev::T\n",
    "    current_point::Int\n",
    "    reached_goal_in_step::Bool\n",
    "    \n",
    "    r_tol::T # tolerance within drones has to reach waypoint\n",
    "    projected_position::Vector{T} # projected position of drone along trajectory\n",
    "\n",
    "    slow_mode::Bool # slow flight learning mode\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4ede3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_TOL = 1.0\n",
    "N_WAYPOINTS = 8\n",
    "SLOW_MODE = false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14a6873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a keyword-based constructor for the type declared in the mutable struct typedef. \n",
    "# It could also be done with the macro Base.@kwdef.\n",
    "function VtolEnv(;\n",
    "    rng = Random.GLOBAL_RNG, # Random number generation\n",
    "    name = \"vtol\",\n",
    "    visualization = false,\n",
    "    realtime = false,\n",
    "    kwargs... # let the function take an arbitrary number of keyword arguments \n",
    ")\n",
    "    \n",
    "    T = Float64; # explicit type which is used e.g. in state. Cannot be altered due to the poor matrix defininon.\n",
    "\n",
    "    \n",
    "    action_space = Space(\n",
    "        ClosedInterval{T}[\n",
    "            0.0..2.0, # propeller 1\n",
    "            0.0..2.0, # propeller 2\n",
    "            ], \n",
    "    )\n",
    "\n",
    "    \n",
    "    state_space = Space( # Three continuous values in state space.\n",
    "        ClosedInterval{T}[\n",
    "            # orientate yourself on the state space from the paper\n",
    "            typemin(T)..typemax(T), # position along x\n",
    "            typemin(T)..typemax(T), # position along z\n",
    "            \n",
    "            typemin(T)..typemax(T), # orientation along x\n",
    "            typemin(T)..typemax(T), # orientation along z\n",
    "            \n",
    "            typemin(T)..typemax(T), # velocity along x BODY coordinates\n",
    "            typemin(T)..typemax(T), # velocity along y BODY coordinates\n",
    "            \n",
    "            typemin(T)..typemax(T), # rotational velocity along z BODY coordinates\n",
    "            \n",
    "            typemin(T)..typemax(T), # position error along x (next gate - current position)\n",
    "            typemin(T)..typemax(T), # position error along z (next gate - current position)\n",
    "            \n",
    "            typemin(T)..typemax(T), # way to next next gate x (next next gate - next gate)\n",
    "            typemin(T)..typemax(T), # way to next next gate z (next next gate - next gate)\n",
    "            # TODO: more points?\n",
    "            ], \n",
    "    )\n",
    "    \n",
    "    num_waypoints = N_WAYPOINTS # number of waypoints, includig start point\n",
    "    waypoints = generate_trajectory(num_waypoints) # trajectory with num_waypoints waypoints (+ start point) \n",
    "    reached_goal = falses(num_waypoints)\n",
    "    \n",
    "    norm_way = 0.0 \n",
    "    for i in 1:(num_waypoints - 1)\n",
    "        norm_way += norm(waypoints[i] - waypoints[i + 1])\n",
    "    end\n",
    "    \n",
    "    if visualization #visualizes VTOL and waypoints\n",
    "        create_VTOL(name, actuators = true, color_vec=[1.0; 1.0; 0.6; 1.0]);\n",
    "        visualize_waypoints(waypoints, 0.15)\n",
    "    end\n",
    "\n",
    "    environment = VtolEnv(\n",
    "        action_space,\n",
    "        state_space, \n",
    "        zeros(T, length(state_space)), # current state, needs to be extended\n",
    "        rand(action_space), #initialization action\n",
    "        false, # episode done \n",
    "        0.0, # time\n",
    "        rng, # random number generator  \n",
    "        \n",
    "        name,\n",
    "        visualization,\n",
    "        realtime,\n",
    "        \n",
    "        zeros(T, 3), # x_W, current position\n",
    "        zeros(T, 3), # v_B, velocity\n",
    "        [1.0 0.0 0.0; 0.0 1.0 0.0; 0.0 0.0 1.0], # R_W, current rotation, Float64... so T needs to be Float64\n",
    "        zeros(T, 3), # ω_B\n",
    "        zeros(T, 3), # wind_W\n",
    "        T(0.025), # Δt \n",
    "        \n",
    "        num_waypoints, # includig start point\n",
    "        waypoints, \n",
    "        reached_goal,\n",
    "        \n",
    "        norm_way, # norm_way\n",
    "        0.0, # progress\n",
    "        0.0, # progress_prev\n",
    "        2, # current point\n",
    "        false, # reached_goal_in_step\n",
    "        \n",
    "        R_TOL, # r_tol\n",
    "        zeros(T, 3), # projected_position\n",
    "\n",
    "        SLOW_MODE # slow_mode\n",
    "    )\n",
    "    \n",
    "    \n",
    "    reset!(environment)\n",
    "    \n",
    "    return environment\n",
    "    \n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec660d5e",
   "metadata": {},
   "source": [
    "Just for explanation:\n",
    "\n",
    "1. A mutable Struct is created. A struct is a constructor and a constructor is a function that creates new objects.\n",
    "2. A outer keyword-based constructor method is added for the type declared in the mutable struct typedef before.\n",
    "\n",
    "So now we have a function with two methods. Julia will decide which method to call by multiple dispatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc3c4bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "# 2 methods for type constructor:<ul><li> VtolEnv(; <i>rng, name, visualization, realtime, kwargs...</i>) in Main at In[6]:3</li> <li> VtolEnv(action_space::<b>A</b>, observation_space::<b>Space{Array{ClosedInterval{T}, 1}}</b>, state::<b>Vector{T}</b>, action::<b>ACT</b>, done::<b>Bool</b>, t::<b>T</b>, rng::<b>R</b>, name::<b>String</b>, visualization::<b>Bool</b>, realtime::<b>Bool</b>, x_W::<b>Vector{T}</b>, v_B::<b>Vector{T}</b>, R_W::<b>Matrix{T}</b>, ω_B::<b>Vector{T}</b>, wind_W::<b>Vector{T}</b>, Δt::<b>T</b>, num_waypoints::<b>Int64</b>, waypoints::<b>Array{Vector{T}, 1}</b>, reached_goal::<b>BitVector</b>, norm_way::<b>T</b>, progress::<b>T</b>, progress_prev::<b>T</b>, current_point::<b>Int64</b>, reached_goal_in_step::<b>Bool</b>, r_tol::<b>T</b>, projected_position::<b>Vector{T}</b>, slow_mode::<b>Bool</b>)<i> where {A, T, ACT, R<:AbstractRNG}</i> in Main at In[4]:2</li> </ul>"
      ],
      "text/plain": [
       "# 2 methods for type constructor:\n",
       "[1] VtolEnv(; rng, name, visualization, realtime, kwargs...) in Main at In[6]:3\n",
       "[2] VtolEnv(action_space::A, observation_space::Space{Array{ClosedInterval{T}, 1}}, state::Vector{T}, action::ACT, done::Bool, t::T, rng::R, name::String, visualization::Bool, realtime::Bool, x_W::Vector{T}, v_B::Vector{T}, R_W::Matrix{T}, ω_B::Vector{T}, wind_W::Vector{T}, Δt::T, num_waypoints::Int64, waypoints::Array{Vector{T}, 1}, reached_goal::BitVector, norm_way::T, progress::T, progress_prev::T, current_point::Int64, reached_goal_in_step::Bool, r_tol::T, projected_position::Vector{T}, slow_mode::Bool) where {A, T, ACT, R<:AbstractRNG} in Main at In[4]:2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methods(VtolEnv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806413d1",
   "metadata": {},
   "source": [
    "# Define the RL interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f822029",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(env::VtolEnv, seed) = Random.seed!(env.rng, seed)\n",
    "RLBase.action_space(env::VtolEnv) = env.action_space\n",
    "RLBase.state_space(env::VtolEnv) = env.observation_space\n",
    "RLBase.is_terminated(env::VtolEnv) = env.done\n",
    "RLBase.state(env::VtolEnv) = env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b48de74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scale_for_slowmode (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function scale_for_slowmode(slow_mode::Bool, v_min::T, v_max::T, d_max::T, x_W::Vector{T}, projected_position::Vector{T}, v_B::Vector{T}) where T\n",
    "    # TODO safe in utils\n",
    "    if slow_mode == false\n",
    "        return 1\n",
    "    else\n",
    "        if norm(v_B) > v_max\n",
    "            s_vmax = 10^(v_max - norm(v_B))\n",
    "        else\n",
    "            s_vmax = 1\n",
    "        end\n",
    "\n",
    "        if norm(v_B) < v_min\n",
    "            s_vmin = 10^(norm(v_B) - v_min)\n",
    "        else\n",
    "            s_vmin = 1\n",
    "        end\n",
    "\n",
    "        if norm(x_W - projected_position) > d_max\n",
    "            s_gd = exp(-norm(x_W - projected_position) + d_max)\n",
    "        else\n",
    "            s_gd = 1\n",
    "        end\n",
    "        s = s_vmax * s_vmin * s_gd\n",
    "    end\n",
    "    return s\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f7fb89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "function computeReward(env::VtolEnv{A,T}) where {A,T}\n",
    "    \n",
    "    v_min = 1.0 # min velocity\n",
    "    v_max = 50.0 # max velocity\n",
    "    d_max = 0.25 \n",
    "\n",
    "\n",
    "    s = scale_for_slowmode(env.slow_mode, v_min, v_max, d_max, env.x_W, env.projected_position, env.v_B)\n",
    "    \n",
    "    # Todo norm weg? \n",
    "    k_p = 5.0 * s / env.norm_way #env.num_waypoints / env.norm_way;# factor for progress (between current position and last position) reward \n",
    "    r_p = (env.progress - env.progress_prev); # reward for progress (between current position and last position)\n",
    "\n",
    "    k_s = s * (2 * v_max * env.Δt) / env.norm_way #5.0 # factor for reached distance (overall) reward\n",
    "    r_s = env.progress # reward for reached distance (overall)\n",
    "    \n",
    "    k_wp = 10.0 # * env.num_waypoints # factor for reached gate reward\n",
    "    r_wp = 0.0 # reward for reached gate, TODO: change to gates later (when gates != waypoints), sinnvoll ohne obstacels?\n",
    "    if env.reached_goal_in_step\n",
    "        r_wp = exp(-norm(env.x_W - env.waypoints[env.current_point - 1])/env.r_tol)\n",
    "    end \n",
    "\n",
    "    k_ω = 0.01 # factor for too high body rate penalty\n",
    "    norm_ω = norm(env.ω_B[3]) # penalty for body rate\n",
    "\n",
    "    if env.x_W[3] < -2\n",
    "        fall = 1\n",
    "    else\n",
    "        fall = 0\n",
    "    end\n",
    "\n",
    "    return k_p * r_p + k_s * r_s + k_wp * r_wp - k_ω * norm_ω - fall\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "RLBase.reward(env::VtolEnv{A,T}) where {A,T} = computeReward(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae45ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "function RLBase.reset!(env::VtolEnv{A,T}) where {A,T}\n",
    "    # Visualize initial state\n",
    "    if env.visualization\n",
    "        set_transform(env.name, env.x_W, QuatRotation(env.R_W));\n",
    "        set_actuators(env.name, [0.0; 0.0; 0.0; 0.0]);\n",
    "    end\n",
    "    \n",
    "    env.x_W = [0.0; 0.0; 0.0];\n",
    "    env.v_B = [0.0; 0.0; 0.0];\n",
    "    env.R_W = Matrix(UnitQuaternion(RotZ(-pi/2.0)*RotY(-pi/2.0)*RotX(pi)));\n",
    "\n",
    "    env.ω_B = [0.0; 0.0; 0.0];\n",
    "    env.wind_W = [0.0; 0.0; 0.0];\n",
    "    \n",
    "    #env.num_waypoints = 4; # includig start point\n",
    "    env.waypoints = generate_trajectory(env.num_waypoints);\n",
    "    env.reached_goal = falses(env.num_waypoints);\n",
    "    env.reached_goal[1] = true; # set first point to reached (start point)\n",
    "    \n",
    "    env.current_point = 2;\n",
    "    env.reached_goal_in_step = false;\n",
    "    #env.r_tol = 0.3;\n",
    "    \n",
    "    if env.visualization\n",
    "        visualize_waypoints(env.waypoints, 0.15); \n",
    "    end\n",
    "    \n",
    "    norm_way = 0.0 \n",
    "    for i in 1:(env.num_waypoints - 1)\n",
    "        norm_way += norm(env.waypoints[i] - env.waypoints[i + 1])\n",
    "    end\n",
    "    \n",
    "    env.norm_way = norm_way\n",
    "    env.progress = 0.0;\n",
    "    env.progress_prev = 0.0;\n",
    "    \n",
    "    \n",
    "    env.state = [env.x_W[1]; # position along x\n",
    "                 env.x_W[3]; # position along z\n",
    "        \n",
    "                 env.R_W[1,1]; # orientation along x\n",
    "                 env.R_W[3,1]; # orientation along z\n",
    "        \n",
    "                 env.v_B[1]; # velocity along x BODY coordinates\n",
    "                 env.v_B[2]; # velocity along y BODY coordinates  \n",
    "        \n",
    "                 env.ω_B[3]; # rotational velocity along z BODY coordinates\n",
    "        \n",
    "                 env.waypoints[2][1] - env.x_W[1]; # position error along x\n",
    "                 env.waypoints[2][3] - env.x_W[3]; # position error along z\n",
    "                 \n",
    "                 # TODO: anpassen wie im step!\n",
    "                 0.0; # way to next next gate x (next next gate - next gate)\n",
    "                 0.0] # way to next next gate z (next next gate - next gate)\n",
    "    \n",
    "    if env.num_waypoints >= 3\n",
    "        env.state[10] = env.waypoints[3][1] - env.x_W[1]; # way to next next gate x (next next gate - next gate)\n",
    "        env.state[11] = env.waypoints[3][3] - env.x_W[3]; # way to next next gate z (next next gate - next gate)\n",
    "    end\n",
    "        \n",
    "    env.t = 0.0;\n",
    "    env.action = [0.0, 0.0];\n",
    "    env.done = false;\n",
    "\n",
    "    env.projected_position = [0; 0; 0]\n",
    "    \n",
    "    nothing\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cf1a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines a methods for a callable object.\n",
    "# So when a VtolEnv object is created, it has this method that can be called\n",
    "function (env::VtolEnv)(a)\n",
    "    # TODO: set flaps later in 3D\n",
    "    # set the propeller trust and the two flaps 2D case\n",
    "    next_action = [a[1], a[2], 0.0, 0.0]\n",
    "   \n",
    "    _step!(env, next_action)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0e9eb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VtolEnv();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26a116cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "# 3 methods for callable object:<ul><li> (env::<b>VtolEnv</b>)(a) in Main at In[12]:3</li> <li> (env::<b>AbstractEnv</b>)(action) in ReinforcementLearningBase</li> <li> (env::<b>AbstractEnv</b>)(action, player) in ReinforcementLearningBase</li> </ul>"
      ],
      "text/plain": [
       "# 3 methods for callable object:\n",
       "[1] (env::VtolEnv)(a) in Main at In[12]:3\n",
       "[2] (env::AbstractEnv)(action) in ReinforcementLearningBase\n",
       "[3] (env::AbstractEnv)(action, player) in ReinforcementLearningBase"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methods(env) # Just to explain which methods the object has"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e459ae8b",
   "metadata": {},
   "source": [
    "TODO:\n",
    "evtl einfügen, dass wenn man über ziel drüber fliegt trotzdem den current point updatet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e7d4727",
   "metadata": {},
   "outputs": [],
   "source": [
    "function _step!(env::VtolEnv, next_action)\n",
    "    # caluclate wind impact\n",
    "    v_in_wind_B = vtol_add_wind(env.v_B, env.R_W, env.wind_W)\n",
    "    # caluclate aerodynamic forces\n",
    "    torque_B, force_B = vtol_model(v_in_wind_B, next_action, eth_vtol_param);\n",
    "    # Limit to 2D\n",
    "    force_B[3] = 0.0; # Body Z\n",
    "    env.v_B[3] = 0.0;\n",
    "    torque_B[1] = 0.0; torque_B[2] = 0.0;  # Body X and Y\n",
    "    env.ω_B[1] = 0.0; env.ω_B[2] = 0.0;\n",
    "    # integrate rigid body dynamics for Δt\n",
    "    env.x_W, env.v_B, env.R_W, env.ω_B, time = rigid_body_simple(torque_B, force_B, env.x_W, env.v_B, env.R_W, env.ω_B, env.t, env.Δt, eth_vtol_param)\n",
    "    \n",
    "    \n",
    "    env.reached_goal_in_step = false;\n",
    "    if norm(env.x_W - env.waypoints[env.current_point]) < env.r_tol\n",
    "        env.reached_goal_in_step = true;\n",
    "        env.reached_goal[env.current_point] = true;\n",
    "        env.current_point += 1;\n",
    "    end\n",
    "        \n",
    "            \n",
    "    # calculate progress on trajectory\n",
    "    env.progress_prev = env.progress\n",
    "    \n",
    "    current_progress = 0.0\n",
    "    line_segment, env.projected_position = calculate_progress(env.waypoints, env.x_W)\n",
    "    \n",
    "    #env.current_point = line_segment + 1\n",
    "\n",
    "    for i in 2:(line_segment)\n",
    "       current_progress +=  norm(env.waypoints[i] - env.waypoints[i - 1])  \n",
    "    end\n",
    "    current_progress += norm(env.waypoints[line_segment] - env.projected_position)\n",
    "    \n",
    "    env.progress = current_progress\n",
    "    \n",
    "\n",
    "    if env.realtime\n",
    "        sleep(env.Δt) # TODO: just a dirty hack. this is of course slower than real time.\n",
    "    end\n",
    "\n",
    "    # Visualize the new state \n",
    "    if env.visualization\n",
    "        set_transform(env.name, env.x_W, QuatRotation(env.R_W));\n",
    "        set_actuators(env.name, next_action)\n",
    "        \n",
    "        for i in eachindex(env.reached_goal)\n",
    "            if env.reached_goal[i]\n",
    "                create_sphere(\"fixgoal_$i\", 0.2, color=RGBA{Float32}(1.0, 0.0, 0.0, 1.0));\n",
    "                set_transform(\"fixgoal_$i\", env.waypoints[i]);\n",
    "            end\n",
    "        end\n",
    "    end\n",
    " \n",
    "\n",
    "    env.t += env.Δt\n",
    "    \n",
    "    env.state[1] = env.x_W[1]; # position along x\n",
    "    env.state[2] = env.x_W[3]; # position along z\n",
    "    \n",
    "    env.state[3] = env.R_W[1,1]; # orientation along x\n",
    "    env.state[4] = env.R_W[3,1]; # orientation along z\n",
    "    \n",
    "    env.state[5] = env.v_B[1]; # velocity along x BODY coordinates\n",
    "    env.state[6] = env.v_B[2]; # velocity along y BODY coordinates\n",
    "    \n",
    "    env.state[7] = env.ω_B[3];  # rotational velocity along z BODY coordinates\n",
    "    \n",
    "    # todo double check if correct, mehr punkte\n",
    "    if env.current_point <= env.num_waypoints\n",
    "        env.state[8] = env.waypoints[env.current_point][1] - env.x_W[1]; # position error along x\n",
    "        env.state[9] = env.waypoints[env.current_point][3] - env.x_W[3]; # position error along z\n",
    "        \n",
    "        if env.current_point <= env.num_waypoints - 1\n",
    "            env.state[10] = env.waypoints[env.current_point + 1][1] - env.x_W[1]; # way to next next gate x (next next gate - next gate)\n",
    "            env.state[11] = env.waypoints[env.current_point + 1][3] - env.x_W[3]; # way to next next gate z (next next gate - next gate)\n",
    "        else\n",
    "            env.state[10] = env.state[8] + 0.0 # way to next next gate x (next next gate - next gate)\n",
    "            env.state[11] = env.state[9] + 1.0 # way to next next gate z (next next gate - next gate)\n",
    "        end\n",
    "    else\n",
    "        env.state[8] = 0.0; # position error along x\n",
    "        env.state[9] = 1.0; # position error along z\n",
    "        env.state[10] = 0.0 # way to next next gate x (next next gate - next gate)\n",
    "        env.state[11] = 1.0 # way to next next gate z (next next gate - next gate)\n",
    "    end\n",
    "        \n",
    "    \n",
    "    # Termination criteria\n",
    "    # TODO: Use many termination criteria so that you do not train unnecessarily in wrong areas\n",
    "    env.done = #true\n",
    "        # After time... How fast is drone+Range of desired point\n",
    "        # After reaching position (circle of r_tol)\n",
    "        norm(env.ω_B) > 100.0 || \n",
    "        norm(env.v_B) > 100.0 || # stop if body is too fast # TODO: set higher later in fast training phase\n",
    "        env.x_W[3] < -5.0 || # stop if body is below -5m\n",
    "        env.t > env.num_waypoints * 3.0 ||# stop after 3s per point\n",
    "        norm(env.x_W - env.projected_position) > 5.0 || # too far off the path \n",
    "        env.current_point > env.num_waypoints #||# all points reached\n",
    "        norm(env.x_W - env.waypoints[end])<env.r_tol\n",
    "\n",
    "    nothing\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c37e8e",
   "metadata": {},
   "source": [
    "changed to 10s (5s before) per point and 5.0m too far off path (2.0 before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e1cd988",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[1mTest Summary:              | \u001b[22m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal  \u001b[22m\u001b[39m\u001b[0m\u001b[1mTime\u001b[22m\n",
      "random policy with VtolEnv | \u001b[32m2000  \u001b[39m\u001b[36m 2000  \u001b[39m\u001b[0m2.7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"random policy with VtolEnv\", Any[], 2000, false, false, true, 1.673887805037553e9, 1.673887807783945e9)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RLBase.test_runnable!(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c223a31f",
   "metadata": {},
   "source": [
    "Show an overview of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe6de74",
   "metadata": {},
   "source": [
    "# Setup of a reinforcement learning experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5683fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "MultiThreadEnv(8 x VtolEnv)"
      ],
      "text/plain": [
       "MultiThreadEnv(8 x VtolEnv)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 123    \n",
    "rng = StableRNG(seed)\n",
    "    N_ENV = 8\n",
    "    UPDATE_FREQ = 1024\n",
    "    \n",
    "    \n",
    "    # define multiple environments for parallel training\n",
    "    env = MultiThreadEnv([\n",
    "        # use different names for the visualization\n",
    "        VtolEnv(; rng = StableRNG(hash(seed+i)), name = \"vtol$i\") for i in 1:N_ENV\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1f128b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function approximator\n",
    "# TODO: change architecture eventually, smaller?\n",
    "    ns, na = length(state(env[1])), length(action_space(env[1]))\n",
    "    approximator = ActorCritic(\n",
    "                actor = GaussianNetwork(\n",
    "                    pre = Chain(\n",
    "                    Dense(ns, 128, tanh; initW = glorot_uniform(rng)),#\n",
    "                    Dense(128, 128, tanh; initW = glorot_uniform(rng)),\n",
    "                    ),\n",
    "                    μ = Chain(Dense(128, na; initW = glorot_uniform(rng))),\n",
    "                    logσ = Chain(Dense(128, na; initW = glorot_uniform(rng))),\n",
    "                ),\n",
    "                critic = Chain(\n",
    "                    Dense(ns, 128, tanh; initW = glorot_uniform(rng)),\n",
    "                    Dense(128, 128, tanh; initW = glorot_uniform(rng)),\n",
    "                    Dense(128, 1; initW = glorot_uniform(rng)),\n",
    "                ),\n",
    "                optimizer = ADAM(1e-3),\n",
    "            );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ea4c37c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: The GPU function is being called but the GPU is not accessible. \n",
      "│ Defaulting back to the CPU. (No action is required if you want to run on the CPU).\n",
      "└ @ Flux /home/larissa/.julia/packages/Flux/7nTyc/src/functor.jl:187\n"
     ]
    }
   ],
   "source": [
    "    agent = Agent( # A wrapper of an AbstractPolicy\n",
    "        # AbstractPolicy: the policy to use\n",
    "        # TODO: change eventually to SAC or compare\n",
    "        policy = PPOPolicy(;\n",
    "                    approximator = approximator |> gpu,\n",
    "                    update_freq=UPDATE_FREQ,\n",
    "                    dist = Normal,\n",
    "                    # For parameters visit the docu: https://juliareinforcementlearning.org/docs/rlzoo/#ReinforcementLearningZoo.PPOPolicy\n",
    "                    ),\n",
    "        \n",
    "        # AbstractTrajectory: used to store transitions between an agent and an environment source\n",
    "        trajectory = PPOTrajectory(;\n",
    "            capacity = UPDATE_FREQ,\n",
    "            state = Matrix{Float64} => (ns, N_ENV),\n",
    "            action = Matrix{Float64} => (na, N_ENV),\n",
    "            action_log_prob = Vector{Float64} => (N_ENV,),\n",
    "            reward = Vector{Float64} => (N_ENV,),\n",
    "            terminal = Vector{Bool} => (N_ENV,),\n",
    "        ),\n",
    "    );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f158a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "function saveModel(t, agent, env)\n",
    "    model = cpu(agent.policy.approximator)   \n",
    "    f = joinpath(\"./RL_models_fast/\", \"vtol_2D_ppo_$t.bson\") # TODO: evtl anpassen\n",
    "    @save f model\n",
    "    println(\"parameters at step $t saved to $f\")\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c689a72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "function loadModel()\n",
    "    f = joinpath(\"./RL_models_slow/\", \"vtol_2D_ppo_700000.bson\") # TODO: evtl anpassen\n",
    "    @load f model\n",
    "    return model\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3c1858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "function validate_policy(t, agent, env)\n",
    "    run(agent.policy, test_env, StopAfterEpisode(1), episode_test_reward_hook)\n",
    "    # the result of the hook\n",
    "    println(\"test reward at step $t: $(episode_test_reward_hook.rewards[end])\")\n",
    "    \n",
    "end;\n",
    "\n",
    "episode_test_reward_hook = TotalRewardPerEpisode(;is_display_on_exit=false)\n",
    "# create a env only for reward test\n",
    "test_env = VtolEnv(;name = \"testVTOL\", visualization = true, realtime = true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af72d48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.policy.approximator = loadModel(); # TODO: un/comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb737010",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   1%|▋                                        |  ETA: 1:42:07\u001b[39m39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 10000: 310.1889042874085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   3%|█▏                                       |  ETA: 0:56:44\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 20000: 232.12754396458675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   4%|█▊                                       |  ETA: 0:46:59\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 30000: 283.8636776890471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   6%|██▍                                      |  ETA: 0:37:26\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 40000: 1220.2717735130568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   7%|██▉                                      |  ETA: 0:37:28\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 50000: 625.250190845946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   8%|███▍                                     |  ETA: 0:36:21\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 60000: 1688.362830703635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  10%|████▏                                    |  ETA: 0:35:20\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 70000: 851.4072482808091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  11%|████▋                                    |  ETA: 0:35:04\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 80000: 1724.513803051927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  13%|█████▎                                   |  ETA: 0:35:17\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 90000: 1003.3473878200097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  14%|█████▉                                   |  ETA: 0:34:47\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 100000 saved to ./RL_models_fast/vtol_2D_ppo_100000.bson\n",
      "test reward at step 100000: 1027.6825502626177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  16%|██████▍                                  |  ETA: 0:34:06\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 110000: 1985.7851761822615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  17%|███████                                  |  ETA: 0:33:57\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 120000: 1879.638945361403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  19%|███████▋                                 |  ETA: 0:33:31\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 130000: 188.47036448215138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  20%|████████▏                                |  ETA: 0:31:45\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 140000: 1348.5331452283294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  21%|████████▊                                |  ETA: 0:31:25\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 150000: 909.4955652390813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  23%|█████████▍                               |  ETA: 0:30:54\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 160000: 1362.131621918642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  24%|█████████▉                               |  ETA: 0:30:36\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 170000: 1462.7712980072683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  26%|██████████▌                              |  ETA: 0:30:15\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 180000: 1670.0090053026543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  27%|███████████                              |  ETA: 0:30:09\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 190000: 314.1050976827923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  28%|███████████▋                             |  ETA: 0:29:02\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 200000 saved to ./RL_models_fast/vtol_2D_ppo_200000.bson\n",
      "test reward at step 200000: 209.66561308946248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  30%|████████████▎                            |  ETA: 0:27:31\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 210000: 1662.3157066857302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  31%|████████████▉                            |  ETA: 0:27:11\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 220000: 1417.9593829552846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  33%|█████████████▍                           |  ETA: 0:26:45\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 230000: 1254.2485165645323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  34%|██████████████                           |  ETA: 0:26:12\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 240000: 695.4359759579596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  35%|██████████████▌                          |  ETA: 0:25:37\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 250000: 1051.6941874162003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  37%|███████████████▎                         |  ETA: 0:25:34\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 260000: 1181.865360205341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  39%|███████████████▊                         |  ETA: 0:24:25\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 270000: 1033.8038981137304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  40%|████████████████▍                        |  ETA: 0:23:58\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 280000: 319.40862313597887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  41%|█████████████████                        |  ETA: 0:23:02\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 290000: 1099.4777096443386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  43%|█████████████████▌                       |  ETA: 0:22:38\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 300000 saved to ./RL_models_fast/vtol_2D_ppo_300000.bson\n",
      "test reward at step 300000: 541.4124917453383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  44%|██████████████████▏                      |  ETA: 0:22:07\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 310000: 1874.7107327048266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  46%|██████████████████▋                      |  ETA: 0:21:44\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 320000: 1897.3201933508392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  47%|███████████████████▎                     |  ETA: 0:21:10\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 330000: 446.7785826070819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  49%|███████████████████▉                     |  ETA: 0:20:17\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 340000: 1645.9312977826437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  50%|████████████████████▌                    |  ETA: 0:19:51\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 350000: 347.98388179070815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  51%|█████████████████████▏                   |  ETA: 0:18:59\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 360000: 840.4297793047691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  53%|█████████████████████▋                   |  ETA: 0:18:27\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 370000: 940.9816856816145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  54%|██████████████████████▎                  |  ETA: 0:17:54\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 380000: 1771.1227933468008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  55%|██████████████████████▊                  |  ETA: 0:17:31\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 390000: 1523.7576848051624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  57%|███████████████████████▍                 |  ETA: 0:16:53\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 400000 saved to ./RL_models_fast/vtol_2D_ppo_400000.bson\n",
      "test reward at step 400000: 891.0221461631944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  58%|████████████████████████                 |  ETA: 0:16:23\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 410000: 1168.7813716324044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  60%|████████████████████████▋                |  ETA: 0:15:51\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 420000: 1129.7482778984497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  61%|█████████████████████████▏               |  ETA: 0:15:26\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 430000: 760.2572521327893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  63%|█████████████████████████▊               |  ETA: 0:14:42\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 440000: 344.5429709641898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  64%|██████████████████████████▍              |  ETA: 0:14:04\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 450000: 1489.6069737819528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  66%|██████████████████████████▉              |  ETA: 0:13:36\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 460000: 305.9289323937838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  67%|███████████████████████████▌             |  ETA: 0:12:56\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 470000: 1203.3785429766233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  68%|████████████████████████████             |  ETA: 0:12:30\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 480000: 873.9900326819788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  70%|████████████████████████████▊            |  ETA: 0:11:51\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 490000: 638.9878784780774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  71%|█████████████████████████████▎           |  ETA: 0:11:19\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 500000 saved to ./RL_models_fast/vtol_2D_ppo_500000.bson\n",
      "test reward at step 500000: 110.56205144015746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  73%|█████████████████████████████▉           |  ETA: 0:10:38\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 510000: 754.9923868588719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  74%|██████████████████████████████▌          |  ETA: 0:10:06\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 520000: 1593.2847654622487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  76%|███████████████████████████████          |  ETA: 0:09:35\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 530000: 1501.2387341498072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  77%|███████████████████████████████▋         |  ETA: 0:09:01\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 540000: 530.5440975504215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  78%|████████████████████████████████▏        |  ETA: 0:08:31\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 550000: 727.3584596863565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  80%|████████████████████████████████▊        |  ETA: 0:07:52\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 560000: 59.08029227925955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  81%|█████████████████████████████████▍       |  ETA: 0:07:14\u001b[39m"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      "  [1] poptask(W::Base.InvasiveLinkedListSynchronized{Task})",
      "    @ Base ./task.jl:921",
      "  [2] wait()",
      "    @ Base ./task.jl:930",
      "  [3] wait(c::Base.GenericCondition{Base.Threads.SpinLock})",
      "    @ Base ./condition.jl:124",
      "  [4] _trywait(t::Timer)",
      "    @ Base ./asyncevent.jl:138",
      "  [5] wait",
      "    @ ./asyncevent.jl:155 [inlined]",
      "  [6] sleep(sec::Float64)",
      "    @ Base ./asyncevent.jl:240",
      "  [7] _step!(env::VtolEnv{Space{Vector{ClosedInterval{Float64}}}, Float64, Vector{Float64}, Random._GLOBAL_RNG}, next_action::Vector{Float64})",
      "    @ Main ./In[15]:40",
      "  [8] (::VtolEnv{Space{Vector{ClosedInterval{Float64}}}, Float64, Vector{Float64}, Random._GLOBAL_RNG})(a::Matrix{Float64})",
      "    @ Main ./In[12]:8",
      "  [9] _run(policy::PPOPolicy{ActorCritic{GaussianNetwork{Chain{Tuple{Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, typeof(tanh)}, Chain{Tuple{Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, ADAM}, Normal, Random._GLOBAL_RNG}, env::VtolEnv{Space{Vector{ClosedInterval{Float64}}}, Float64, Vector{Float64}, Random._GLOBAL_RNG}, stop_condition::StopAfterEpisode{ProgressMeter.Progress}, hook::TotalRewardPerEpisode)",
      "    @ ReinforcementLearningCore ~/.julia/packages/ReinforcementLearningCore/yeRLW/src/core/run.jl:32",
      " [10] run(policy::PPOPolicy{ActorCritic{GaussianNetwork{Chain{Tuple{Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, typeof(tanh)}, Chain{Tuple{Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, ADAM}, Normal, Random._GLOBAL_RNG}, env::VtolEnv{Space{Vector{ClosedInterval{Float64}}}, Float64, Vector{Float64}, Random._GLOBAL_RNG}, stop_condition::StopAfterEpisode{ProgressMeter.Progress}, hook::TotalRewardPerEpisode)",
      "    @ ReinforcementLearningCore ~/.julia/packages/ReinforcementLearningCore/yeRLW/src/core/run.jl:10",
      " [11] validate_policy(t::Int64, agent::Agent{PPOPolicy{ActorCritic{GaussianNetwork{Chain{Tuple{Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, typeof(tanh)}, Chain{Tuple{Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, ADAM}, Normal, Random._GLOBAL_RNG}, Trajectory{NamedTuple{(:action_log_prob, :state, :action, :reward, :terminal), Tuple{CircularArrayBuffers.CircularArrayBuffer{Float64, 2, Matrix{Float64}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 3, Array{Float64, 3}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 3, Array{Float64, 3}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 2, Matrix{Float64}}, CircularArrayBuffers.CircularArrayBuffer{Bool, 2, Matrix{Bool}}}}}}, env::MultiThreadEnv{VtolEnv{Space{Vector{ClosedInterval{Float64}}}, Float64, Vector{Float64}, StableRNGs.LehmerRNG}, Matrix{Float64}, Vector{Float64}, Space{Matrix{ClosedInterval{Float64}}}, Space{Matrix{ClosedInterval{Float64}}}, Nothing})",
      "    @ Main ./In[22]:2",
      " [12] (::DoEveryNStep{typeof(validate_policy)})(#unused#::PostActStage, agent::Agent{PPOPolicy{ActorCritic{GaussianNetwork{Chain{Tuple{Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, typeof(tanh)}, Chain{Tuple{Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, ADAM}, Normal, Random._GLOBAL_RNG}, Trajectory{NamedTuple{(:action_log_prob, :state, :action, :reward, :terminal), Tuple{CircularArrayBuffers.CircularArrayBuffer{Float64, 2, Matrix{Float64}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 3, Array{Float64, 3}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 3, Array{Float64, 3}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 2, Matrix{Float64}}, CircularArrayBuffers.CircularArrayBuffer{Bool, 2, Matrix{Bool}}}}}}, env::MultiThreadEnv{VtolEnv{Space{Vector{ClosedInterval{Float64}}}, Float64, Vector{Float64}, StableRNGs.LehmerRNG}, Matrix{Float64}, Vector{Float64}, Space{Matrix{ClosedInterval{Float64}}}, Space{Matrix{ClosedInterval{Float64}}}, Nothing})",
      "    @ ReinforcementLearningCore ~/.julia/packages/ReinforcementLearningCore/yeRLW/src/core/hooks.jl:283",
      " [13] (::ComposedHook{Tuple{DoEveryNStep{typeof(saveModel)}, DoEveryNStep{typeof(validate_policy)}}})(::PostActStage, ::Agent{PPOPolicy{ActorCritic{GaussianNetwork{Chain{Tuple{Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, typeof(tanh)}, Chain{Tuple{Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, ADAM}, Normal, Random._GLOBAL_RNG}, Trajectory{NamedTuple{(:action_log_prob, :state, :action, :reward, :terminal), Tuple{CircularArrayBuffers.CircularArrayBuffer{Float64, 2, Matrix{Float64}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 3, Array{Float64, 3}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 3, Array{Float64, 3}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 2, Matrix{Float64}}, CircularArrayBuffers.CircularArrayBuffer{Bool, 2, Matrix{Bool}}}}}}, ::Vararg{Any}; kw::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "    @ ReinforcementLearningCore ~/.julia/packages/ReinforcementLearningCore/yeRLW/src/core/hooks.jl:54",
      " [14] ComposedHook",
      "    @ ~/.julia/packages/ReinforcementLearningCore/yeRLW/src/core/hooks.jl:52 [inlined]",
      " [15] _run(policy::Agent{PPOPolicy{ActorCritic{GaussianNetwork{Chain{Tuple{Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, typeof(tanh)}, Chain{Tuple{Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, ADAM}, Normal, Random._GLOBAL_RNG}, Trajectory{NamedTuple{(:action_log_prob, :state, :action, :reward, :terminal), Tuple{CircularArrayBuffers.CircularArrayBuffer{Float64, 2, Matrix{Float64}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 3, Array{Float64, 3}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 3, Array{Float64, 3}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 2, Matrix{Float64}}, CircularArrayBuffers.CircularArrayBuffer{Bool, 2, Matrix{Bool}}}}}}, env::MultiThreadEnv{VtolEnv{Space{Vector{ClosedInterval{Float64}}}, Float64, Vector{Float64}, StableRNGs.LehmerRNG}, Matrix{Float64}, Vector{Float64}, Space{Matrix{ClosedInterval{Float64}}}, Space{Matrix{ClosedInterval{Float64}}}, Nothing}, stop_condition::StopAfterStep{ProgressMeter.Progress}, hook::ComposedHook{Tuple{DoEveryNStep{typeof(saveModel)}, DoEveryNStep{typeof(validate_policy)}}})",
      "    @ ReinforcementLearningZoo ~/.julia/packages/ReinforcementLearningZoo/tvfq9/src/algorithms/policy_gradient/run.jl:23",
      " [16] run(policy::Agent{PPOPolicy{ActorCritic{GaussianNetwork{Chain{Tuple{Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, typeof(tanh)}, Chain{Tuple{Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, ADAM}, Normal, Random._GLOBAL_RNG}, Trajectory{NamedTuple{(:action_log_prob, :state, :action, :reward, :terminal), Tuple{CircularArrayBuffers.CircularArrayBuffer{Float64, 2, Matrix{Float64}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 3, Array{Float64, 3}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 3, Array{Float64, 3}}, CircularArrayBuffers.CircularArrayBuffer{Float64, 2, Matrix{Float64}}, CircularArrayBuffers.CircularArrayBuffer{Bool, 2, Matrix{Bool}}}}}}, env::MultiThreadEnv{VtolEnv{Space{Vector{ClosedInterval{Float64}}}, Float64, Vector{Float64}, StableRNGs.LehmerRNG}, Matrix{Float64}, Vector{Float64}, Space{Matrix{ClosedInterval{Float64}}}, Space{Matrix{ClosedInterval{Float64}}}, Nothing}, stop_condition::StopAfterStep{ProgressMeter.Progress}, hook::ComposedHook{Tuple{DoEveryNStep{typeof(saveModel)}, DoEveryNStep{typeof(validate_policy)}}})",
      "    @ ReinforcementLearningCore ~/.julia/packages/ReinforcementLearningCore/yeRLW/src/core/run.jl:10",
      " [17] top-level scope",
      "    @ In[24]:1",
      " [18] eval",
      "    @ ./boot.jl:368 [inlined]",
      " [19] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1428"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: both Losses and NNlib export \"ctc_loss\"; uses of it in module Flux must be qualified\n"
     ]
    }
   ],
   "source": [
    "ReinforcementLearning.run(\n",
    "    agent,\n",
    "    env,\n",
    "    StopAfterStep(700_000),\n",
    "    ComposedHook(\n",
    "        DoEveryNStep(saveModel, n=100_000), \n",
    "        DoEveryNStep(validate_policy, n=10_000)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49a302a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n<defs>\n  <clipPath id=\"clip130\">\n    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n  </clipPath>\n</defs>\n<path clip-path=\"url(#clip130)\" d=\"\nM0 1600 L2400 1600 L2400 0 L0 0  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<defs>\n  <clipPath id=\"clip131\">\n    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n  </clipPath>\n</defs>\n<path clip-path=\"url(#clip130)\" d=\"\nM201.019 1486.45 L2352.76 1486.45 L2352.76 47.2441 L201.019 47.2441  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<defs>\n  <clipPath id=\"clip132\">\n    <rect x=\"201\" y=\"47\" width=\"2153\" height=\"1440\"/>\n  </clipPath>\n</defs>\n<polyline clip-path=\"url(#clip132)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  225.009,1486.45 225.009,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip132)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  594.09,1486.45 594.09,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip132)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  963.17,1486.45 963.17,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip132)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  1332.25,1486.45 1332.25,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip132)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  1701.33,1486.45 1701.33,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip132)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  2070.41,1486.45 2070.41,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip130)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  201.019,1486.45 2352.76,1486.45 \n  \"/>\n<polyline clip-path=\"url(#clip130)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  225.009,1486.45 225.009,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip130)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  594.09,1486.45 594.09,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip130)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  963.17,1486.45 963.17,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip130)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1332.25,1486.45 1332.25,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip130)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1701.33,1486.45 1701.33,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip130)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  2070.41,1486.45 2070.41,1467.55 \n  \"/>\n<path clip-path=\"url(#clip130)\" d=\"M225.009 1517.37 Q221.398 1517.37 219.57 1520.93 Q217.764 1524.47 217.764 1531.6 Q217.764 1538.71 219.57 1542.27 Q221.398 1545.82 225.009 1545.82 Q228.644 1545.82 230.449 1542.27 Q232.278 1538.71 232.278 1531.6 Q232.278 1524.47 230.449 1520.93 Q228.644 1517.37 225.009 1517.37 M225.009 1513.66 Q230.82 1513.66 233.875 1518.27 Q236.954 1522.85 236.954 1531.6 Q236.954 1540.33 233.875 1544.94 Q230.82 1549.52 225.009 1549.52 Q219.199 1549.52 216.121 1544.94 Q213.065 1540.33 213.065 1531.6 Q213.065 1522.85 216.121 1518.27 Q219.199 1513.66 225.009 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip130)\" d=\"M568.777 1544.91 L576.416 1544.91 L576.416 1518.55 L568.106 1520.21 L568.106 1515.95 L576.37 1514.29 L581.046 1514.29 L581.046 1544.91 L588.684 1544.91 L588.684 1548.85 L568.777 1548.85 L568.777 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip130)\" d=\"M608.129 1517.37 Q604.518 1517.37 602.689 1520.93 Q600.883 1524.47 600.883 1531.6 Q600.883 1538.71 602.689 1542.27 Q604.518 1545.82 608.129 1545.82 Q611.763 1545.82 613.569 1542.27 Q615.397 1538.71 615.397 1531.6 Q615.397 1524.47 613.569 1520.93 Q611.763 1517.37 608.129 1517.37 M608.129 1513.66 Q613.939 1513.66 616.995 1518.27 Q620.073 1522.85 620.073 1531.6 Q620.073 1540.33 616.995 1544.94 Q613.939 1549.52 608.129 1549.52 Q602.319 1549.52 599.24 1544.94 Q596.184 1540.33 596.184 1531.6 Q596.184 1522.85 599.24 1518.27 Q602.319 1513.66 608.129 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip130)\" d=\"M941.943 1544.91 L958.262 1544.91 L958.262 1548.85 L936.318 1548.85 L936.318 1544.91 Q938.98 1542.16 943.563 1537.53 Q948.17 1532.88 949.35 1531.53 Q951.596 1529.01 952.475 1527.27 Q953.378 1525.51 953.378 1523.82 Q953.378 1521.07 951.434 1519.33 Q949.512 1517.6 946.41 1517.6 Q944.211 1517.6 941.758 1518.36 Q939.327 1519.13 936.549 1520.68 L936.549 1515.95 Q939.373 1514.82 941.827 1514.24 Q944.281 1513.66 946.318 1513.66 Q951.688 1513.66 954.883 1516.35 Q958.077 1519.03 958.077 1523.52 Q958.077 1525.65 957.267 1527.57 Q956.48 1529.47 954.373 1532.07 Q953.795 1532.74 950.693 1535.95 Q947.591 1539.15 941.943 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip130)\" d=\"M978.077 1517.37 Q974.466 1517.37 972.637 1520.93 Q970.832 1524.47 970.832 1531.6 Q970.832 1538.71 972.637 1542.27 Q974.466 1545.82 978.077 1545.82 Q981.711 1545.82 983.517 1542.27 Q985.345 1538.71 985.345 1531.6 Q985.345 1524.47 983.517 1520.93 Q981.711 1517.37 978.077 1517.37 M978.077 1513.66 Q983.887 1513.66 986.943 1518.27 Q990.021 1522.85 990.021 1531.6 Q990.021 1540.33 986.943 1544.94 Q983.887 1549.52 978.077 1549.52 Q972.267 1549.52 969.188 1544.94 Q966.133 1540.33 966.133 1531.6 Q966.133 1522.85 969.188 1518.27 Q972.267 1513.66 978.077 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip130)\" d=\"M1321.09 1530.21 Q1324.45 1530.93 1326.32 1533.2 Q1328.22 1535.47 1328.22 1538.8 Q1328.22 1543.92 1324.7 1546.72 Q1321.18 1549.52 1314.7 1549.52 Q1312.53 1549.52 1310.21 1549.08 Q1307.92 1548.66 1305.47 1547.81 L1305.47 1543.29 Q1307.41 1544.43 1309.73 1545.01 Q1312.04 1545.58 1314.56 1545.58 Q1318.96 1545.58 1321.25 1543.85 Q1323.57 1542.11 1323.57 1538.8 Q1323.57 1535.75 1321.42 1534.03 Q1319.29 1532.3 1315.47 1532.3 L1311.44 1532.3 L1311.44 1528.45 L1315.65 1528.45 Q1319.1 1528.45 1320.93 1527.09 Q1322.76 1525.7 1322.76 1523.11 Q1322.76 1520.45 1320.86 1519.03 Q1318.99 1517.6 1315.47 1517.6 Q1313.55 1517.6 1311.35 1518.01 Q1309.15 1518.43 1306.51 1519.31 L1306.51 1515.14 Q1309.17 1514.4 1311.49 1514.03 Q1313.82 1513.66 1315.88 1513.66 Q1321.21 1513.66 1324.31 1516.09 Q1327.41 1518.5 1327.41 1522.62 Q1327.41 1525.49 1325.77 1527.48 Q1324.12 1529.45 1321.09 1530.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip130)\" d=\"M1347.09 1517.37 Q1343.48 1517.37 1341.65 1520.93 Q1339.84 1524.47 1339.84 1531.6 Q1339.84 1538.71 1341.65 1542.27 Q1343.48 1545.82 1347.09 1545.82 Q1350.72 1545.82 1352.53 1542.27 Q1354.36 1538.71 1354.36 1531.6 Q1354.36 1524.47 1352.53 1520.93 Q1350.72 1517.37 1347.09 1517.37 M1347.09 1513.66 Q1352.9 1513.66 1355.95 1518.27 Q1359.03 1522.85 1359.03 1531.6 Q1359.03 1540.33 1355.95 1544.94 Q1352.9 1549.52 1347.09 1549.52 Q1341.28 1549.52 1338.2 1544.94 Q1335.14 1540.33 1335.14 1531.6 Q1335.14 1522.85 1338.2 1518.27 Q1341.28 1513.66 1347.09 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip130)\" d=\"M1689.5 1518.36 L1677.7 1536.81 L1689.5 1536.81 L1689.5 1518.36 M1688.27 1514.29 L1694.15 1514.29 L1694.15 1536.81 L1699.08 1536.81 L1699.08 1540.7 L1694.15 1540.7 L1694.15 1548.85 L1689.5 1548.85 L1689.5 1540.7 L1673.9 1540.7 L1673.9 1536.19 L1688.27 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip130)\" d=\"M1716.82 1517.37 Q1713.2 1517.37 1711.38 1520.93 Q1709.57 1524.47 1709.57 1531.6 Q1709.57 1538.71 1711.38 1542.27 Q1713.2 1545.82 1716.82 1545.82 Q1720.45 1545.82 1722.26 1542.27 Q1724.08 1538.71 1724.08 1531.6 Q1724.08 1524.47 1722.26 1520.93 Q1720.45 1517.37 1716.82 1517.37 M1716.82 1513.66 Q1722.63 1513.66 1725.68 1518.27 Q1728.76 1522.85 1728.76 1531.6 Q1728.76 1540.33 1725.68 1544.94 Q1722.63 1549.52 1716.82 1549.52 Q1711.01 1549.52 1707.93 1544.94 Q1704.87 1540.33 1704.87 1531.6 Q1704.87 1522.85 1707.93 1518.27 Q1711.01 1513.66 1716.82 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip130)\" d=\"M2045.11 1514.29 L2063.47 1514.29 L2063.47 1518.22 L2049.39 1518.22 L2049.39 1526.7 Q2050.41 1526.35 2051.43 1526.19 Q2052.45 1526 2053.47 1526 Q2059.25 1526 2062.63 1529.17 Q2066.01 1532.34 2066.01 1537.76 Q2066.01 1543.34 2062.54 1546.44 Q2059.07 1549.52 2052.75 1549.52 Q2050.57 1549.52 2048.3 1549.15 Q2046.06 1548.78 2043.65 1548.04 L2043.65 1543.34 Q2045.73 1544.47 2047.96 1545.03 Q2050.18 1545.58 2052.66 1545.58 Q2056.66 1545.58 2059 1543.48 Q2061.34 1541.37 2061.34 1537.76 Q2061.34 1534.15 2059 1532.04 Q2056.66 1529.94 2052.66 1529.94 Q2050.78 1529.94 2048.91 1530.35 Q2047.05 1530.77 2045.11 1531.65 L2045.11 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip130)\" d=\"M2085.22 1517.37 Q2081.61 1517.37 2079.78 1520.93 Q2077.98 1524.47 2077.98 1531.6 Q2077.98 1538.71 2079.78 1542.27 Q2081.61 1545.82 2085.22 1545.82 Q2088.86 1545.82 2090.66 1542.27 Q2092.49 1538.71 2092.49 1531.6 Q2092.49 1524.47 2090.66 1520.93 Q2088.86 1517.37 2085.22 1517.37 M2085.22 1513.66 Q2091.03 1513.66 2094.09 1518.27 Q2097.17 1522.85 2097.17 1531.6 Q2097.17 1540.33 2094.09 1544.94 Q2091.03 1549.52 2085.22 1549.52 Q2079.41 1549.52 2076.34 1544.94 Q2073.28 1540.33 2073.28 1531.6 Q2073.28 1522.85 2076.34 1518.27 Q2079.41 1513.66 2085.22 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip132)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  201.019,1135 2352.76,1135 \n  \"/>\n<polyline clip-path=\"url(#clip132)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  201.019,782.654 2352.76,782.654 \n  \"/>\n<polyline clip-path=\"url(#clip132)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  201.019,430.307 2352.76,430.307 \n  \"/>\n<polyline clip-path=\"url(#clip132)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  201.019,77.9592 2352.76,77.9592 \n  \"/>\n<polyline clip-path=\"url(#clip130)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  201.019,1486.45 201.019,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip130)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  201.019,1135 219.917,1135 \n  \"/>\n<polyline clip-path=\"url(#clip130)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  201.019,782.654 219.917,782.654 \n  \"/>\n<polyline clip-path=\"url(#clip130)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  201.019,430.307 219.917,430.307 \n  \"/>\n<polyline clip-path=\"url(#clip130)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  201.019,77.9592 219.917,77.9592 \n  \"/>\n<path clip-path=\"url(#clip130)\" d=\"M82.7975 1117.72 L101.154 1117.72 L101.154 1121.66 L87.0799 1121.66 L87.0799 1130.13 Q88.0984 1129.78 89.1169 1129.62 Q90.1354 1129.43 91.1539 1129.43 Q96.941 1129.43 100.321 1132.61 Q103.7 1135.78 103.7 1141.19 Q103.7 1146.77 100.228 1149.87 Q96.7558 1152.95 90.4364 1152.95 Q88.2604 1152.95 85.9919 1152.58 Q83.7466 1152.21 81.3392 1151.47 L81.3392 1146.77 Q83.4225 1147.91 85.6447 1148.46 Q87.8669 1149.02 90.3438 1149.02 Q94.3484 1149.02 96.6863 1146.91 Q99.0243 1144.8 99.0243 1141.19 Q99.0243 1137.58 96.6863 1135.48 Q94.3484 1133.37 90.3438 1133.37 Q88.4688 1133.37 86.5938 1133.79 Q84.7419 1134.2 82.7975 1135.08 L82.7975 1117.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip130)\" d=\"M122.913 1120.8 Q119.302 1120.8 117.473 1124.37 Q115.668 1127.91 115.668 1135.04 Q115.668 1142.14 117.473 1145.71 Q119.302 1149.25 122.913 1149.25 Q126.547 1149.25 128.353 1145.71 Q130.182 1142.14 130.182 1135.04 Q130.182 1127.91 128.353 1124.37 Q126.547 1120.8 122.913 1120.8 M122.913 1117.1 Q128.723 1117.1 131.779 1121.7 Q134.857 1126.29 134.857 1135.04 Q134.857 1143.76 131.779 1148.37 Q128.723 1152.95 122.913 1152.95 Q117.103 1152.95 114.024 1148.37 Q110.969 1143.76 110.969 1135.04 Q110.969 1126.29 114.024 1121.7 Q117.103 1117.1 122.913 1117.1 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip130)\" d=\"M153.075 1120.8 Q149.464 1120.8 147.635 1124.37 Q145.83 1127.91 145.83 1135.04 Q145.83 1142.14 147.635 1145.71 Q149.464 1149.25 153.075 1149.25 Q156.709 1149.25 158.515 1145.71 Q160.343 1142.14 160.343 1135.04 Q160.343 1127.91 158.515 1124.37 Q156.709 1120.8 153.075 1120.8 M153.075 1117.1 Q158.885 1117.1 161.941 1121.7 Q165.019 1126.29 165.019 1135.04 Q165.019 1143.76 161.941 1148.37 Q158.885 1152.95 153.075 1152.95 Q147.265 1152.95 144.186 1148.37 Q141.131 1143.76 141.131 1135.04 Q141.131 1126.29 144.186 1121.7 Q147.265 1117.1 153.075 1117.1 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip130)\" d=\"M53.3995 795.999 L61.0384 795.999 L61.0384 769.633 L52.7282 771.3 L52.7282 767.041 L60.9921 765.374 L65.668 765.374 L65.668 795.999 L73.3068 795.999 L73.3068 799.934 L53.3995 799.934 L53.3995 795.999 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip130)\" d=\"M92.7512 768.453 Q89.1401 768.453 87.3114 772.018 Q85.5058 775.559 85.5058 782.689 Q85.5058 789.795 87.3114 793.36 Q89.1401 796.902 92.7512 796.902 Q96.3854 796.902 98.1909 793.36 Q100.02 789.795 100.02 782.689 Q100.02 775.559 98.1909 772.018 Q96.3854 768.453 92.7512 768.453 M92.7512 764.749 Q98.5613 764.749 101.617 769.356 Q104.696 773.939 104.696 782.689 Q104.696 791.416 101.617 796.022 Q98.5613 800.606 92.7512 800.606 Q86.941 800.606 83.8623 796.022 Q80.8068 791.416 80.8068 782.689 Q80.8068 773.939 83.8623 769.356 Q86.941 764.749 92.7512 764.749 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip130)\" d=\"M122.913 768.453 Q119.302 768.453 117.473 772.018 Q115.668 775.559 115.668 782.689 Q115.668 789.795 117.473 793.36 Q119.302 796.902 122.913 796.902 Q126.547 796.902 128.353 793.36 Q130.182 789.795 130.182 782.689 Q130.182 775.559 128.353 772.018 Q126.547 768.453 122.913 768.453 M122.913 764.749 Q128.723 764.749 131.779 769.356 Q134.857 773.939 134.857 782.689 Q134.857 791.416 131.779 796.022 Q128.723 800.606 122.913 800.606 Q117.103 800.606 114.024 796.022 Q110.969 791.416 110.969 782.689 Q110.969 773.939 114.024 769.356 Q117.103 764.749 122.913 764.749 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip130)\" d=\"M153.075 768.453 Q149.464 768.453 147.635 772.018 Q145.83 775.559 145.83 782.689 Q145.83 789.795 147.635 793.36 Q149.464 796.902 153.075 796.902 Q156.709 796.902 158.515 793.36 Q160.343 789.795 160.343 782.689 Q160.343 775.559 158.515 772.018 Q156.709 768.453 153.075 768.453 M153.075 764.749 Q158.885 764.749 161.941 769.356 Q165.019 773.939 165.019 782.689 Q165.019 791.416 161.941 796.022 Q158.885 800.606 153.075 800.606 Q147.265 800.606 144.186 796.022 Q141.131 791.416 141.131 782.689 Q141.131 773.939 144.186 769.356 Q147.265 764.749 153.075 764.749 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip130)\" d=\"M53.3995 443.652 L61.0384 443.652 L61.0384 417.286 L52.7282 418.953 L52.7282 414.693 L60.9921 413.027 L65.668 413.027 L65.668 443.652 L73.3068 443.652 L73.3068 447.587 L53.3995 447.587 L53.3995 443.652 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip130)\" d=\"M82.7975 413.027 L101.154 413.027 L101.154 416.962 L87.0799 416.962 L87.0799 425.434 Q88.0984 425.087 89.1169 424.925 Q90.1354 424.74 91.1539 424.74 Q96.941 424.74 100.321 427.911 Q103.7 431.082 103.7 436.499 Q103.7 442.077 100.228 445.179 Q96.7558 448.258 90.4364 448.258 Q88.2604 448.258 85.9919 447.888 Q83.7466 447.517 81.3392 446.777 L81.3392 442.077 Q83.4225 443.212 85.6447 443.767 Q87.8669 444.323 90.3438 444.323 Q94.3484 444.323 96.6863 442.216 Q99.0243 440.11 99.0243 436.499 Q99.0243 432.888 96.6863 430.781 Q94.3484 428.675 90.3438 428.675 Q88.4688 428.675 86.5938 429.091 Q84.7419 429.508 82.7975 430.388 L82.7975 413.027 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip130)\" d=\"M122.913 416.105 Q119.302 416.105 117.473 419.67 Q115.668 423.212 115.668 430.341 Q115.668 437.448 117.473 441.013 Q119.302 444.554 122.913 444.554 Q126.547 444.554 128.353 441.013 Q130.182 437.448 130.182 430.341 Q130.182 423.212 128.353 419.67 Q126.547 416.105 122.913 416.105 M122.913 412.402 Q128.723 412.402 131.779 417.008 Q134.857 421.591 134.857 430.341 Q134.857 439.068 131.779 443.675 Q128.723 448.258 122.913 448.258 Q117.103 448.258 114.024 443.675 Q110.969 439.068 110.969 430.341 Q110.969 421.591 114.024 417.008 Q117.103 412.402 122.913 412.402 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip130)\" d=\"M153.075 416.105 Q149.464 416.105 147.635 419.67 Q145.83 423.212 145.83 430.341 Q145.83 437.448 147.635 441.013 Q149.464 444.554 153.075 444.554 Q156.709 444.554 158.515 441.013 Q160.343 437.448 160.343 430.341 Q160.343 423.212 158.515 419.67 Q156.709 416.105 153.075 416.105 M153.075 412.402 Q158.885 412.402 161.941 417.008 Q165.019 421.591 165.019 430.341 Q165.019 439.068 161.941 443.675 Q158.885 448.258 153.075 448.258 Q147.265 448.258 144.186 443.675 Q141.131 439.068 141.131 430.341 Q141.131 421.591 144.186 417.008 Q147.265 412.402 153.075 412.402 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip130)\" d=\"M56.6171 91.304 L72.9365 91.304 L72.9365 95.2392 L50.9921 95.2392 L50.9921 91.304 Q53.6541 88.5494 58.2375 83.9198 Q62.8439 79.267 64.0245 77.9244 Q66.2698 75.4013 67.1494 73.6652 Q68.0522 71.906 68.0522 70.2161 Q68.0522 67.4615 66.1078 65.7254 Q64.1865 63.9893 61.0847 63.9893 Q58.8856 63.9893 56.4319 64.7532 Q54.0014 65.5171 51.2236 67.068 L51.2236 62.3458 Q54.0477 61.2116 56.5014 60.6329 Q58.955 60.0542 60.9921 60.0542 Q66.3624 60.0542 69.5568 62.7393 Q72.7513 65.4245 72.7513 69.9152 Q72.7513 72.0448 71.9411 73.9661 Q71.1541 75.8643 69.0476 78.4568 Q68.4689 79.1281 65.367 82.3457 Q62.2652 85.5401 56.6171 91.304 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip130)\" d=\"M92.7512 63.7578 Q89.1401 63.7578 87.3114 67.3226 Q85.5058 70.8643 85.5058 77.9939 Q85.5058 85.1003 87.3114 88.6651 Q89.1401 92.2068 92.7512 92.2068 Q96.3854 92.2068 98.1909 88.6651 Q100.02 85.1003 100.02 77.9939 Q100.02 70.8643 98.1909 67.3226 Q96.3854 63.7578 92.7512 63.7578 M92.7512 60.0542 Q98.5613 60.0542 101.617 64.6606 Q104.696 69.2439 104.696 77.9939 Q104.696 86.7207 101.617 91.3271 Q98.5613 95.9105 92.7512 95.9105 Q86.941 95.9105 83.8623 91.3271 Q80.8068 86.7207 80.8068 77.9939 Q80.8068 69.2439 83.8623 64.6606 Q86.941 60.0542 92.7512 60.0542 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip130)\" d=\"M122.913 63.7578 Q119.302 63.7578 117.473 67.3226 Q115.668 70.8643 115.668 77.9939 Q115.668 85.1003 117.473 88.6651 Q119.302 92.2068 122.913 92.2068 Q126.547 92.2068 128.353 88.6651 Q130.182 85.1003 130.182 77.9939 Q130.182 70.8643 128.353 67.3226 Q126.547 63.7578 122.913 63.7578 M122.913 60.0542 Q128.723 60.0542 131.779 64.6606 Q134.857 69.2439 134.857 77.9939 Q134.857 86.7207 131.779 91.3271 Q128.723 95.9105 122.913 95.9105 Q117.103 95.9105 114.024 91.3271 Q110.969 86.7207 110.969 77.9939 Q110.969 69.2439 114.024 64.6606 Q117.103 60.0542 122.913 60.0542 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip130)\" d=\"M153.075 63.7578 Q149.464 63.7578 147.635 67.3226 Q145.83 70.8643 145.83 77.9939 Q145.83 85.1003 147.635 88.6651 Q149.464 92.2068 153.075 92.2068 Q156.709 92.2068 158.515 88.6651 Q160.343 85.1003 160.343 77.9939 Q160.343 70.8643 158.515 67.3226 Q156.709 63.7578 153.075 63.7578 M153.075 60.0542 Q158.885 60.0542 161.941 64.6606 Q165.019 69.2439 165.019 77.9939 Q165.019 86.7207 161.941 91.3271 Q158.885 95.9105 153.075 95.9105 Q147.265 95.9105 144.186 91.3271 Q141.131 86.7207 141.131 77.9939 Q141.131 69.2439 144.186 64.6606 Q147.265 60.0542 153.075 60.0542 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip132)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  261.917,1268.76 298.826,1323.77 335.734,1287.31 372.642,627.43 409.55,1046.74 446.458,297.568 483.366,887.367 520.274,272.093 557.182,780.295 594.09,763.146 \n  630.998,87.9763 667.906,162.777 704.814,1354.54 741.722,537.045 778.63,846.432 815.538,527.462 852.446,456.542 889.354,310.502 926.262,1266 963.17,1339.6 \n  1000.08,315.924 1036.99,488.12 1073.89,603.487 1110.8,997.279 1147.71,746.226 1184.62,654.495 1221.53,758.833 1258.43,1262.26 1295.34,712.553 1332.25,1105.82 \n  1369.16,166.25 1406.07,150.317 1442.97,1172.51 1479.88,327.47 1516.79,1242.13 1553.7,895.103 1590.61,824.244 1627.51,239.248 1664.42,413.565 1701.33,859.45 \n  1738.24,663.715 1775.15,691.221 1812.05,951.6 1848.96,1244.55 1885.87,437.631 1922.78,1271.76 1959.69,639.334 1996.59,871.453 2033.5,1037.06 2070.41,1409.44 \n  2107.32,955.31 2144.23,364.569 2181.13,429.434 2218.04,1113.48 2254.95,974.783 2291.86,1445.72 \n  \"/>\n<path clip-path=\"url(#clip130)\" d=\"\nM1989.52 198.898 L2281.03 198.898 L2281.03 95.2176 L1989.52 95.2176  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<polyline clip-path=\"url(#clip130)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1989.52,198.898 2281.03,198.898 2281.03,95.2176 1989.52,95.2176 1989.52,198.898 \n  \"/>\n<polyline clip-path=\"url(#clip130)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  2013.43,147.058 2156.88,147.058 \n  \"/>\n<path clip-path=\"url(#clip130)\" d=\"M2194.63 166.745 Q2192.82 171.375 2191.11 172.787 Q2189.4 174.199 2186.53 174.199 L2183.12 174.199 L2183.12 170.634 L2185.62 170.634 Q2187.38 170.634 2188.35 169.8 Q2189.33 168.967 2190.51 165.865 L2191.27 163.921 L2180.78 138.412 L2185.3 138.412 L2193.4 158.689 L2201.5 138.412 L2206.02 138.412 L2194.63 166.745 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip130)\" d=\"M2213.31 160.402 L2220.95 160.402 L2220.95 134.037 L2212.64 135.703 L2212.64 131.444 L2220.9 129.778 L2225.58 129.778 L2225.58 160.402 L2233.21 160.402 L2233.21 164.338 L2213.31 164.338 L2213.31 160.402 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(episode_test_reward_hook.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb0d4ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "close_visualization(); # closes the MeshCat visualization"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
