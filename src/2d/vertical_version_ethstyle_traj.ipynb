{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0649b76a",
   "metadata": {},
   "source": [
    "# Init Bionic VTOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96751412",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../Flyonic.jl\");\n",
    "using .Flyonic;\n",
    "\n",
    "using Rotations; # used for initial position\n",
    "\n",
    "using ReinforcementLearning;\n",
    "using StableRNGs;\n",
    "using Flux;\n",
    "using Flux.Losses;\n",
    "using Random;\n",
    "using IntervalSets;\n",
    "using LinearAlgebra;\n",
    "using Distributions;\n",
    "\n",
    "using Plots;\n",
    "using Statistics;\n",
    "\n",
    "using BSON: @save, @load # save mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7e4ee64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: MeshCat server started. You can open the visualizer by visiting the following URL in your browser:\n",
      "│ http://127.0.0.1:8700\n",
      "└ @ MeshCat /home/larissa/.julia/packages/MeshCat/Ax8pH/src/visualizer.jl:73\n"
     ]
    }
   ],
   "source": [
    "create_visualization();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d9557df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indicates how many threads Julia was started with. This is important for the multi-threaded environment\n",
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0589d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo is gravity really on?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5411db62",
   "metadata": {},
   "source": [
    "# Create Reinforcement Learning Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96af6ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct VtolEnv{A,T,ACT,R<:AbstractRNG} <: AbstractEnv # Parametric Constructor for a subtype of AbstractEnv\n",
    "    action_space::A # action space\n",
    "    observation_space::Space{Vector{ClosedInterval{T}}} # observation space\n",
    "    state::Vector{T} # current state space\n",
    "    action::ACT # action space\n",
    "    done::Bool # done\n",
    "    t::T # time\n",
    "    rng::R # random number generator\n",
    "\n",
    "    name::String # for multible environoments\n",
    "    visualization::Bool # visualization\n",
    "    realtime::Bool # realtime\n",
    "    \n",
    "    # Everything you need aditionaly can also go in here.\n",
    "    x_W::Vector{T} # current position\n",
    "    v_B::Vector{T} # velocity\n",
    "    R_W::Matrix{T} # current rotation\n",
    "    ω_B::Vector{T} # rotation velocitiy\n",
    "    wind_W::Vector{T} # wind\n",
    "    Δt::T # Δ time\n",
    "    \n",
    "    # Current Bonus / Target\n",
    "    num_waypoints::Int # includig start point\n",
    "    waypoints::Vector{Vector{T}}\n",
    "    reached_goal::BitVector\n",
    "    \n",
    "    progress::T\n",
    "    progress_prev::T\n",
    "    current_point::Int\n",
    "    reached_goal_in_step::Bool\n",
    "    \n",
    "    r_tol::T # tolerance within drones has to reach waypoint\n",
    "    projected_position::Vector{T} # projected position of drone along trajectory\n",
    "\n",
    "    slow_mode::Bool # slow flight learning mode\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4ede3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_TOL = 0.5\n",
    "N_WAYPOINTS = 4\n",
    "SLOW_MODE = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14a6873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a keyword-based constructor for the type declared in the mutable struct typedef. \n",
    "# It could also be done with the macro Base.@kwdef.\n",
    "function VtolEnv(;\n",
    "    rng = Random.GLOBAL_RNG, # Random number generation\n",
    "    name = \"vtol\",\n",
    "    visualization = false,\n",
    "    realtime = false,\n",
    "    kwargs... # let the function take an arbitrary number of keyword arguments \n",
    ")\n",
    "    \n",
    "    T = Float64; # explicit type which is used e.g. in state. Cannot be altered due to the poor matrix defininon.\n",
    "\n",
    "    \n",
    "    action_space = Space(\n",
    "        ClosedInterval{T}[\n",
    "            0.0..2.0, # propeller 1\n",
    "            0.0..2.0, # propeller 2\n",
    "            ], \n",
    "    )\n",
    "\n",
    "    \n",
    "    state_space = Space( # Three continuous values in state space.\n",
    "        ClosedInterval{T}[\n",
    "            # orientate yourself on the state space from the paper\n",
    "            typemin(T)..typemax(T), # position along x\n",
    "            typemin(T)..typemax(T), # position along z\n",
    "            \n",
    "            typemin(T)..typemax(T), # orientation along x\n",
    "            typemin(T)..typemax(T), # orientation along z\n",
    "            \n",
    "            typemin(T)..typemax(T), # velocity along x BODY coordinates\n",
    "            typemin(T)..typemax(T), # velocity along y BODY coordinates\n",
    "            \n",
    "            typemin(T)..typemax(T), # rotational velocity along z BODY coordinates\n",
    "            \n",
    "            typemin(T)..typemax(T), # position error along x (next gate - current position)\n",
    "            typemin(T)..typemax(T), # position error along z (next gate - current position)\n",
    "            \n",
    "            typemin(T)..typemax(T), # way to next next gate x (next next gate - next gate)\n",
    "            typemin(T)..typemax(T), # way to next next gate z (next next gate - next gate)\n",
    "            # TODO: more points?\n",
    "            ], \n",
    "    )\n",
    "    \n",
    "    num_waypoints = N_WAYPOINTS # number of waypoints, includig start point\n",
    "    waypoints = generate_trajectory(num_waypoints) # trajectory with num_waypoints waypoints (+ start point) \n",
    "    reached_goal = falses(num_waypoints)\n",
    "    \n",
    "    if visualization #visualizes VTOL and waypoints\n",
    "        create_VTOL(name, actuators = true, color_vec=[1.0; 1.0; 0.6; 1.0]);\n",
    "        visualize_waypoints(waypoints, 0.15)\n",
    "    end\n",
    "\n",
    "    environment = VtolEnv(\n",
    "        action_space,\n",
    "        state_space, \n",
    "        zeros(T, length(state_space)), # current state, needs to be extended\n",
    "        rand(action_space), #initialization action\n",
    "        false, # episode done \n",
    "        0.0, # time\n",
    "        rng, # random number generator  \n",
    "        \n",
    "        name,\n",
    "        visualization,\n",
    "        realtime,\n",
    "        \n",
    "        zeros(T, 3), # x_W, current position\n",
    "        zeros(T, 3), # v_B, velocity\n",
    "        [1.0 0.0 0.0; 0.0 1.0 0.0; 0.0 0.0 1.0], # R_W, current rotation, Float64... so T needs to be Float64\n",
    "        zeros(T, 3), # ω_B\n",
    "        zeros(T, 3), # wind_W\n",
    "        T(0.025), # Δt \n",
    "        \n",
    "        num_waypoints, # includig start point\n",
    "        waypoints, \n",
    "        reached_goal,\n",
    "        \n",
    "        0.0, # progress\n",
    "        0.0, # progress_prev\n",
    "        2, # current point\n",
    "        false, # reached_goal_in_step\n",
    "        \n",
    "        R_TOL, # r_tol\n",
    "        zeros(T, 3), # projected_position\n",
    "\n",
    "        SLOW_MODE # slow_mode\n",
    "    )\n",
    "    \n",
    "    \n",
    "    reset!(environment)\n",
    "    \n",
    "    return environment\n",
    "    \n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec660d5e",
   "metadata": {},
   "source": [
    "Just for explanation:\n",
    "\n",
    "1. A mutable Struct is created. A struct is a constructor and a constructor is a function that creates new objects.\n",
    "2. A outer keyword-based constructor method is added for the type declared in the mutable struct typedef before.\n",
    "\n",
    "So now we have a function with two methods. Julia will decide which method to call by multiple dispatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc3c4bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "# 2 methods for type constructor:<ul><li> VtolEnv(; <i>rng, name, visualization, realtime, kwargs...</i>) in Main at In[6]:3</li> <li> VtolEnv(action_space::<b>A</b>, observation_space::<b>Space{Array{ClosedInterval{T}, 1}}</b>, state::<b>Vector{T}</b>, action::<b>ACT</b>, done::<b>Bool</b>, t::<b>T</b>, rng::<b>R</b>, name::<b>String</b>, visualization::<b>Bool</b>, realtime::<b>Bool</b>, x_W::<b>Vector{T}</b>, v_B::<b>Vector{T}</b>, R_W::<b>Matrix{T}</b>, ω_B::<b>Vector{T}</b>, wind_W::<b>Vector{T}</b>, Δt::<b>T</b>, num_waypoints::<b>Int64</b>, waypoints::<b>Array{Vector{T}, 1}</b>, reached_goal::<b>BitVector</b>, progress::<b>T</b>, progress_prev::<b>T</b>, current_point::<b>Int64</b>, reached_goal_in_step::<b>Bool</b>, r_tol::<b>T</b>, projected_position::<b>Vector{T}</b>, slow_mode::<b>Bool</b>)<i> where {A, T, ACT, R<:AbstractRNG}</i> in Main at In[4]:2</li> </ul>"
      ],
      "text/plain": [
       "# 2 methods for type constructor:\n",
       "[1] VtolEnv(; rng, name, visualization, realtime, kwargs...) in Main at In[6]:3\n",
       "[2] VtolEnv(action_space::A, observation_space::Space{Array{ClosedInterval{T}, 1}}, state::Vector{T}, action::ACT, done::Bool, t::T, rng::R, name::String, visualization::Bool, realtime::Bool, x_W::Vector{T}, v_B::Vector{T}, R_W::Matrix{T}, ω_B::Vector{T}, wind_W::Vector{T}, Δt::T, num_waypoints::Int64, waypoints::Array{Vector{T}, 1}, reached_goal::BitVector, progress::T, progress_prev::T, current_point::Int64, reached_goal_in_step::Bool, r_tol::T, projected_position::Vector{T}, slow_mode::Bool) where {A, T, ACT, R<:AbstractRNG} in Main at In[4]:2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methods(VtolEnv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806413d1",
   "metadata": {},
   "source": [
    "# Define the RL interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f822029",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(env::VtolEnv, seed) = Random.seed!(env.rng, seed)\n",
    "RLBase.action_space(env::VtolEnv) = env.action_space\n",
    "RLBase.state_space(env::VtolEnv) = env.observation_space\n",
    "RLBase.is_terminated(env::VtolEnv) = env.done\n",
    "RLBase.state(env::VtolEnv) = env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b48de74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scale_for_slowmode (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function scale_for_slowmode(slow_mode::Bool, v_min::T, v_max::T, d_max::T, x_W::Vector{T}, projected_position::Vector{T}, v_B::Vector{T}) where T\n",
    "    # TODO safe in utils\n",
    "    if slow_mode == false\n",
    "        return 1\n",
    "    else\n",
    "        if norm(v_B) > v_max\n",
    "            s_vmax = 10^(v_max - norm(v_B))\n",
    "        else\n",
    "            s_vmax = 1\n",
    "        end\n",
    "\n",
    "        if norm(v_B) < v_min\n",
    "            s_vmin = 10^(norm(v_B) - v_min)\n",
    "        else\n",
    "            s_vmin = 1\n",
    "        end\n",
    "\n",
    "        if norm(x_W - projected_position) > d_max\n",
    "            s_gd = exp(-norm(x_W - projected_position) + d_max)\n",
    "        else\n",
    "            s_gd = 1\n",
    "        end\n",
    "        s = s_vmax * s_vmin * s_gd\n",
    "    end\n",
    "    return s\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f7fb89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "function computeReward(env::VtolEnv{A,T}) where {A,T}\n",
    "    \n",
    "    norm_way = 0.0 # TODO: eventually save in environment\n",
    "    for i in 1:(env.num_waypoints - 1)\n",
    "        norm_way += norm(env.waypoints[i] - env.waypoints[i + 1])\n",
    "    end\n",
    "\n",
    "    v_min = 1.0 # min velocity\n",
    "    v_max = 6.0 # max velocity\n",
    "    d_max = 0.25 \n",
    "\n",
    "\n",
    "    s = scale_for_slowmode(env.slow_mode, v_min, v_max, d_max, env.x_W, env.projected_position, env.v_B)\n",
    "    \n",
    "\n",
    "    k_p = 5.0 * s #env.num_waypoints / norm_way;# factor for progress (between current position and last position) reward \n",
    "    r_p = (env.progress - env.progress_prev); # reward for progress (between current position and last position)\n",
    "\n",
    "    k_s = s * (2 * v_max * env.Δt)/norm_way #5.0 # factor for reached distance (overall) reward, TODO later add factor as in paper (p. 4)\n",
    "    r_s = env.progress # reward for reached distance (overall)\n",
    "    \n",
    "    k_wp = 10.0 * env.num_waypoints # factor for reached gate reward\n",
    "    r_wp = 0.0 # reward for reached gate, TODO: change to gates later (when gates != waypoints)\n",
    "    if env.reached_goal_in_step\n",
    "        r_wp = exp(-norm(env.x_W - env.waypoints[env.current_point - 1])/env.r_tol)\n",
    "    end \n",
    "\n",
    "    k_ω = 0.01 # factor for too high body rate penalty\n",
    "    norm_ω = norm(env.ω_B[3]) # penalty for body rate\n",
    "\n",
    "    if env.x_W[3] < -2\n",
    "        fall = 1\n",
    "    else\n",
    "        fall = 0\n",
    "    end\n",
    "\n",
    "    return k_p * r_p + k_s * r_s + k_wp * r_wp - k_ω * norm_ω - fall # - k_v * norm_v\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "RLBase.reward(env::VtolEnv{A,T}) where {A,T} = computeReward(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae45ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "function RLBase.reset!(env::VtolEnv{A,T}) where {A,T}\n",
    "    # Visualize initial state\n",
    "    if env.visualization\n",
    "        set_transform(env.name, env.x_W, QuatRotation(env.R_W));\n",
    "        set_actuators(env.name, [0.0; 0.0; 0.0; 0.0]);\n",
    "    end\n",
    "    \n",
    "    env.x_W = [0.0; 0.0; 0.0];\n",
    "    env.v_B = [0.0; 0.0; 0.0];\n",
    "    env.R_W = Matrix(UnitQuaternion(RotZ(-pi/2.0)*RotY(-pi/2.0)*RotX(pi)));\n",
    "\n",
    "    env.ω_B = [0.0; 0.0; 0.0];\n",
    "    env.wind_W = [0.0; 0.0; 0.0];\n",
    "    \n",
    "    #env.num_waypoints = 4; # includig start point\n",
    "    env.waypoints = generate_trajectory(env.num_waypoints);\n",
    "    env.reached_goal = falses(env.num_waypoints);\n",
    "    env.reached_goal[1] = true; # set first point to reached (start point)\n",
    "    \n",
    "    env.current_point = 2;\n",
    "    env.reached_goal_in_step = false;\n",
    "    #env.r_tol = 0.3;\n",
    "    \n",
    "    if env.visualization\n",
    "        visualize_waypoints(env.waypoints, 0.15); # debug: other radius?\n",
    "    end\n",
    "    \n",
    "\n",
    "    env.progress = 0.0;\n",
    "    env.progress_prev = 0.0;\n",
    "    \n",
    "    \n",
    "    env.state = [env.x_W[1]; # position along x\n",
    "                 env.x_W[3]; # position along z\n",
    "        \n",
    "                 env.R_W[1,1]; # orientation along x\n",
    "                 env.R_W[3,1]; # orientation along z\n",
    "        \n",
    "                 env.v_B[1]; # velocity along x BODY coordinates\n",
    "                 env.v_B[2]; # velocity along y BODY coordinates  \n",
    "        \n",
    "                 env.ω_B[3]; # rotational velocity along z BODY coordinates\n",
    "        \n",
    "                 env.waypoints[2][1] - env.x_W[1]; # position error along x\n",
    "                 env.waypoints[2][3] - env.x_W[3]; # position error along z\n",
    "                 \n",
    "                 0.0; # way to next next gate x (next next gate - next gate)\n",
    "                 0.0] # way to next next gate z (next next gate - next gate)\n",
    "    \n",
    "    if env.num_waypoints >= 3\n",
    "        env.state[10] = env.waypoints[3][1] - env.waypoints[2][1]; # way to next next gate x (next next gate - next gate)\n",
    "        env.state[11] = env.waypoints[3][3] - env.waypoints[2][1]; # way to next next gate z (next next gate - next gate)\n",
    "    end\n",
    "        \n",
    "    env.t = 0.0;\n",
    "    env.action = [0.0, 0.0];\n",
    "    env.done = false;\n",
    "\n",
    "    env.projected_position = [0; 0; 0]\n",
    "    \n",
    "    nothing\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cf1a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines a methods for a callable object.\n",
    "# So when a VtolEnv object is created, it has this method that can be called\n",
    "function (env::VtolEnv)(a)\n",
    "    # TODO: set flaps later in 3D\n",
    "    # set the propeller trust and the two flaps 2D case\n",
    "    next_action = [a[1], a[2], 0.0, 0.0]\n",
    "   \n",
    "    _step!(env, next_action)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0e9eb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VtolEnv();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26a116cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "# 3 methods for callable object:<ul><li> (env::<b>VtolEnv</b>)(a) in Main at In[12]:3</li> <li> (env::<b>AbstractEnv</b>)(action) in ReinforcementLearningBase</li> <li> (env::<b>AbstractEnv</b>)(action, player) in ReinforcementLearningBase</li> </ul>"
      ],
      "text/plain": [
       "# 3 methods for callable object:\n",
       "[1] (env::VtolEnv)(a) in Main at In[12]:3\n",
       "[2] (env::AbstractEnv)(action) in ReinforcementLearningBase\n",
       "[3] (env::AbstractEnv)(action, player) in ReinforcementLearningBase"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methods(env) # Just to explain which methods the object has"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad88655",
   "metadata": {},
   "source": [
    "TODO:\n",
    "evtl einfügen, dass wenn man über ziel drüber fliegt trotzdem den current point updatet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e7d4727",
   "metadata": {},
   "outputs": [],
   "source": [
    "function _step!(env::VtolEnv, next_action)\n",
    "    # caluclate wind impact\n",
    "    v_in_wind_B = vtol_add_wind(env.v_B, env.R_W, env.wind_W)\n",
    "    # caluclate aerodynamic forces\n",
    "    torque_B, force_B = vtol_model(v_in_wind_B, next_action, eth_vtol_param);\n",
    "    # Limit to 2D\n",
    "    force_B[3] = 0.0; # Body Z\n",
    "    env.v_B[3] = 0.0;\n",
    "    torque_B[1] = 0.0; torque_B[2] = 0.0;  # Body X and Y\n",
    "    env.ω_B[1] = 0.0; env.ω_B[2] = 0.0;\n",
    "    # integrate rigid body dynamics for Δt\n",
    "    env.x_W, env.v_B, env.R_W, env.ω_B, time = rigid_body_simple(torque_B, force_B, env.x_W, env.v_B, env.R_W, env.ω_B, env.t, env.Δt, eth_vtol_param)\n",
    "    \n",
    "    \n",
    "    env.reached_goal_in_step = false;\n",
    "    if norm(env.x_W - env.waypoints[env.current_point]) < env.r_tol\n",
    "        env.reached_goal_in_step = true;\n",
    "        env.reached_goal[env.current_point] = true;\n",
    "        env.current_point += 1;\n",
    "    end\n",
    "        \n",
    "            \n",
    "    # calculate progress on trajectory\n",
    "    env.progress_prev = env.progress\n",
    "    \n",
    "    current_progress = 0.0\n",
    "    line_segment, env.projected_position = calculate_progress(env.waypoints, env.x_W)\n",
    "    \n",
    "    #env.current_point = line_segment + 1\n",
    "\n",
    "    for i in 2:(line_segment)\n",
    "       current_progress +=  norm(env.waypoints[i] - env.waypoints[i - 1])  \n",
    "    end\n",
    "    current_progress += norm(env.waypoints[line_segment] - env.projected_position)\n",
    "    \n",
    "    env.progress = current_progress\n",
    "    \n",
    "\n",
    "    if env.realtime\n",
    "        sleep(env.Δt) # TODO: just a dirty hack. this is of course slower than real time.\n",
    "    end\n",
    "\n",
    "    # Visualize the new state \n",
    "    if env.visualization\n",
    "        set_transform(env.name, env.x_W, QuatRotation(env.R_W));\n",
    "        set_actuators(env.name, next_action)\n",
    "        \n",
    "        for i in eachindex(env.reached_goal)\n",
    "            if env.reached_goal[i]\n",
    "                create_sphere(\"fixgoal_$i\", 0.2, color=RGBA{Float32}(1.0, 0.0, 0.0, 1.0));\n",
    "                set_transform(\"fixgoal_$i\", env.waypoints[i]);\n",
    "            end\n",
    "        end\n",
    "    end\n",
    " \n",
    "\n",
    "    env.t += env.Δt\n",
    "    \n",
    "    env.state[1] = env.x_W[1]; # position along x\n",
    "    env.state[2] = env.x_W[3]; # position along z\n",
    "    \n",
    "    env.state[3] = env.R_W[1,1]; # orientation along x\n",
    "    env.state[4] = env.R_W[3,1]; # orientation along z\n",
    "    \n",
    "    env.state[5] = env.v_B[1]; # velocity along x BODY coordinates\n",
    "    env.state[6] = env.v_B[2]; # velocity along y BODY coordinates\n",
    "    \n",
    "    env.state[7] = env.ω_B[3];  # rotational velocity along z BODY coordinates\n",
    "    \n",
    "    \n",
    "    if env.current_point <= env.num_waypoints\n",
    "        env.state[8] = env.waypoints[env.current_point][1] - env.x_W[1]; # position error along x\n",
    "        env.state[9] = env.waypoints[env.current_point][3] - env.x_W[3]; # position error along z\n",
    "        \n",
    "        if env.current_point <= env.num_waypoints - 1\n",
    "            env.state[10] = env.waypoints[env.current_point + 1][1] - env.waypoints[env.current_point][1]; # way to next next gate x (next next gate - next gate)\n",
    "            env.state[11] = env.waypoints[env.current_point + 1][3] - env.waypoints[env.current_point][3]; # way to next next gate z (next next gate - next gate)\n",
    "        else\n",
    "            env.state[10] = 0.0 # way to next next gate x (next next gate - next gate)\n",
    "            env.state[11] = 0.0 # way to next next gate z (next next gate - next gate)\n",
    "        end\n",
    "    else\n",
    "        env.state[8] = 0.0; # position error along x\n",
    "        env.state[9] = 0.0; # position error along z\n",
    "        env.state[10] = 0.0 # way to next next gate x (next next gate - next gate)\n",
    "        env.state[11] = 0.0 # way to next next gate z (next next gate - next gate)\n",
    "    end\n",
    "        \n",
    "    \n",
    "    # Termination criteria\n",
    "    # TODO: Use many termination criteria so that you do not train unnecessarily in wrong areas\n",
    "    env.done = #true\n",
    "        # After time... How fast is drone+Range of desired point\n",
    "        # After reaching position (circle of r_tol)\n",
    "        norm(env.ω_B) > 100.0 || \n",
    "        norm(env.v_B) > 100.0 || # stop if body is too fast # TODO: set higher later in fast training phase\n",
    "        env.x_W[3] < -5.0 || # stop if body is below -5m\n",
    "        env.t > env.num_waypoints * 10.0 ||# stop after 10s per point\n",
    "        norm(env.x_W - env.projected_position) > 5.0 || # too far off the path \n",
    "        env.current_point > env.num_waypoints #||# all points reached\n",
    "        norm(env.x_W - env.waypoints[end])<env.r_tol\n",
    "\n",
    "    nothing\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c37e8e",
   "metadata": {},
   "source": [
    "changed to 10s (5s before) per point and 5.0m too far off path (2.0 before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e1cd988",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[1mTest Summary:              | \u001b[22m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal  \u001b[22m\u001b[39m\u001b[0m\u001b[1mTime\u001b[22m\n",
      "random policy with VtolEnv | \u001b[32m2000  \u001b[39m\u001b[36m 2000  \u001b[39m\u001b[0m2.5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"random policy with VtolEnv\", Any[], 2000, false, false, true, 1.67275324664309e9, 1.672753249153092e9)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RLBase.test_runnable!(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c223a31f",
   "metadata": {},
   "source": [
    "Show an overview of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe6de74",
   "metadata": {},
   "source": [
    "# Setup of a reinforcement learning experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5683fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "MultiThreadEnv(8 x VtolEnv)"
      ],
      "text/plain": [
       "MultiThreadEnv(8 x VtolEnv)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 123    \n",
    "rng = StableRNG(seed)\n",
    "    N_ENV = 8\n",
    "    UPDATE_FREQ = 1024\n",
    "    \n",
    "    \n",
    "    # define multiple environments for parallel training\n",
    "    env = MultiThreadEnv([\n",
    "        # use different names for the visualization\n",
    "        VtolEnv(; rng = StableRNG(hash(seed+i)), name = \"vtol$i\") for i in 1:N_ENV\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1f128b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function approximator\n",
    "# TODO: change architecture eventually \n",
    "    ns, na = length(state(env[1])), length(action_space(env[1]))\n",
    "    approximator = ActorCritic(\n",
    "                actor = GaussianNetwork(\n",
    "                    pre = Chain(\n",
    "                    Dense(ns, 128, tanh; initW = glorot_uniform(rng)),#\n",
    "                    Dense(128, 128, tanh; initW = glorot_uniform(rng)),\n",
    "                    ),\n",
    "                    μ = Chain(Dense(128, na; initW = glorot_uniform(rng))),\n",
    "                    logσ = Chain(Dense(128, na; initW = glorot_uniform(rng))),\n",
    "                ),\n",
    "                critic = Chain(\n",
    "                    Dense(ns, 128, tanh; initW = glorot_uniform(rng)),\n",
    "                    Dense(128, 128, tanh; initW = glorot_uniform(rng)),\n",
    "                    Dense(128, 1; initW = glorot_uniform(rng)),\n",
    "                ),\n",
    "                optimizer = ADAM(1e-3),\n",
    "            );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ea4c37c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: The GPU function is being called but the GPU is not accessible. \n",
      "│ Defaulting back to the CPU. (No action is required if you want to run on the CPU).\n",
      "└ @ Flux /home/larissa/.julia/packages/Flux/7nTyc/src/functor.jl:187\n"
     ]
    }
   ],
   "source": [
    "    agent = Agent( # A wrapper of an AbstractPolicy\n",
    "        # AbstractPolicy: the policy to use\n",
    "        # TODO: change eventually\n",
    "        policy = PPOPolicy(;\n",
    "                    approximator = approximator |> gpu,\n",
    "                    update_freq=UPDATE_FREQ,\n",
    "                    dist = Normal,\n",
    "                    # For parameters visit the docu: https://juliareinforcementlearning.org/docs/rlzoo/#ReinforcementLearningZoo.PPOPolicy\n",
    "                    ),\n",
    "        \n",
    "        # AbstractTrajectory: used to store transitions between an agent and an environment source\n",
    "        trajectory = PPOTrajectory(;\n",
    "            capacity = UPDATE_FREQ,\n",
    "            state = Matrix{Float64} => (ns, N_ENV),\n",
    "            action = Matrix{Float64} => (na, N_ENV),\n",
    "            action_log_prob = Vector{Float64} => (N_ENV,),\n",
    "            reward = Vector{Float64} => (N_ENV,),\n",
    "            terminal = Vector{Bool} => (N_ENV,),\n",
    "        ),\n",
    "    );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f158a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "function saveModel(t, agent, env)\n",
    "    model = cpu(agent.policy.approximator)   \n",
    "    f = joinpath(\"./RL_models_fast/\", \"vtol_2D_ppo_$t.bson\") # TODO: evtl anpassen\n",
    "    @save f model\n",
    "    println(\"parameters at step $t saved to $f\")\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c689a72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "function loadModel()\n",
    "    f = joinpath(\"./RL_models_slow/\", \"vtol_2D_ppo_700000.bson\") # TODO: evtl anpassen\n",
    "    @load f model\n",
    "    return model\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3c1858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "function validate_policy(t, agent, env)\n",
    "    run(agent.policy, test_env, StopAfterEpisode(1), episode_test_reward_hook)\n",
    "    # the result of the hook\n",
    "    println(\"test reward at step $t: $(episode_test_reward_hook.rewards[end])\")\n",
    "    \n",
    "end;\n",
    "\n",
    "episode_test_reward_hook = TotalRewardPerEpisode(;is_display_on_exit=false)\n",
    "# create a env only for reward test\n",
    "test_env = VtolEnv(;name = \"testVTOL\", visualization = true, realtime = true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af72d48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.policy.approximator = loadModel(); # TODO: un/comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb737010",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   1%|▌                                        |  ETA: 0:17:07\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 10000: 144.3519352691646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   3%|█▏                                       |  ETA: 0:20:38\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 20000: 165.33170811655472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   4%|█▊                                       |  ETA: 0:33:28\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 30000: 281.6322946273957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   5%|██▎                                      |  ETA: 0:30:14\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 40000: 449.65951576453955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   7%|██▉                                      |  ETA: 0:36:51\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 50000: 155.29101294291692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   9%|███▌                                     |  ETA: 0:34:09\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 60000: 175.1424016273894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  10%|████                                     |  ETA: 0:33:08\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 70000: 114.20958635947875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  11%|████▋                                    |  ETA: 0:31:15\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 80000: 173.7060442832955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  13%|█████▎                                   |  ETA: 0:31:07\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 90000: 160.1551013500934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  14%|█████▊                                   |  ETA: 0:29:30\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 100000 saved to ./RL_models_slow/vtol_2D_ppo_100000.bson\n",
      "test reward at step 100000: 368.74828172610165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  16%|██████▍                                  |  ETA: 0:31:41\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 110000: 178.98435905737597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  17%|███████                                  |  ETA: 0:30:37\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 120000: 160.45975096118443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  19%|███████▋                                 |  ETA: 0:29:37\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 130000: 131.72434004385875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  20%|████████▏                                |  ETA: 0:28:21\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 140000: 108.17879362356811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  21%|████████▋                                |  ETA: 0:27:22\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 150000: 178.35680746923512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  23%|█████████▍                               |  ETA: 0:26:26\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 160000: 190.52586435672225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  24%|█████████▉                               |  ETA: 0:27:49\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 170000: 424.59046040344265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  26%|██████████▌                              |  ETA: 0:28:40\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 180000: 142.7733163259469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  27%|███████████▏                             |  ETA: 0:27:42\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 190000: 150.70935243822498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  28%|███████████▋                             |  ETA: 0:26:42\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 200000 saved to ./RL_models_slow/vtol_2D_ppo_200000.bson\n",
      "test reward at step 200000: 262.0110946384369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  30%|████████████▎                            |  ETA: 0:27:21\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 210000: 139.1646348153616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  31%|████████████▉                            |  ETA: 0:26:23\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 220000: 394.96541291323604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  33%|█████████████▌                           |  ETA: 0:26:47\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 230000: 259.3291542723874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  34%|██████████████                           |  ETA: 0:27:04\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 240000: 443.66312087502223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  36%|██████████████▋                          |  ETA: 0:27:27\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 250000: 103.66932930920676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  37%|███████████████▎                         |  ETA: 0:27:34\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 260000: 309.49311236055155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  38%|███████████████▊                         |  ETA: 0:26:30\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 270000: 312.21676391571634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  40%|████████████████▍                        |  ETA: 0:26:26\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 280000: 116.97875292145451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  41%|█████████████████                        |  ETA: 0:25:23\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 290000: 139.7816433486828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  43%|█████████████████▌                       |  ETA: 0:24:31\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 300000 saved to ./RL_models_slow/vtol_2D_ppo_300000.bson\n",
      "test reward at step 300000: 212.74964233898072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  44%|██████████████████▏                      |  ETA: 0:24:17\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 310000: 148.59771655344284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  45%|██████████████████▋                      |  ETA: 0:23:36\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 320000: 414.5034487804272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  47%|███████████████████▎                     |  ETA: 0:23:18\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 330000: 365.86785352362665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  48%|███████████████████▉                     |  ETA: 0:23:05\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 340000: 149.0440211349149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  50%|████████████████████▍                    |  ETA: 0:22:17\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 350000: 431.2859117862631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  51%|█████████████████████▏                   |  ETA: 0:21:55\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 360000: 352.58225066247616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  53%|█████████████████████▋                   |  ETA: 0:21:46\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 370000: 142.25047257355402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  54%|██████████████████████▎                  |  ETA: 0:20:43\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 380000: 137.44785380378934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  56%|██████████████████████▉                  |  ETA: 0:19:53\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 390000: 421.42546893654765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  57%|███████████████████████▍                 |  ETA: 0:19:30\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 400000 saved to ./RL_models_slow/vtol_2D_ppo_400000.bson\n",
      "test reward at step 400000: 107.11539262537808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  59%|████████████████████████                 |  ETA: 0:18:37\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 410000: 372.4403568497219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  60%|████████████████████████▋                |  ETA: 0:18:12\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 420000: 411.51480375277777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  61%|█████████████████████████▏               |  ETA: 0:17:50\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 430000: 277.51449055002524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  63%|█████████████████████████▊               |  ETA: 0:17:23\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 440000: 129.48712324369416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  64%|██████████████████████████▍              |  ETA: 0:16:26\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 450000: 160.40272417466724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  65%|██████████████████████████▉              |  ETA: 0:15:42\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 460000: 366.17768423351697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  67%|███████████████████████████▌             |  ETA: 0:15:05\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 470000: 210.80748213376646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  68%|████████████████████████████             |  ETA: 0:14:39\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 480000: 41.137092536415594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  70%|████████████████████████████▋            |  ETA: 0:13:48\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 490000: 154.0594965548018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  71%|█████████████████████████████▎           |  ETA: 0:12:59\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 500000 saved to ./RL_models_slow/vtol_2D_ppo_500000.bson\n",
      "test reward at step 500000: 174.16049412731022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  73%|█████████████████████████████▉           |  ETA: 0:12:14\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 510000: 443.7313847155369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  74%|██████████████████████████████▌          |  ETA: 0:11:40\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 520000: 391.52764634199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  76%|███████████████████████████████          |  ETA: 0:11:07\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 530000: 152.77056210949513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  77%|███████████████████████████████▋         |  ETA: 0:10:26\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 540000: 145.48480081635455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  78%|████████████████████████████████▏        |  ETA: 0:09:46\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 550000: 344.52996252454227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  80%|████████████████████████████████▊        |  ETA: 0:09:07\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 560000: 335.7612878061074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  81%|█████████████████████████████████▍       |  ETA: 0:08:36\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 570000: 149.71614428359436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  83%|██████████████████████████████████       |  ETA: 0:07:57\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 580000: 314.18112256188965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  84%|██████████████████████████████████▌      |  ETA: 0:07:20\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 590000: 352.3320290930795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  86%|███████████████████████████████████▏     |  ETA: 0:06:38\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 600000 saved to ./RL_models_slow/vtol_2D_ppo_600000.bson\n",
      "test reward at step 600000: 392.45175104115583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  87%|███████████████████████████████████▊     |  ETA: 0:06:00\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 610000: 354.06515795054156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  88%|████████████████████████████████████▎    |  ETA: 0:05:25\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 620000: 382.7523585458642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  90%|████████████████████████████████████▉    |  ETA: 0:04:47\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 630000: 113.76230209655942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  91%|█████████████████████████████████████▍   |  ETA: 0:04:08\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 640000: 383.2534560255042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  93%|██████████████████████████████████████   |  ETA: 0:03:24\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 650000: 424.7262923864415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  94%|██████████████████████████████████████▋  |  ETA: 0:02:44\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 660000: 133.22298857391257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  95%|███████████████████████████████████████▏ |  ETA: 0:02:08\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 670000: 151.84414278395894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  97%|███████████████████████████████████████▊ |  ETA: 0:01:23\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 680000: 370.4697668971649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  98%|████████████████████████████████████████▍|  ETA: 0:00:46\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 690000: 106.10251403068207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:46:41\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 700000 saved to ./RL_models_slow/vtol_2D_ppo_700000.bson\n",
      "test reward at step 700000: 406.3958957114468\n"
     ]
    }
   ],
   "source": [
    "ReinforcementLearning.run(\n",
    "    agent,\n",
    "    env,\n",
    "    StopAfterStep(700_000),\n",
    "    ComposedHook(\n",
    "        DoEveryNStep(saveModel, n=100_000), \n",
    "        DoEveryNStep(validate_policy, n=10_000)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49a302a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip690\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip690)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip691\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip690)\" d=\"\n",
       "M172.015 1486.45 L2352.76 1486.45 L2352.76 47.2441 L172.015 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip692\">\n",
       "    <rect x=\"172\" y=\"47\" width=\"2182\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip692)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  203.918,1486.45 203.918,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip692)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  800.238,1486.45 800.238,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip692)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1396.56,1486.45 1396.56,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip692)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1992.88,1486.45 1992.88,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip690)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  172.015,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip690)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  203.918,1486.45 203.918,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip690)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  800.238,1486.45 800.238,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip690)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1396.56,1486.45 1396.56,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip690)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1992.88,1486.45 1992.88,1467.55 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip690)\" d=\"M203.918 1517.37 Q200.307 1517.37 198.478 1520.93 Q196.673 1524.47 196.673 1531.6 Q196.673 1538.71 198.478 1542.27 Q200.307 1545.82 203.918 1545.82 Q207.552 1545.82 209.358 1542.27 Q211.186 1538.71 211.186 1531.6 Q211.186 1524.47 209.358 1520.93 Q207.552 1517.37 203.918 1517.37 M203.918 1513.66 Q209.728 1513.66 212.784 1518.27 Q215.862 1522.85 215.862 1531.6 Q215.862 1540.33 212.784 1544.94 Q209.728 1549.52 203.918 1549.52 Q198.108 1549.52 195.029 1544.94 Q191.974 1540.33 191.974 1531.6 Q191.974 1522.85 195.029 1518.27 Q198.108 1513.66 203.918 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M779.011 1544.91 L795.33 1544.91 L795.33 1548.85 L773.386 1548.85 L773.386 1544.91 Q776.048 1542.16 780.631 1537.53 Q785.238 1532.88 786.418 1531.53 Q788.664 1529.01 789.543 1527.27 Q790.446 1525.51 790.446 1523.82 Q790.446 1521.07 788.502 1519.33 Q786.58 1517.6 783.478 1517.6 Q781.279 1517.6 778.826 1518.36 Q776.395 1519.13 773.617 1520.68 L773.617 1515.95 Q776.441 1514.82 778.895 1514.24 Q781.349 1513.66 783.386 1513.66 Q788.756 1513.66 791.951 1516.35 Q795.145 1519.03 795.145 1523.52 Q795.145 1525.65 794.335 1527.57 Q793.548 1529.47 791.441 1532.07 Q790.863 1532.74 787.761 1535.95 Q784.659 1539.15 779.011 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M815.145 1517.37 Q811.534 1517.37 809.705 1520.93 Q807.9 1524.47 807.9 1531.6 Q807.9 1538.71 809.705 1542.27 Q811.534 1545.82 815.145 1545.82 Q818.779 1545.82 820.585 1542.27 Q822.413 1538.71 822.413 1531.6 Q822.413 1524.47 820.585 1520.93 Q818.779 1517.37 815.145 1517.37 M815.145 1513.66 Q820.955 1513.66 824.011 1518.27 Q827.089 1522.85 827.089 1531.6 Q827.089 1540.33 824.011 1544.94 Q820.955 1549.52 815.145 1549.52 Q809.335 1549.52 806.256 1544.94 Q803.201 1540.33 803.201 1531.6 Q803.201 1522.85 806.256 1518.27 Q809.335 1513.66 815.145 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M1384.73 1518.36 L1372.92 1536.81 L1384.73 1536.81 L1384.73 1518.36 M1383.5 1514.29 L1389.38 1514.29 L1389.38 1536.81 L1394.31 1536.81 L1394.31 1540.7 L1389.38 1540.7 L1389.38 1548.85 L1384.73 1548.85 L1384.73 1540.7 L1369.13 1540.7 L1369.13 1536.19 L1383.5 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M1412.04 1517.37 Q1408.43 1517.37 1406.6 1520.93 Q1404.8 1524.47 1404.8 1531.6 Q1404.8 1538.71 1406.6 1542.27 Q1408.43 1545.82 1412.04 1545.82 Q1415.68 1545.82 1417.48 1542.27 Q1419.31 1538.71 1419.31 1531.6 Q1419.31 1524.47 1417.48 1520.93 Q1415.68 1517.37 1412.04 1517.37 M1412.04 1513.66 Q1417.85 1513.66 1420.91 1518.27 Q1423.99 1522.85 1423.99 1531.6 Q1423.99 1540.33 1420.91 1544.94 Q1417.85 1549.52 1412.04 1549.52 Q1406.23 1549.52 1403.15 1544.94 Q1400.1 1540.33 1400.1 1531.6 Q1400.1 1522.85 1403.15 1518.27 Q1406.23 1513.66 1412.04 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M1978.28 1529.7 Q1975.13 1529.7 1973.28 1531.86 Q1971.45 1534.01 1971.45 1537.76 Q1971.45 1541.49 1973.28 1543.66 Q1975.13 1545.82 1978.28 1545.82 Q1981.43 1545.82 1983.26 1543.66 Q1985.11 1541.49 1985.11 1537.76 Q1985.11 1534.01 1983.26 1531.86 Q1981.43 1529.7 1978.28 1529.7 M1987.56 1515.05 L1987.56 1519.31 Q1985.81 1518.48 1984 1518.04 Q1982.22 1517.6 1980.46 1517.6 Q1975.83 1517.6 1973.37 1520.72 Q1970.94 1523.85 1970.6 1530.17 Q1971.96 1528.15 1974.02 1527.09 Q1976.08 1526 1978.56 1526 Q1983.77 1526 1986.78 1529.17 Q1989.81 1532.32 1989.81 1537.76 Q1989.81 1543.08 1986.66 1546.3 Q1983.51 1549.52 1978.28 1549.52 Q1972.29 1549.52 1969.12 1544.94 Q1965.94 1540.33 1965.94 1531.6 Q1965.94 1523.41 1969.83 1518.55 Q1973.72 1513.66 1980.27 1513.66 Q1982.03 1513.66 1983.81 1514.01 Q1985.62 1514.36 1987.56 1515.05 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M2007.87 1517.37 Q2004.25 1517.37 2002.43 1520.93 Q2000.62 1524.47 2000.62 1531.6 Q2000.62 1538.71 2002.43 1542.27 Q2004.25 1545.82 2007.87 1545.82 Q2011.5 1545.82 2013.31 1542.27 Q2015.13 1538.71 2015.13 1531.6 Q2015.13 1524.47 2013.31 1520.93 Q2011.5 1517.37 2007.87 1517.37 M2007.87 1513.66 Q2013.68 1513.66 2016.73 1518.27 Q2019.81 1522.85 2019.81 1531.6 Q2019.81 1540.33 2016.73 1544.94 Q2013.68 1549.52 2007.87 1549.52 Q2002.06 1549.52 1998.98 1544.94 Q1995.92 1540.33 1995.92 1531.6 Q1995.92 1522.85 1998.98 1518.27 Q2002.06 1513.66 2007.87 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip692)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  172.015,1250.08 2352.76,1250.08 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip692)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  172.015,917.729 2352.76,917.729 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip692)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  172.015,585.375 2352.76,585.375 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip692)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  172.015,253.022 2352.76,253.022 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip690)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  172.015,1486.45 172.015,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip690)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  172.015,1250.08 190.912,1250.08 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip690)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  172.015,917.729 190.912,917.729 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip690)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  172.015,585.375 190.912,585.375 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip690)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  172.015,253.022 190.912,253.022 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip690)\" d=\"M54.5569 1263.43 L62.1958 1263.43 L62.1958 1237.06 L53.8856 1238.73 L53.8856 1234.47 L62.1495 1232.8 L66.8254 1232.8 L66.8254 1263.43 L74.4642 1263.43 L74.4642 1267.36 L54.5569 1267.36 L54.5569 1263.43 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M93.9086 1235.88 Q90.2975 1235.88 88.4688 1239.45 Q86.6632 1242.99 86.6632 1250.12 Q86.6632 1257.22 88.4688 1260.79 Q90.2975 1264.33 93.9086 1264.33 Q97.5428 1264.33 99.3483 1260.79 Q101.177 1257.22 101.177 1250.12 Q101.177 1242.99 99.3483 1239.45 Q97.5428 1235.88 93.9086 1235.88 M93.9086 1232.18 Q99.7187 1232.18 102.774 1236.78 Q105.853 1241.37 105.853 1250.12 Q105.853 1258.84 102.774 1263.45 Q99.7187 1268.03 93.9086 1268.03 Q88.0984 1268.03 85.0197 1263.45 Q81.9642 1258.84 81.9642 1250.12 Q81.9642 1241.37 85.0197 1236.78 Q88.0984 1232.18 93.9086 1232.18 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M124.07 1235.88 Q120.459 1235.88 118.631 1239.45 Q116.825 1242.99 116.825 1250.12 Q116.825 1257.22 118.631 1260.79 Q120.459 1264.33 124.07 1264.33 Q127.705 1264.33 129.51 1260.79 Q131.339 1257.22 131.339 1250.12 Q131.339 1242.99 129.51 1239.45 Q127.705 1235.88 124.07 1235.88 M124.07 1232.18 Q129.881 1232.18 132.936 1236.78 Q136.015 1241.37 136.015 1250.12 Q136.015 1258.84 132.936 1263.45 Q129.881 1268.03 124.07 1268.03 Q118.26 1268.03 115.182 1263.45 Q112.126 1258.84 112.126 1250.12 Q112.126 1241.37 115.182 1236.78 Q118.26 1232.18 124.07 1232.18 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M57.7745 931.074 L74.0939 931.074 L74.0939 935.009 L52.1495 935.009 L52.1495 931.074 Q54.8115 928.319 59.3949 923.69 Q64.0013 919.037 65.1819 917.694 Q67.4272 915.171 68.3068 913.435 Q69.2096 911.676 69.2096 909.986 Q69.2096 907.231 67.2652 905.495 Q65.3439 903.759 62.2421 903.759 Q60.043 903.759 57.5893 904.523 Q55.1588 905.287 52.381 906.838 L52.381 902.116 Q55.2051 900.981 57.6588 900.403 Q60.1124 899.824 62.1495 899.824 Q67.5198 899.824 70.7142 902.509 Q73.9087 905.194 73.9087 909.685 Q73.9087 911.815 73.0985 913.736 Q72.3115 915.634 70.205 918.227 Q69.6263 918.898 66.5245 922.115 Q63.4226 925.31 57.7745 931.074 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M93.9086 903.528 Q90.2975 903.528 88.4688 907.092 Q86.6632 910.634 86.6632 917.764 Q86.6632 924.87 88.4688 928.435 Q90.2975 931.977 93.9086 931.977 Q97.5428 931.977 99.3483 928.435 Q101.177 924.87 101.177 917.764 Q101.177 910.634 99.3483 907.092 Q97.5428 903.528 93.9086 903.528 M93.9086 899.824 Q99.7187 899.824 102.774 904.43 Q105.853 909.014 105.853 917.764 Q105.853 926.49 102.774 931.097 Q99.7187 935.68 93.9086 935.68 Q88.0984 935.68 85.0197 931.097 Q81.9642 926.49 81.9642 917.764 Q81.9642 909.014 85.0197 904.43 Q88.0984 899.824 93.9086 899.824 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M124.07 903.528 Q120.459 903.528 118.631 907.092 Q116.825 910.634 116.825 917.764 Q116.825 924.87 118.631 928.435 Q120.459 931.977 124.07 931.977 Q127.705 931.977 129.51 928.435 Q131.339 924.87 131.339 917.764 Q131.339 910.634 129.51 907.092 Q127.705 903.528 124.07 903.528 M124.07 899.824 Q129.881 899.824 132.936 904.43 Q136.015 909.014 136.015 917.764 Q136.015 926.49 132.936 931.097 Q129.881 935.68 124.07 935.68 Q118.26 935.68 115.182 931.097 Q112.126 926.49 112.126 917.764 Q112.126 909.014 115.182 904.43 Q118.26 899.824 124.07 899.824 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M67.9133 584.021 Q71.2698 584.739 73.1448 587.007 Q75.0429 589.276 75.0429 592.609 Q75.0429 597.725 71.5244 600.526 Q68.0059 603.327 61.5245 603.327 Q59.3486 603.327 57.0338 602.887 Q54.7421 602.47 52.2884 601.614 L52.2884 597.1 Q54.2328 598.234 56.5477 598.813 Q58.8625 599.391 61.3856 599.391 Q65.7837 599.391 68.0754 597.655 Q70.3902 595.919 70.3902 592.609 Q70.3902 589.553 68.2374 587.84 Q66.1078 586.104 62.2884 586.104 L58.2606 586.104 L58.2606 582.262 L62.4735 582.262 Q65.9226 582.262 67.7513 580.896 Q69.58 579.507 69.58 576.915 Q69.58 574.253 67.6819 572.841 Q65.8069 571.405 62.2884 571.405 Q60.3671 571.405 58.168 571.822 Q55.969 572.239 53.3301 573.118 L53.3301 568.952 Q55.9921 568.211 58.3069 567.841 Q60.6449 567.47 62.705 567.47 Q68.0291 567.47 71.1309 569.901 Q74.2327 572.308 74.2327 576.429 Q74.2327 579.299 72.5892 581.29 Q70.9457 583.257 67.9133 584.021 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M93.9086 571.174 Q90.2975 571.174 88.4688 574.739 Q86.6632 578.28 86.6632 585.41 Q86.6632 592.516 88.4688 596.081 Q90.2975 599.623 93.9086 599.623 Q97.5428 599.623 99.3483 596.081 Q101.177 592.516 101.177 585.41 Q101.177 578.28 99.3483 574.739 Q97.5428 571.174 93.9086 571.174 M93.9086 567.47 Q99.7187 567.47 102.774 572.077 Q105.853 576.66 105.853 585.41 Q105.853 594.137 102.774 598.743 Q99.7187 603.327 93.9086 603.327 Q88.0984 603.327 85.0197 598.743 Q81.9642 594.137 81.9642 585.41 Q81.9642 576.66 85.0197 572.077 Q88.0984 567.47 93.9086 567.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M124.07 571.174 Q120.459 571.174 118.631 574.739 Q116.825 578.28 116.825 585.41 Q116.825 592.516 118.631 596.081 Q120.459 599.623 124.07 599.623 Q127.705 599.623 129.51 596.081 Q131.339 592.516 131.339 585.41 Q131.339 578.28 129.51 574.739 Q127.705 571.174 124.07 571.174 M124.07 567.47 Q129.881 567.47 132.936 572.077 Q136.015 576.66 136.015 585.41 Q136.015 594.137 132.936 598.743 Q129.881 603.327 124.07 603.327 Q118.26 603.327 115.182 598.743 Q112.126 594.137 112.126 585.41 Q112.126 576.66 115.182 572.077 Q118.26 567.47 124.07 567.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M66.5939 239.816 L54.7884 258.265 L66.5939 258.265 L66.5939 239.816 M65.367 235.742 L71.2466 235.742 L71.2466 258.265 L76.1772 258.265 L76.1772 262.153 L71.2466 262.153 L71.2466 270.302 L66.5939 270.302 L66.5939 262.153 L50.9921 262.153 L50.9921 257.64 L65.367 235.742 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M93.9086 238.82 Q90.2975 238.82 88.4688 242.385 Q86.6632 245.927 86.6632 253.056 Q86.6632 260.163 88.4688 263.727 Q90.2975 267.269 93.9086 267.269 Q97.5428 267.269 99.3483 263.727 Q101.177 260.163 101.177 253.056 Q101.177 245.927 99.3483 242.385 Q97.5428 238.82 93.9086 238.82 M93.9086 235.117 Q99.7187 235.117 102.774 239.723 Q105.853 244.306 105.853 253.056 Q105.853 261.783 102.774 266.39 Q99.7187 270.973 93.9086 270.973 Q88.0984 270.973 85.0197 266.39 Q81.9642 261.783 81.9642 253.056 Q81.9642 244.306 85.0197 239.723 Q88.0984 235.117 93.9086 235.117 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M124.07 238.82 Q120.459 238.82 118.631 242.385 Q116.825 245.927 116.825 253.056 Q116.825 260.163 118.631 263.727 Q120.459 267.269 124.07 267.269 Q127.705 267.269 129.51 263.727 Q131.339 260.163 131.339 253.056 Q131.339 245.927 129.51 242.385 Q127.705 238.82 124.07 238.82 M124.07 235.117 Q129.881 235.117 132.936 239.723 Q136.015 244.306 136.015 253.056 Q136.015 261.783 132.936 266.39 Q129.881 270.973 124.07 270.973 Q118.26 270.973 115.182 266.39 Q112.126 261.783 112.126 253.056 Q112.126 244.306 115.182 239.723 Q118.26 235.117 124.07 235.117 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip692)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  233.734,1102.68 263.55,1032.95 293.366,646.421 323.182,87.9763 352.998,1066.32 382.814,1000.34 412.63,1202.86 442.446,1005.12 472.262,1050.15 502.078,356.888 \n",
       "  531.894,987.575 561.71,1049.14 591.526,1144.65 621.342,1222.9 651.158,989.661 680.974,949.217 710.79,171.294 740.606,1107.92 770.422,1081.55 800.238,711.633 \n",
       "  830.054,1119.92 859.87,269.754 889.686,720.546 919.502,107.906 949.318,1237.89 979.134,553.825 1008.95,544.772 1038.77,1193.65 1068.58,1117.87 1098.4,875.355 \n",
       "  1128.21,1088.57 1158.03,204.819 1187.85,366.461 1217.66,1087.08 1247.48,149.042 1277.29,410.616 1307.11,1109.66 1336.93,1125.62 1366.74,181.813 1396.56,1226.43 \n",
       "  1426.37,344.617 1456.19,214.752 1486.01,660.107 1515.82,1152.08 1545.64,1049.33 1575.45,365.431 1605.27,881.81 1635.09,1445.72 1664.9,1070.41 1694.72,1003.61 \n",
       "  1724.53,107.679 1754.35,281.18 1784.17,1074.7 1813.98,1098.91 1843.8,437.378 1873.61,466.521 1903.43,1084.85 1933.25,538.244 1963.06,411.448 1992.88,278.108 \n",
       "  2022.69,405.688 2052.51,310.345 2082.32,1204.34 2112.14,308.679 2141.96,170.843 2171.77,1139.66 2201.59,1077.78 2231.4,351.166 2261.22,1229.8 2291.04,231.765 \n",
       "  \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip690)\" d=\"\n",
       "M1985.33 198.898 L2280.06 198.898 L2280.06 95.2176 L1985.33 95.2176  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip690)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1985.33,198.898 2280.06,198.898 2280.06,95.2176 1985.33,95.2176 1985.33,198.898 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip690)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2009.56,147.058 2154.94,147.058 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip690)\" d=\"M2193.02 166.745 Q2191.21 171.375 2189.5 172.787 Q2187.78 174.199 2184.91 174.199 L2181.51 174.199 L2181.51 170.634 L2184.01 170.634 Q2185.77 170.634 2186.74 169.8 Q2187.71 168.967 2188.9 165.865 L2189.66 163.921 L2179.17 138.412 L2183.69 138.412 L2191.79 158.689 L2199.89 138.412 L2204.4 138.412 L2193.02 166.745 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip690)\" d=\"M2211.7 160.402 L2219.34 160.402 L2219.34 134.037 L2211.03 135.703 L2211.03 131.444 L2219.29 129.778 L2223.96 129.778 L2223.96 160.402 L2231.6 160.402 L2231.6 164.338 L2211.7 164.338 L2211.7 160.402 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(episode_test_reward_hook.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb0d4ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "close_visualization(); # closes the MeshCat visualization"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
