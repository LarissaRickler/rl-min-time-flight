{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f85185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "if isfile(\"Project.toml\") && isfile(\"Manifest.toml\")\n",
    "    Pkg.activate(\".\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0649b76a",
   "metadata": {},
   "source": [
    "# Init Bionic VTOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96751412",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../Flyonic.jl\");\n",
    "using .Flyonic;\n",
    "\n",
    "using Rotations; # used for initial position\n",
    "\n",
    "using ReinforcementLearning;\n",
    "using StableRNGs;\n",
    "using Flux;\n",
    "using Flux.Losses;\n",
    "using Random;\n",
    "using IntervalSets;\n",
    "using LinearAlgebra;\n",
    "using Distributions;\n",
    "\n",
    "using Plots;\n",
    "using Statistics;\n",
    "\n",
    "using TensorBoardLogger\n",
    "using Logging\n",
    "\n",
    "using BSON: @save, @load # save mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "917bb919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: set as desired\n",
    "R_TOL = 0.5;\n",
    "N_WAYPOINTS = 4; # including startpoint, >= 2\n",
    "SLOW_MODE = true;\n",
    "TRAINING = true;\n",
    "EVALUATION = true;\n",
    "VIDEO = false;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7e4ee64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMeshCat server started. You can open the visualizer by visiting the following URL in your browser:\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39mhttp://192.168.0.106:8700\n"
     ]
    }
   ],
   "source": [
    "create_remote_visualization();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e19cce80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TBLogger:\n",
       "\t- Log level     : Info\n",
       "\t- Current step  : 0\n",
       "\t- Output        : /home/larissa/Documents/Projects/ADLR/ADLR_project/src/RL_SAC_quadcopter_3D/tensorboard_SAC\n",
       "\t- open files    : 1\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TensorBoard\n",
    "logger = TBLogger(\"tensorboard_SAC\", tb_increment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d9557df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indicates how many threads Julia was started with. This is important for the multi-threaded environment\n",
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10d945ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Δt = 0.025;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5411db62",
   "metadata": {},
   "source": [
    "# Create Reinforcement Learning Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96af6ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct VtolEnv{A,T,ACT,R<:AbstractRNG} <: AbstractEnv # Parametric Constructor for a subtype of AbstractEnv\n",
    "    action_space::A # All possible actions the agent can take\n",
    "    observation_space::Space{Vector{ClosedInterval{T}}} # All possible states that the agent can observe.\n",
    "    state::Vector{T} # Current state\n",
    "    action::ACT # next action the agent wants to apply in the environment.\n",
    "    done::Bool # shows whether a terminal condition has been reached.\n",
    "    t::T # time\n",
    "    rng::R # random number generator\n",
    "\n",
    "    name::String # for different naming of multible environoments\n",
    "    visualization::Bool # activate visualisation (Faster computation without visualisation)\n",
    "    realtime::Bool # visualization in \"real-world\" time (only for watching or filming).\n",
    "    \n",
    "    # Overall state of the environment. This does not correspond to the observation space of the agent but contains all states that describe the environment.\n",
    "    x_W::Vector{T} # Position in World frame\n",
    "    v_B::Vector{T} # Velocity in Body frame\n",
    "    R_W::Matrix{T} # Rotation (matrix) in World frame\n",
    "    ω_B::Vector{T} # Rotation velocity in Body frame\n",
    "    wind_W::Vector{T} # Externel linear velocity acting on the drone\n",
    "    Δt::T # Time step for physics simulation in seconds\n",
    "\n",
    "    ###NEW###\n",
    "    # Current Bonus / Target\n",
    "    num_waypoints::Int # includig start point\n",
    "    waypoints::Vector{Vector{T}}\n",
    "    reached_goal::BitVector\n",
    "    \n",
    "    norm_way::T\n",
    "    progress::T\n",
    "    progress_prev::T\n",
    "    current_point::Int\n",
    "    reached_goal_in_step::Bool\n",
    "    \n",
    "    r_tol::T\n",
    "    projected_position::Vector{T}\n",
    "\n",
    "    slow_mode::Bool\n",
    "    ######\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14a6873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a keyword-based constructor for the type declared in the mutable struct typedef. \n",
    "# It could also be done with the macro Base.@kwdef.\n",
    "function VtolEnv(;\n",
    "    rng = Random.GLOBAL_RNG, # Random number generation\n",
    "    name = \"Crazyflie\",\n",
    "    visualization = false,\n",
    "    realtime = false,\n",
    "    kwargs... # let the function take an arbitrary number of keyword arguments\n",
    ")\n",
    "    \n",
    "    T = Float64; # explicit type which is used e.g. in state. Cannot be altered due to the poor matrix defininon.\n",
    "    \n",
    "    # final PWM Values for Crazyflie. The interval definition has no effect in the current implementation.\n",
    "    action_space = Space(\n",
    "        ClosedInterval{T}[\n",
    "            0..1#0..65535, # motor 1\n",
    "            0..1#0..65535, # motor 2\n",
    "            0..1#0..65535, # motor 3\n",
    "            0..1#0..65535, # motor 4\n",
    "            ], \n",
    "    )\n",
    "\n",
    "    \n",
    "    state_space = Space( # Three continuous values in state space.\n",
    "        ClosedInterval{T}[#todo\n",
    "            typemin(T)..typemax(T), # 1 x\n",
    "            typemin(T)..typemax(T), # 2 y\n",
    "            typemin(T)..typemax(T), # 3 z\n",
    "\n",
    "            typemin(T)..typemax(T), # 4  World Vector UP x\n",
    "            typemin(T)..typemax(T), # 5  World Vector UP y\n",
    "            typemin(T)..typemax(T), # 6  World Vector UP z\n",
    "\n",
    "            typemin(T)..typemax(T), # 7  World Vector FRONT x\n",
    "            typemin(T)..typemax(T), # 8  World Vector FRONT y\n",
    "            typemin(T)..typemax(T), # 9  World Vector FRONT z\n",
    "            \n",
    "            typemin(T)..typemax(T), # 10 Body velocity along x\n",
    "            typemin(T)..typemax(T), # 11 Body velocity along y\n",
    "            typemin(T)..typemax(T), # 12 Body velocity along z\n",
    "            \n",
    "            typemin(T)..typemax(T), # 13 Body rotational velocity around x\n",
    "            typemin(T)..typemax(T), # 14 Body rotational velocity around y\n",
    "            typemin(T)..typemax(T), # 15 Body rotational velocity around z\n",
    "            \n",
    "            ###NEW###\n",
    "            typemin(T)..typemax(T), # 16 position error along x (next gate - current position)\n",
    "            typemin(T)..typemax(T), # 17 position error along y (next gate - current position)\n",
    "            typemin(T)..typemax(T), # 18 position error along z (next gate - current position)\n",
    "            \n",
    "            typemin(T)..typemax(T), # 19 way to next next gate x (next next gate - next gate)\n",
    "            typemin(T)..typemax(T), # 20 way to next next gate y (next next gate - next gate)\n",
    "            typemin(T)..typemax(T), # 21 way to next next gate z (next next gate - next gate)\n",
    "            ######\n",
    "            ], \n",
    "    )\n",
    "\n",
    "    ###NEW###\n",
    "    num_waypoints = N_WAYPOINTS # number of waypoints, includig start point\n",
    "    waypoints = generate_trajectory(num_waypoints + 1) # trajectory with num_waypoints waypoints (+ start point), (with dummy points) \n",
    "    reached_goal = falses(num_waypoints)\n",
    "    \n",
    "    norm_way = 0.0 \n",
    "    for i in 1:(num_waypoints - 1)\n",
    "        norm_way += norm(waypoints[i] - waypoints[i + 1])\n",
    "    end\n",
    "    ######\n",
    "    \n",
    "    if visualization\n",
    "        create_Crazyflie(name, actuators = true);\n",
    "        visualize_waypoints(waypoints[1:num_waypoints], 0.05)\n",
    "\n",
    "        set_Crazyflie_actuators(name, [0.0; 0.0; 0.0; 0.0]);\n",
    "        set_transform(name, [0.0; 0.0; 0.0] ,one(QuatRotation));\n",
    "        set_arrow(string(name, \"vel\"), color_vec=[0.0; 1.0; 0.0; 1.0]);\n",
    "        transform_arrow(string(name, \"vel\"), [0.0; 0.0; 0.0], [0.0; 0.0; 1.0], max_head_radius=0.05)\n",
    "#         set_arrow(string(name, \"_vel_current\"), color_vec=[1.0; 0.0; 0.0; 1.0]);\n",
    "#         transform_arrow(string(name, \"_vel_current\"), [0.0; 0.0; 0.0], [0.0; 0.0; 1.0], max_head_radius=0.02)                  \n",
    "    end\n",
    "    \n",
    "\n",
    "\n",
    "    environment = VtolEnv(\n",
    "        action_space,\n",
    "        state_space,\n",
    "        zeros(T, length(state_space)), # current state, needs to be extended.\n",
    "        [0.25; 0.25; 0.25; 0.25],#rand(action_space), #todo test with random\n",
    "        false, # episode done ?\n",
    "        0.0, # time\n",
    "        rng, # random number generator  \n",
    "\n",
    "        name,\n",
    "        visualization,\n",
    "        realtime,\n",
    "\n",
    "        zeros(T, 3), # x_W\n",
    "        zeros(T, 3), # v_B\n",
    "        Matrix(one(QuatRotation)), # Float64... so T needs to be Float64\n",
    "        zeros(T, 3), # ω_B\n",
    "        zeros(T, 3), # wind_W\n",
    "        Δt, # Δt\n",
    "\n",
    "        num_waypoints, # includig start point\n",
    "        waypoints, \n",
    "        reached_goal,\n",
    "\n",
    "        norm_way, # norm_way\n",
    "        0.0, # progress\n",
    "        0.0, # progress_prev\n",
    "        2, # current point\n",
    "        false, # reached_goal_in_step\n",
    "        \n",
    "        R_TOL, # r_tol\n",
    "        zeros(T, 3), # projected_position\n",
    "\n",
    "        SLOW_MODE # slow_mode\n",
    "\n",
    "        )\n",
    "    \n",
    "    \n",
    "    RLBase.reset!(environment)\n",
    "    \n",
    "    return environment\n",
    "    \n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec660d5e",
   "metadata": {},
   "source": [
    "Just for explanation:\n",
    "\n",
    "1. A mutable Struct is created. A struct is a constructor and a constructor is a function that creates new objects.\n",
    "2. A outer keyword-based constructor method is added for the type declared in the mutable struct typedef before.\n",
    "\n",
    "So now we have a function with two methods. Julia will decide which method to call by multiple dispatch."
   ]
  },
  {
   "cell_type": "raw",
   "id": "23dd4047",
   "metadata": {},
   "source": [
    "methods(VtolEnv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806413d1",
   "metadata": {},
   "source": [
    "# Define the RL interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f822029",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(env::VtolEnv, seed) = Random.seed!(env.rng, seed)\n",
    "RLBase.action_space(env::VtolEnv) = env.action_space\n",
    "RLBase.state_space(env::VtolEnv) = env.observation_space\n",
    "RLBase.is_terminated(env::VtolEnv) = env.done\n",
    "RLBase.state(env::VtolEnv) = env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aad040d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "function scale_for_slowmode(slow_mode::Bool, v_min::T, v_max::T, d_max::T, x_W::Vector{T}, projected_position::Vector{T}, v_B::Vector{T}) where T\n",
    "    \n",
    "    if slow_mode == false\n",
    "        return 1\n",
    "    else\n",
    "        if norm(v_B) > v_max\n",
    "            s_vmax = 10^(v_max - norm(v_B))\n",
    "        else\n",
    "            s_vmax = 1\n",
    "        end\n",
    "\n",
    "        if norm(v_B) < v_min\n",
    "            s_vmin = 10^(norm(v_B) - v_min)\n",
    "        else\n",
    "            s_vmin = 1\n",
    "        end\n",
    "\n",
    "        if norm(x_W - projected_position) > d_max\n",
    "            s_gd = exp(-norm(x_W - projected_position) + d_max)\n",
    "        else\n",
    "            s_gd = 1\n",
    "        end\n",
    "        s = s_vmax * s_vmin * s_gd\n",
    "    end\n",
    "    return s\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f7fb89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "function computeReward(env::VtolEnv{A,T}) where {A,T}\n",
    "    \n",
    "    \n",
    "    if env.slow_mode\n",
    "        v_min = 1.0 # min velocity\n",
    "        v_max = 3.0 # max velocity\n",
    "        d_max = 0.5 \n",
    "    else\n",
    "        v_min = 4.0 # min velocity\n",
    "        v_max = 50.0 # max velocity\n",
    "        d_max = 1.0 \n",
    "    end\n",
    "\n",
    "\n",
    "    s = scale_for_slowmode(true, v_min, v_max, d_max, env.x_W, env.projected_position, env.v_B)\n",
    "    \n",
    "    # TODO: test norming\n",
    "    k_p = 5.0 * s #/ env.norm_way # factor for progress (between current position and last position) reward \n",
    "    r_p = (env.progress - env.progress_prev); # reward for progress (between current position and last position)\n",
    "\n",
    "    k_s = s * (2 * v_max * env.Δt) / env.norm_way # factor for reached distance (overall) reward\n",
    "    r_s = env.progress # reward for reached distance (overall)\n",
    "    \n",
    "    k_wp = 50.0 # factor for reached gate reward\n",
    "    r_wp = 0.0 # reward for reached gate\n",
    "    if env.reached_goal_in_step\n",
    "        r_wp = exp(-norm(env.x_W - env.waypoints[env.current_point - 1])/env.r_tol)\n",
    "    end \n",
    "\n",
    "    # factor for too high body rate penalty\n",
    "    if env.slow_mode\n",
    "        k_ω = 0.01\n",
    "    else\n",
    "        k_ω = 0.001\n",
    "    end\n",
    "    #norm_ω = norm(env.ω_B[3]) # penalty for body rate\n",
    "    norm_ω = norm(env.ω_B) # penalty for body rate\n",
    "\n",
    "    if env.x_W[3] < 0\n",
    "        fall = 1\n",
    "    else\n",
    "        fall = 0\n",
    "    end\n",
    "    \n",
    "    if !env.slow_mode\n",
    "        k_s /= env.norm_way\n",
    "        k_p /= env.norm_way\n",
    "    end\n",
    "    \n",
    "    return k_p * r_p + k_s * r_s + k_wp * r_wp - k_ω * norm_ω - fall\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "RLBase.reward(env::VtolEnv{A,T}) where {A,T} = computeReward(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae45ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "function RLBase.reset!(env::VtolEnv{A,T}) where {A,T}\n",
    "    \n",
    "    env.x_W = [0.0; 0.0; 0.0];\n",
    "    env.v_B = [0.0; 0.0; 0.0];\n",
    "    env.R_W = Matrix(one(QuatRotation)); # Identity matrix (no rotation)\n",
    "    env.ω_B = [0.0; 0.0; 0.0];\n",
    "    env.wind_W = [0.0; 0.0; 0.0];\n",
    "\n",
    "\n",
    "    env.waypoints = generate_trajectory(env.num_waypoints + 1); # Dummy points\n",
    "    env.reached_goal = falses(env.num_waypoints);\n",
    "    env.reached_goal[1] = true; # set first point to reached (start point)\n",
    "    \n",
    "    env.current_point = 2;\n",
    "    env.reached_goal_in_step = false;\n",
    "    #env.r_tol = 0.3;\n",
    "    \n",
    "    if env.visualization\n",
    "        visualize_waypoints(env.waypoints[1:env.num_waypoints], 0.05); \n",
    "    end\n",
    "    \n",
    "    norm_way = 0.0 \n",
    "    for i in 1:(env.num_waypoints - 1)\n",
    "        norm_way += norm(env.waypoints[i] - env.waypoints[i + 1])\n",
    "    end\n",
    "    \n",
    "    env.norm_way = norm_way\n",
    "    env.progress = 0.0;\n",
    "    env.progress_prev = 0.0;\n",
    "        \n",
    " #todo\n",
    "    env.state = [env.x_W[1]; # 1 position along x\n",
    "                 env.x_W[2]; # 2 position along y\n",
    "                 env.x_W[3]; # 3 position along z\n",
    "#todo right R_W?\n",
    "                 env.R_W[1,1];\n",
    "                 env.R_W[2,1];\n",
    "                 env.R_W[3,1];\n",
    "        \n",
    "                 env.R_W[1,3]; # 4  World Vector UP x\n",
    "                 env.R_W[2,3]; # 5  World Vector UP y\n",
    "                 env.R_W[3,3]; # 6  World Vector UP z\n",
    "\n",
    "#                  env.R_W[1,1]; # 7  World Vector FRONT x\n",
    "#                  env.R_W[2,1]; # 8  World Vector FRONT y\n",
    "#                  env.R_W[3,1]; # 9  World Vector FRONT z\n",
    "\n",
    "                 env.v_B[1]; #  10 Body velocity along x\n",
    "                 env.v_B[2]; #  11 Body velocity along y\n",
    "                 env.v_B[3]; #  12 Body velocity along z\n",
    "\n",
    "                 env.ω_B[1]; #  13  Body rotational velocity around x\n",
    "                 env.ω_B[2]; #  14  Body rotational velocity around y\n",
    "                 env.ω_B[3]; #  15  Body rotational velocity around z\n",
    "\n",
    "                 env.waypoints[2][1] - env.x_W[1]; # 16 position error to next gate along x\n",
    "                 env.waypoints[2][2] - env.x_W[2]; # 17 position error to next gate along z\n",
    "                 env.waypoints[2][3] - env.x_W[3]; # 18 position error to next gate along z\n",
    "                 \n",
    "                 env.waypoints[3][1] - env.waypoints[2][1]; # 19 way to next next gate x \n",
    "                 env.waypoints[3][2] - env.waypoints[2][2]; # 20 way to next next gate y\n",
    "                 env.waypoints[3][3] - env.waypoints[2][3]]  # 21 way to next next gate z \n",
    "    \n",
    "\n",
    "    env.t = 0.0; # time 0s\n",
    "    env.action = [0.25; 0.25; 0.25; 0.25] # normalized # todo try with 0.0\n",
    "    #env.last_action = [0.255; 0.255; 0.255; 0.255] # normalized\n",
    "    #env.current_action = [0.255; 0.255; 0.255; 0.255] # normalized\n",
    "\n",
    "    env.done = false # reset termination\n",
    "\n",
    "    env.projected_position = [0; 0; 0]\n",
    "    \n",
    "    if env.visualization\n",
    "        # Visualize initial state\n",
    "        set_transform(env.name, env.x_W,QuatRotation(env.R_W));\n",
    "        set_Crazyflie_actuators(env.name, [0.0; 0.0; 0.0; 0.0]);\n",
    "        #transform_arrow(string(env.name, \"_vel\"), env.x_W, env.v_W_target, max_head_radius=0.05) \n",
    "        transform_arrow(string(env.name, \"vel\"), env.x_W, [0.0; 0.0; 0.0], max_head_radius=0.05) \n",
    "    end\n",
    "    \n",
    "    nothing # return nothing\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cf1a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines a methods for a callable object.\n",
    "# So when a VtolEnv object is created, it has this method that can be called\n",
    "function (env::VtolEnv)(a)\n",
    "\n",
    "\n",
    "    # call the step on the environoment with the next action \n",
    "    _step!(env, a)\n",
    "        \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26a5a0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VtolEnv();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fa04886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "# 3 methods for callable object:<ul><li> (env::<b>VtolEnv</b>)(a) in Main at In[14]:3</li> <li> (env::<b>AbstractEnv</b>)(action) in ReinforcementLearningBase</li> <li> (env::<b>AbstractEnv</b>)(action, player) in ReinforcementLearningBase</li> </ul>"
      ],
      "text/plain": [
       "# 3 methods for callable object:\n",
       "[1] (env::VtolEnv)(a) in Main at In[14]:3\n",
       "[2] (env::AbstractEnv)(action) in ReinforcementLearningBase\n",
       "[3] (env::AbstractEnv)(action, player) in ReinforcementLearningBase"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methods(env) # Just to explain which methods the object has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4d67302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scale_actions (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function scale_actions(next_action)\n",
    "    return next_action*500_000.0#22000.0 #todo\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e7d4727",
   "metadata": {},
   "outputs": [],
   "source": [
    "function _step!(env::VtolEnv, next_action)\n",
    "    \n",
    "\n",
    "    #env.last_action = copy(env.current_action)\n",
    "    #env.current_action[1] = next_action[1]\n",
    "    #env.current_action[2] = next_action[2]\n",
    "    #env.current_action[3] = next_action[3]\n",
    "    #env.current_action[4] = next_action[4]\n",
    "    \n",
    "    \n",
    "    \n",
    "    scaled_actions = scale_actions.(next_action) # between 0 and 1 for neual network\n",
    "    # caluclate wind impact\n",
    "    v_in_wind_B = vtol_add_wind(env.v_B, env.R_W, env.wind_W)\n",
    "    # caluclate aerodynamic forces\n",
    "    torque_B, force_B = crazyflie_model(scaled_actions);\n",
    "    # integrate rigid body dynamics for Δt\n",
    "    env.x_W, env.v_B, env.R_W, env.ω_B, env.t = rigid_body_simple(torque_B, force_B, env.x_W, env.v_B, env.R_W, env.ω_B, env.t, env.Δt, crazyflie_param)\n",
    "\n",
    "    \n",
    "    env.reached_goal_in_step = false;\n",
    "    if norm(env.x_W - env.waypoints[env.current_point]) < env.r_tol\n",
    "        env.reached_goal_in_step = true;\n",
    "        env.reached_goal[env.current_point] = true;\n",
    "        env.current_point += 1;\n",
    "    end\n",
    "        \n",
    "            \n",
    "    # calculate progress on trajectory\n",
    "    env.progress_prev = env.progress\n",
    "    \n",
    "    current_progress = 0.0\n",
    "    line_segment, env.projected_position = calculate_progress(env.waypoints, env.x_W)\n",
    "    \n",
    "    #env.current_point = line_segment + 1\n",
    "\n",
    "    for i in 2:(line_segment)\n",
    "       current_progress +=  norm(env.waypoints[i] - env.waypoints[i - 1])  \n",
    "    end\n",
    "    current_progress += norm(env.waypoints[line_segment] - env.projected_position)\n",
    "    \n",
    "    env.progress = current_progress\n",
    "    \n",
    "\n",
    "    if env.realtime\n",
    "        sleep(env.Δt) # TODO: just a dirty hack. this is of course slower than real time.\n",
    "    end\n",
    "\n",
    "    # env.t += env.Δt # todo reevaluate\n",
    "\n",
    "\n",
    "    if env.visualization\n",
    "        set_transform(env.name, env.x_W,QuatRotation(env.R_W));\n",
    "        set_Crazyflie_actuators(env.name, next_action[1:4])\n",
    "        #transform_arrow(string(env.name, \"_vel\"), env.x_W, env.v_W_target, max_head_radius=0.05)               \n",
    "        transform_arrow(string(env.name, \"vel\"), env.x_W, env.R_W*env.v_B, max_head_radius=0.05) \n",
    "    \n",
    "        for i in eachindex(env.reached_goal)\n",
    "            if env.reached_goal[i]\n",
    "                create_sphere(\"fixgoal_$i\", 0.05, color=RGBA{Float32}(1.0, 0.0, 0.0, 1.0));\n",
    "                set_transform(\"fixgoal_$i\", env.waypoints[i]);\n",
    "            end\n",
    "        end\n",
    "\n",
    "    end\n",
    "    \n",
    "    \n",
    "    #v_B_target = transpose(env.R_W)*env.v_W_target\n",
    "    \n",
    "    \n",
    "    # State space\n",
    "    #todo\n",
    "    env.state[1] = env.x_W[1];\n",
    "    env.state[2] = env.x_W[2];\n",
    "    env.state[3] = env.x_W[3];\n",
    "    \n",
    "    env.state[4] = env.R_W[1,1] # 1  World Vector UP x\n",
    "    env.state[5] = env.R_W[2,1] # 2  World Vector UP y\n",
    "    env.state[6] = env.R_W[3,1] # 3  World Vector UP z\n",
    "\n",
    "    env.state[7] = env.R_W[1,3] # 4  World Vector FRONT x\n",
    "    env.state[8] = env.R_W[2,3] # 5  World Vector FRONT y\n",
    "    env.state[9] = env.R_W[3,3] # 6  World Vector FRONT z\n",
    "        \n",
    "    env.state[10] = env.v_B[1] # 7  Body velocity along x\n",
    "    env.state[11] = env.v_B[2] # 8  Body velocity along y\n",
    "    env.state[12] = env.v_B[3] # 9  Body velocity along z\n",
    "\n",
    "    env.state[13] = env.ω_B[1] # 10 Body rotational velocity around x\n",
    "    env.state[14] = env.ω_B[2] # 11 Body rotational velocity around y\n",
    "    env.state[15] = env.ω_B[3] # 12 Body rotational velocity around z\n",
    "\n",
    "    env.state[16] = env.waypoints[env.current_point][1] - env.x_W[1] # 13 position error to next gate along x\n",
    "    env.state[17] = env.waypoints[env.current_point][2] - env.x_W[2]; # 14 position error to next gate along z\n",
    "    env.state[18] = env.waypoints[env.current_point][3] - env.x_W[3]; # 15 position error to next gate along z\n",
    "     #todo            \n",
    "    if env.current_point <= env.num_waypoints\n",
    "        env.state[19] = env.waypoints[env.current_point + 1][1] - env.waypoints[env.current_point][1] ; # 16 way to next next gate x (next next gate - next gate), dummy integriert\n",
    "        env.state[20] = env.waypoints[env.current_point + 1][2] - env.waypoints[env.current_point][2]; # 17 way to next next gate y (next next gate - next gate), dummy integriert\n",
    "        env.state[21] = env.waypoints[env.current_point + 1][3] - env.waypoints[env.current_point][3]; # 18 way to next next gate z (next next gate - next gate), dummy integriert\n",
    "    end\n",
    "\n",
    "    \n",
    "    \n",
    "    # Termination criteria\n",
    "    env.done = #true\n",
    "        # After time... How fast is drone+Range of desired point\n",
    "        # After reaching position (circle of r_tol)\n",
    "         norm(env.ω_B) > 100.0 || \n",
    "         norm(env.v_B) > 100.0 || # stop if body is too fast_point \n",
    "        env.x_W[3] < -1.0 || # stop if body is below -5m\n",
    "        env.t > env.num_waypoints * 3.0 ||# stop after 3s per point #todo set in fast learning phase\n",
    "        norm(env.x_W - env.projected_position) > 5.0 || # too far off the path \n",
    "        env.reached_goal == trues(env.num_waypoints)\n",
    "\n",
    "    nothing\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e1cd988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[1mTest Summary:              | \u001b[22m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal  \u001b[22m\u001b[39m\u001b[0m\u001b[1mTime\u001b[22m\n",
      "random policy with VtolEnv | \u001b[32m2000  \u001b[39m\u001b[36m 2000  \u001b[39m\u001b[0m2.6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"random policy with VtolEnv\", Any[], 2000, false, false, true, 1.676922934241285e9, 1.676922936840678e9)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RLBase.test_runnable!(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c223a31f",
   "metadata": {},
   "source": [
    "Show an overview of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe6de74",
   "metadata": {},
   "source": [
    "# Setup of a reinforcement learning experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5683fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# VtolEnv\n",
       "\n",
       "## Traits\n",
       "\n",
       "| Trait Type        |                  Value |\n",
       "|:----------------- | ----------------------:|\n",
       "| NumAgentStyle     |          SingleAgent() |\n",
       "| DynamicStyle      |           Sequential() |\n",
       "| InformationStyle  | ImperfectInformation() |\n",
       "| ChanceStyle       |           Stochastic() |\n",
       "| RewardStyle       |           StepReward() |\n",
       "| UtilityStyle      |           GeneralSum() |\n",
       "| ActionStyle       |     MinimalActionSet() |\n",
       "| StateStyle        |     Observation{Any}() |\n",
       "| DefaultStateStyle |     Observation{Any}() |\n",
       "\n",
       "## Is Environment Terminated?\n",
       "\n",
       "No\n",
       "\n",
       "## State Space\n",
       "\n",
       "`Space{Vector{ClosedInterval{Float64}}}(ClosedInterval{Float64}[-Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf])`\n",
       "\n",
       "## Action Space\n",
       "\n",
       "`Space{Vector{ClosedInterval{Float64}}}(ClosedInterval{Float64}[0.0..1.0, 0.0..1.0, 0.0..1.0, 0.0..1.0])`\n",
       "\n",
       "## Current State\n",
       "\n",
       "```\n",
       "[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5257336750651325, 0.2780214283458484, 1.0473584463611312, -0.024506272974075927, 0.6997206876889033, 0.6024122675209753]\n",
       "```\n"
      ],
      "text/plain": [
       "# VtolEnv\n",
       "\n",
       "## Traits\n",
       "\n",
       "| Trait Type        |                  Value |\n",
       "|:----------------- | ----------------------:|\n",
       "| NumAgentStyle     |          SingleAgent() |\n",
       "| DynamicStyle      |           Sequential() |\n",
       "| InformationStyle  | ImperfectInformation() |\n",
       "| ChanceStyle       |           Stochastic() |\n",
       "| RewardStyle       |           StepReward() |\n",
       "| UtilityStyle      |           GeneralSum() |\n",
       "| ActionStyle       |     MinimalActionSet() |\n",
       "| StateStyle        |     Observation{Any}() |\n",
       "| DefaultStateStyle |     Observation{Any}() |\n",
       "\n",
       "## Is Environment Terminated?\n",
       "\n",
       "No\n",
       "\n",
       "## State Space\n",
       "\n",
       "`Space{Vector{ClosedInterval{Float64}}}(ClosedInterval{Float64}[-Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf])`\n",
       "\n",
       "## Action Space\n",
       "\n",
       "`Space{Vector{ClosedInterval{Float64}}}(ClosedInterval{Float64}[0.0..1.0, 0.0..1.0, 0.0..1.0, 0.0..1.0])`\n",
       "\n",
       "## Current State\n",
       "\n",
       "```\n",
       "[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5257336750651325, 0.2780214283458484, 1.0473584463611312, -0.024506272974075927, 0.6997206876889033, 0.6024122675209753]\n",
       "```\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 123    \n",
    "rng = StableRNG(seed)\n",
    "#N_ENV = 8 # TODO 1 environment\n",
    "UPDATE_FREQ = 2048\n",
    "EVALUATION_FREQ = 10_000\n",
    "SAVE_FREQ = 100_000\n",
    "\n",
    "# define multiple environments for parallel training\n",
    "env = VtolEnv(; rng = StableRNG(hash(seed)), name = \"cf_SAC\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1f128b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_q_net (generic function with 1 method)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the function approximator\n",
    "# TODO ?\n",
    "    ns, na = length(state(env)), length(action_space(env))\n",
    "\n",
    "create_policy_net() = NeuralNetworkApproximator(\n",
    "        model = GaussianNetwork(\n",
    "            pre = Chain(\n",
    "                Dense(ns, 256, tanh, init = glorot_uniform(rng)),\n",
    "                Dense(256, 256, tanh, init = glorot_uniform(rng)),\n",
    "            ),\n",
    "            μ = Chain(Dense(256, na, init = glorot_uniform(rng))),\n",
    "            logσ = Chain(Dense(256, na, x -> clamp.(x, typeof(x)(-10), typeof(x)(2)), init = glorot_uniform(rng))),\n",
    "        ),\n",
    "        optimizer = ADAM(1e-4),\n",
    "    )\n",
    "\n",
    "create_q_net() = NeuralNetworkApproximator(\n",
    "        model = Chain(\n",
    "            Dense(ns + na, 256, tanh; init = glorot_uniform(rng)),\n",
    "            Dense(256, 256, tanh; init = glorot_uniform(rng)),\n",
    "            Dense(256, 1; init = glorot_uniform(rng)),\n",
    "        ),\n",
    "        optimizer = ADAM(1e-4),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05ab2a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size = 100_000_000\n",
    "start_steps = dataset_size # puffer size\n",
    "trajectory_num = dataset_size\n",
    "#TODO ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ea4c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    agent = Agent( # A wrapper of an AbstractPolicy\n",
    "     # TODO ?    \n",
    "    \n",
    "    policy = SACPolicy(\n",
    "            policy = create_policy_net(),\n",
    "            qnetwork1 = create_q_net(),\n",
    "            qnetwork2 = create_q_net(),\n",
    "            #target_qnetwork1 = create_q_net(),\n",
    "            #target_qnetwork2 = create_q_net(),\n",
    "            γ = 0.99f0,\n",
    "            τ = 0.005f0,\n",
    "            α = 0.2f0,\n",
    "            batch_size = 256,\n",
    "            start_steps = start_steps,\n",
    "            start_policy = RandomPolicy(Space([0.0..1.0 for _ in 1:na]); rng = rng),\n",
    "            update_after = start_steps,\n",
    "            update_freq = UPDATE_FREQ,\n",
    "            automatic_entropy_tuning = true,\n",
    "            lr_alpha = 0.003f0,\n",
    "            action_dims = na,\n",
    "            rng = rng,\n",
    "        ),\n",
    "        trajectory = CircularArraySARTTrajectory(\n",
    "            capacity = dataset_size+1,\n",
    "            state = Vector{Float32} => (ns,),\n",
    "            action = Vector{Float32} => (na,),\n",
    "        ),\n",
    "\n",
    "    \n",
    "    );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aafcb113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "saveModel (generic function with 1 method)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function saveModel(t, agent, env)\n",
    "    model = cpu(agent.policy)  \n",
    "    if SLOW_MODE\n",
    "        f = joinpath(\"./RL_models_slow/\", \"cf_sac_$t.bson\")\n",
    "    else\n",
    "        f = joinpath(\"./RL_models_fast/\", \"cf_sac_$t.bson\")\n",
    "    end;\n",
    "    @save f model\n",
    "    println(\"parameters at step $t saved to $f\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50638c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loadModel (generic function with 1 method)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loadModel()\n",
    "    f = joinpath(\"./RL_models_slow/\", \"cf_sac_$(load_from_slow_step).bson\")\n",
    "    @load f model\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7470f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "function validate_policy(t, agent, env)\n",
    "    # for validation extract the policy from the agend\n",
    "    run(agent.policy, test_env, StopAfterEpisode(1), \n",
    "        ComposedHook(\n",
    "        episode_test_step_hook, \n",
    "        episode_test_reward_hook\n",
    "    ),\n",
    "        )\n",
    "    # the result of the hook\n",
    "    reward = round((episode_test_reward_hook.rewards[end]),digits = 3)\n",
    "    length = episode_test_step_hook.steps[end-1]\n",
    "    \n",
    "    println(\"step: \", t, \" reward : \",reward, \" length: \", length)\n",
    "\n",
    "    with_logger(logger) do\n",
    "        @info \"evaluating\" avg_length = length  avg_reward = reward  log_step_increment = 0\n",
    "    end\n",
    "    end;\n",
    "\n",
    "episode_test_reward_hook = TotalRewardPerEpisode( is_display_on_exit=false)\n",
    "episode_test_step_hook = StepsPerEpisode()\n",
    "# create a env only for reward test\n",
    "\n",
    "test_env = VtolEnv(;name = \"test_cf\", visualization = true, realtime = true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af9745c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000000000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of steps\n",
    "steps_slow = 200_000_000_000\n",
    "steps_fast = 200_000_000_000\n",
    "load_from_slow_step = 200_000_000_000 # TODO: choose slow model\n",
    "\n",
    "save_freq = 1_000_000\n",
    "validate_freq = 1_000_000\n",
    "\n",
    "steps = 0\n",
    "if SLOW_MODE\n",
    "    steps = steps_slow\n",
    "else\n",
    "    steps = steps_fast\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6aadbfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hook which is called during the training\n",
    "total_reward_per_episode = TotalRewardPerEpisode(is_display_on_exit = false)\n",
    "hook = ComposedHook(\n",
    "    #total_batch_reward_per_episode,\n",
    "    DoEveryNStep(saveModel, n=save_freq),\n",
    "    DoEveryNStep(validate_policy, n=validate_freq),\n",
    "    #=\n",
    "    DoEveryNStep() do t, agent, env\n",
    "        p = agent.policy\n",
    "        with_logger(logger) do\n",
    "            @info \"training\" loss = mean(p.loss)  actor_loss = mean(p.actor_loss)  critic_loss = mean(p.critic_loss)\n",
    "        end\n",
    "    end,\n",
    "    =#\n",
    "    DoEveryNStep() do t, agent, env\n",
    "        with_logger(logger) do\n",
    "            if length(total_reward_per_episode.rewards) > 1\n",
    "                @info \"training\" total_reward_per_episode.rewards[end]\n",
    "            end\n",
    "        end\n",
    "    end,\n",
    "    #=\n",
    "    DoEveryNStep() do t, agent, env\n",
    "        with_logger(logger) do\n",
    "            @info \"training\" action_thrust_1 = env[1].action[1]  action_thrust_2 = env[1].action[2] action_thrust_3 = env[1].action[3] action_thrust_4 = env[1].action[4]\n",
    "        end\n",
    "    end,\n",
    "    =#\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d479e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo load model\n",
    "if !SLOW_MODE\n",
    "    agent.policy.approximator = loadModel(); \n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dfdb68",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   0%|                                         |  ETA: 84.26 days\u001b[39mm[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 1000000 saved to ./RL_models_slow/cf_sac_1000000.bson\n",
      "step: 1000000 reward : -14.34 length: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   0%|                                         |  ETA: 87.67 days\u001b[39mm"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 2000000 saved to ./RL_models_slow/cf_sac_2000000.bson\n",
      "step: 2000000 reward : -23.0 length: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   0%|                                         |  ETA: 82.43 days\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 3000000 saved to ./RL_models_slow/cf_sac_3000000.bson\n",
      "step: 3000000 reward : -17.278 length: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   0%|                                         |  ETA: 78.98 days\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 4000000 saved to ./RL_models_slow/cf_sac_4000000.bson\n",
      "step: 4000000 reward : -26.846 length: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   0%|                                         |  ETA: 75.36 days\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 5000000 saved to ./RL_models_slow/cf_sac_5000000.bson\n",
      "step: 5000000 reward : -14.293 length: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   0%|                                         |  ETA: 73.29 days\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 6000000 saved to ./RL_models_slow/cf_sac_6000000.bson\n",
      "step: 6000000 reward : -29.922 length: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   0%|                                         |  ETA: 70.75 days\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 7000000 saved to ./RL_models_slow/cf_sac_7000000.bson\n",
      "step: 7000000 reward : -13.084 length: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   0%|                                         |  ETA: 70.76 days\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 8000000 saved to ./RL_models_slow/cf_sac_8000000.bson\n",
      "step: 8000000 reward : -13.281 length: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   0%|                                         |  ETA: 70.16 days\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 9000000 saved to ./RL_models_slow/cf_sac_9000000.bson\n",
      "step: 9000000 reward : -16.731 length: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   0%|                                         |  ETA: 70.51 days\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 10000000 saved to ./RL_models_slow/cf_sac_10000000.bson\n",
      "step: 10000000 reward : -26.85 length: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   0%|                                         |  ETA: 70.07 days\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 11000000 saved to ./RL_models_slow/cf_sac_11000000.bson\n",
      "step: 11000000 reward : -28.409 length: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   0%|                                         |  ETA: 68.94 days\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 12000000 saved to ./RL_models_slow/cf_sac_12000000.bson\n",
      "step: 12000000 reward : -17.036 length: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   0%|                                         |  ETA: 68.86 days\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 13000000 saved to ./RL_models_slow/cf_sac_13000000.bson\n",
      "step: 13000000 reward : -16.002 length: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   0%|                                         |  ETA: 68.65 days\u001b[39m"
     ]
    }
   ],
   "source": [
    "if TRAINING\n",
    "    ReinforcementLearning.run(\n",
    "        agent,\n",
    "        env,\n",
    "        StopAfterStep(steps),\n",
    "        hook\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a5d7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAINING\n",
    "    plot(episode_test_reward_hook.rewards)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c1dc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAINING\n",
    "    plot(episode_test_step_hook.steps[1:2:end])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c89b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "close_visualization(); # closes the MeshCat visualization"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
